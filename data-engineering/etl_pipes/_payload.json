[{"data":1,"prerenderedAt":224},["Reactive",2],{"search-api":3},[4,11,19,29,37,47,53,71,78,103,113,129,149,159,175,182,197,209,212,218],{"id":5,"path":6,"dir":7,"title":8,"description":7,"keywords":9,"body":10},"content:0.index.md","/","","Home",[],"     Tutorials & Technical Guides on All Things Data   This website contains years' worth of content that I've created related to programming, data science, engineering, communications, and design. If you find this place useful, share it with other people. If you have questions about what's in here or want to share your experience with others, send me a message.     Lessons on Programming, Web Development and DevOps Engineering  Project-based content about Data Analytics, Science and Engineering  Notes on how to be a good communicator  Writing on writing for you and others  Design tips and tricks to tip you into tricking others in cool ways  A Discord Community  All free, open-source, interactive, and downloadable in 1 click       Programming   Learn how to write code in different languages while building stuff.    Data Analytics   Make sense of data with real-world examples and exercises.    Data Engineering   Become a data janitor unafraid of working with the muddiest data pipelines out in the wild.    Data Science   Learn about statistics, experimentation, and machine learning with fun code examples, memes, and cartoons.    Machine Learning Engineering   Deploy and monitor machine learning models for free, on a budget, or by breaking the bank.    Data Visualization   Create compelling stories with your data, others' data, or all of the data out there.    DevOps   Create infrastructure that's reliable and easy to maintain.    Software Development   Lessons around full-stack web and mobile development.    AI Engineering   Build products with AI at their core or as a major aspect of the user's experience.    Communications   Tips and tricks for creating compelling presentations and workshops.    Creative Coding   Create compelling stories with your data, others' data, or all of the data out there.    Game Development   Develop games in different programming languages.   What's included  ",{"id":12,"path":13,"dir":7,"title":14,"description":15,"keywords":16,"body":18},"content:0.introduction:1.index.md","/introduction","Introduction","Here is some useful information to get you set up and ready to learn cool stuff.",[17],"Roadmap","  Introduction  Here is some useful information to get you set up and ready to learn cool stuff.  Roadmap   Reconstruct all Lessons  Add installation videos",{"id":20,"path":21,"dir":22,"title":23,"description":7,"keywords":24,"body":28},"content:0.introduction:2.python_installation.md","/introduction/python_installation","introduction","Python Installation",[25,26,27],"macOS / Mac OS X:","Windows","Linux","  Python Installation    Python is a high-level programming language that is  widely used for software and web development, scripting, data analytics, reasearch, and scientific programming. To download and install the latest version of Python onto your computer, please follow the steps below for your respective operating system.   macOS / Mac OS X:  Download Python from the from the Python website & Install it in your machine.   Open a web browser and go to the   Python downloads page .\n   Click on the link/button to download Python 3.9 or later.\n   Once dowloaded, open the   .pkg  installer in your Downloads folder and follow the installation instructions (we recommend to leave all of the default settings as they are). Enter your password if prompted.   After the installation is finished, open the Terminal application of macOS. You can press   Command + Space Bar , this will open up the   Spotlight Search Bar  and you will then type the word   terminal  and press   enter/return  when the application pops up. You can also go to   Application  →   Utilities  and search for the terminal. A bash terminal will open where you can enter commands and do cool things in your computer.  Once you have it opened, let’s proceed with the basics. Enter the following command in the terminal, and then press   enter/return :     pwd\n  You should now see the working directory or main folder where everything that exists within your username will come up. Next, type the command below and press   enter/return :     python3\n  You should see the Python version you have installed (i.e. 3.x), and you can now run Python code using your terminal. Print “Hello world” using the print command. Type   print(\"Hello World!\")    You can type   exit()  in your terminal to exit out of Python and then close the terminal window.  Setting Python 3 as the default version in Mac OS  If an older version of python already exists in your computer, you can change the alias of the python command in your bash terminal to refer to the python 3.x version you just installed.  To do this, you will need to follow the following steps in your terminal.   Open your terminal again. Click on the Spotlight Search button, type   terminal , and then press   enter/return .  Type   nano ~/.bash_profile  Add the following in a new line:     alias   python  =  \"python3\"\n   Press   Control + o  and then press   enter/return  Press   Control + x  to exit nano  If you close that terminal and then open a new one, you can now use the “python” command and it should invoke the latest version of python3.    Windows  Download Python from the from the Python website & Install it in your machine.   Open a web browser and go to the   Python downloads page for Windows . Underneath the Python Releases for Windows find Latest Python 3 Release – Python 3.8.x (latest stable release at the time of writing, is Python 3.9.2):   Click on the latest stable release near the top of the page. Then on the release page, scroll down to Files and click on Windows x86-64 executable installer for 64-bit or Windows x86 executable installer for 32-bit. If you are unsure about your operating system, please check before downloading a version of Python.   Once dowloaded, open the   .exe  installer in your Downloads folder and follow the installation instructions. Make sure to check “Add Python 3.x to PATH” or otherwise you will have to do it explicitly. Click “Install Now” and after doing so, it will start installing Python on Windows.   After the installation is complete, click   Close . The latest version of Python should now be installed.    You can use the Command Prompt in Windows to invoke Python. Go to the start menu and open the Command Prompt application. Type   python  in the command prompt to confirm that it was installed correctly. Then try entering the command to   print(“Hello World!”)  to run your first line of code.      Linux  Python comes pre-installed in most Linux machines so we will first be checking for the version we have, if any, and then updating it to the latest stable release.  Debian / Ubuntu (apt-get)  To see which version of Python is installed, open a terminal and run:     python3   --version\n  If you are using Ubuntu 18.10 or newer, then you can easily install the latest version of Python 3 with the following commands:     sudo   apt-get   update\n   sudo   apt-get   install   python3\n  The latest version of Python language should now be installed on your Linux system, and you can confirm it by running the previous command again:     python3   --version\n  Fedora (dnf/yum)  Other Linux systems may also have Python pre-installed. You can update to the latest version using your Linux distribution’s package manager. On Fedora, use the command:     $   sudo   dnf   install   python3\n  Open a command prompt or shell and run the following command to verify that Python was installed correctly.     $   python3   --version\n   Python   3.7  .5\n  How to set Python 3 as the default version in Linux?  You can set python3 as the default Python of your Linux system, so that whenever you enter “python” anywhere in the terminal it always executes python3. Open the terminal and type the following command     vim   ~/.bash_profile\n  Press the letter   i  and then add the following command in a new line.     alias   python  =  \"python3\"\n  Then you will press the Escape buttom in your keyboard, the type   :wq  and then press   Enter . Close and restart your Terminal, and now, any code executed in your terminal will automatically use python3 as the default version.  Type   python  in your terminal and then   print(“Hello world”) .    html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":30,"path":31,"dir":22,"title":32,"description":33,"keywords":34,"body":36},"content:0.introduction:3.git_installation.md","/introduction/git_installation","Git Installation","First, create an account on GitHub (recommended) or GitLab. Both are hosting sites for online Git repositories. You can think of a repository as a bucket containing snapshots of the state of your project and all of the code within it at a given point in time. This means that if you ever change something you should have not changed, make ammendments to bugs and other issues, or would simply like to go back to the previous version of a file, you will always have all of your versions at your disposal.",[35,26],"Mac","  Git Installation  First, create an account on   GitHub  (recommended) or   GitLab . Both are hosting sites for online Git repositories. You can think of a repository as a bucket containing snapshots of the state of your project and all of the code within it at a given point in time. This means that if you ever change something you should have not changed, make ammendments to bugs and other issues, or would simply like to go back to the previous version of a file, you will always have all of your versions at your disposal.  Follow the instructions below for your respective OS to set up git on your machine.   Note:  We will go over a “Brief Introduction To Git” during our first session.    Mac  By deafault, you will have Git available on your Mac, but if you would like to have the absolute latest version, please folow the steps outlined in this section.   Download the latest Git for Mac installer from   sourceforge .  Once the download finishes, open the   .dmg  package and follow the prompts to install Git.   Open a terminal and verify the installation was successful by typing   git --version :     $   git   --version\n   git   version   2.15  .0\n    Windows   Download the latest Git for Windows installer.  Run the installer and you should see the Git Setup wizard screen. You can leave most of the default options as they are, except where noted below.   Note:  Git Bash is an application for Microsoft Windows environments which provides an emulation layer for a Git command line experience. It is a useful option for users to interface with git through written commands. Click   here  for information on using Git Bash.  Please choose the following option during installation:  Follow the Next and Finish prompts to complete the installation.   Open a Command Prompt or Git Bash from the Start menu to start using Git.    Linux :  On Linux, you can install git using the following commands for your respective OS.    Debian / Ubuntu (apt-get)  Git packages are available via apt:   From your shell, install Git using apt-get (enter your password if prompted):     $   sudo   apt-get   update   \n   $   sudo   apt-get   install   git\n   Verify the installation was successful by typing   git --version :     $   git   --version\n   git   version   2.20  .1\n    Fedora (dnf/yum)  Git packages are available via both yum and dnf:   From your shell, install Git using dnf (or yum, on older versions of Fedora):     $   sudo   dnf   install   git\n  Or     $   sudo   yum   install   git\n  Note: You may be asked to confirm the download and installation - enter y to confirm.   Verify the installation was successful by typing git --version:     $   git   --version\n    git   version   2.20  .1\n  If your OS is not listed above, please check the installation instructions here   linux-other-os .  Useful Guides & Resources:    The Official Git Homepage   Interactive Git Tutorial   GitHub Introduction  &   GitHub Help Page   Atlassian Git Tutorials and Training   Basic Git Commands  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":38,"path":39,"dir":22,"title":40,"description":7,"keywords":41,"body":46},"content:0.introduction:4.jupyter_intro.md","/introduction/jupyter_intro","Jupyter Lab & Notebooks Intro",[42,43,44,45],"Table of Contents","II. Learning Outcomes","1. What is Python?","Examples","  Jupyter Lab & Notebooks Intro    Table of Contents   Learning Outcomes  What is Python? 🐍  Intro to JupyterLab  Intro to Python\n   Data Types  Variables  Data Structures  Printing  Math  Packages/Libraries  input data  Essential of programming\n   Conditional statements  Functions  Loops  Summary  Feedback  References  II. Learning Outcomes  By the end of this week, you will:   Have learned what the Python programming language is and how to use JupyterLab to run programs  Have learned about the different data types and data structures in Python  Be comfortable doing basic calculations in Python  Have learned how variables, loops, functions, and if-else statements work  Get experience creating and running a simple python program  1. What is Python?  If natural languages are one of the many ways in which we communicate with one another, programming languages are what we use to communicate with our computers. When you click and interact with websites through a   graphical user interface (GUI) , what makes that possible is the code running behind the scenes. This is essentially what python is, a language that allows us to communicate with our computers to create useful things together.  Python can also be used in a variety of domains, from developing websites and mobile phone applications, to creating games and analysing data. Although the focus of this course will be on the latter category, data analysis, our hope is that by the end of the course, the transition from data analytics to any of the other domains in which python operates, would be a smooth one for you.  A bit of history  Python was created in the late 1980's, early 90's by   Guido van Rossum , a retired dutch programmer who's most recent role was at   dropbox .  Why is it so popular?  Python is an excellent language with a lot of funtionalities, but one of its best characteristics is the plethora of   modules  it has available to increase our productivity and improve our workflow when analysing data. You can think of these modules or libraries of code as the additional benefits of this language. Here is a metaphor to give you an example, we humans are very awesome just the way we are born (e.g., without clothes, language, knowledge, etc.), but in order to function better in society, and cruise through different stages of our lives, we make use of different languages and objects that provide us with a better experience. The clothes, the accessories, and the languages we use are our added benefits just like modules, packages and libraries are Python's added benefits (not the most clever analogy, I know, stay with me though😎).   Note:  We will be using the terms library, module, and package interchangeably throughout this course.  Some of the modules that make up a big component of the data analytics/science ecosystem (and the ones we will use the most in this course), are the following ones:  For Data Analysis    pandas  -> \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"   numpy  -> \"is the fundamental package for scientific computing with Python. It contains among other things, a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code, useful linear algebra, Fourier transform, and random number capabilities.\"   SciPy  -> \"is a Python-based ecosystem of open-source software for mathematics, science, and engineering.\"  For Data Visualisation    matplotlib  -> \"is a comprehensive library for creating static, animated, and interactive visualizations in Python.\"   seaborn  -> \"s a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\"   altair  -> Although Altair has not had the same adoption from the community as the previous two data visualisation libraries, it is a great library for visualising your data.  * Definitions have been taken straight from the packages respective websites.  2. Introduction to JupyterLab  JupyterLab is an   Integrated Development Environment  (IDE) created by the   Jupyter Project . It allows you to combine different tools that are paramount for a good coding workflow. For example, you can have the terminal, a Jupyter notebook, and a markdown file for note-taking/documenting your work, as well as other files, opened at the same time to improve your workflow as you write code (see image below).  A silly metaphor to think about IDEs is that, IDEs are to programmers, data analysts, scientists, researcher, etc..., what a kitchen is to a chef, an indispensable piece to get things done.    Source  -   https://jupyterlab.readthedocs.io  Jupyter Lab is composed of cells and each cell has 3 states with the default state beign \"code,\" and the other two being markdown and raw text.  To run code you will use the following two commands:   Shift + Enter  The first option will run the cell where you have your cursor at and take you to the next one. If there is no cell underneath the one you just ran, it will insert a new one for you.   Alt + Enter  This second option will run the cell and insert a new one below automatically. Alternatively, you can also run the cells using the play (▶︎) button at the top or with the   Run menu  on the top left-hand corner.  Anything that follows a hash   #  sign is a comment and will not be evaluated by Python. They are useful for documenting your code and letting others know what is happening with every line of code or with every cell.  To check the information of a package, function, method, etc., use   ?  or   ??  at the begining or end of such element, and it will provide you with a lot of information about it.   What you will most often use:     Jupyter Notebooks  Straight from the   Jupiter Project website : \"The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.\"  Jupyter notebooks are a great tool for exploratory data analysis, creating educational content, and essentially, making sure you can go over your code as much as you want before moving it to a   .py  file for either production or later use. Which is what you will most-likely need to do to be able to run your code in production or for the later reproducibility of your analyses.     Command Line  A   command line interface (CLI)  or terminal, allows us to interact directly with our operating system by writing commands in a textual format as opposed to by clicking and dragging objects in the background through a GUI.  There are CLI's or terminals for each operating systems. The most widely used ones are   Bash  PowerShell  CMD (Windows)  Linux    Markdown Documents  \"Markdown is a text-to-HTML conversion tool for web writers. Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML). Thus, “Markdown” is two things: (1) a plain text formatting syntax, and (2) a software tool written in Perl that converts plain text formatting into HTML.\" by   Daring Fireball     IPython  IPython is an interactive environment for running python code. It allows you to quickly test code line-by-line before moving it to a full program. Some nice features of IPython are that it lets you know how a function works by providing you not only with a list of its arguments but also with a description of what that argument does. IPython also has the very handy autocomplete funtionality which can save you from a lot of typing, especially when what you are trying to do needs to be fast and iterative. We will be using IPython throughout the course.     Tutorials   Jupyter Notebook\n    Real Python   Dataquest   YouTube Corey Schafer  Markdown\n    Daring Fireball   Markdown Guide  You can also go to the pallette-looking command to your left called,   Commands , and go to   HELP  >   Markdown Reference  for a quick tutorial  IPython\n    Official Tutorial  Examples     # this is a code cell, look at the top where it says Code\n   # type 5 + 7 and press Shift + Enter\n     # change this cell to markdown at the top, come back and type a hash (#) followed by This is my first notebook, then delete this comment\n     # change this cell to raw text format and type \"Data is awesome\" :) then press Shift + Enter\n  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":48,"path":49,"dir":7,"title":50,"description":7,"keywords":51,"body":52},"content:1.programming:1.index.md","/programming","Programming Tutorials",[],"  Programming Tutorials",{"id":54,"path":55,"dir":56,"title":57,"description":58,"keywords":59,"body":70},"content:1.programming:terminal:1.shell_intro.md","/programming/terminal/shell_intro","terminal","Intro to the Terminal","100% Generated by AI! 🤖",[60,61,62,63,64,65,66,67,68,69],"Part 1: The Journey Begins","Part 2: Manipulating Reality","Part 3: The Power of Words","Part 4: System Infiltration","Part 5: Access and Control","Part 6: Network Reconnaissance","Part 7: Data Extraction and Manipulation","Part 8: Scripting and Automation","Part 9: Encryption and Steganography","Part 10: The Final Hack","  Intro to the Terminal  100% Generated by AI! 🤖  The Command Line Chronicles: A Cyberpunk Odyssey  In a dystopian future where technology reigns supreme, you find yourself navigating the treacherous depths of the Linux command line. As a skilled hacker in this cyberpunk world, your mastery of the command line is your greatest weapon against the oppressive corporations that control society. Each command you learn is a step closer to unraveling the secrets hidden within the digital realm.  Part 1: The Journey Begins  Your journey begins in the neon-lit streets of the sprawling metropolis. As you boot up your terminal, you find yourself at the command prompt, ready to explore the vast digital landscape.     $   ls\n   data    documents    projects    secrets\n  The   ls  command becomes your eyes, allowing you to scan your surroundings and revealing the files and directories that lie before you.     $   cd   projects\n   $   pwd\n   /home/hacker/projects\n  With the   cd  command, you navigate through the labyrinthine directory structure, moving from one location to another like a ghost in the machine.  Part 2: Manipulating Reality  In this world, information is power, and the ability to manipulate files is a crucial skill.     $   cp   secrets/corporate_plans.txt   backups/\n   $   ls   backups/\n   corporate_plans.txt\n  The   cp  command allows you to duplicate files, creating backups or planting digital decoys to mislead your enemies.     $   mv   backups/corporate_plans.txt   projects/target_acquired.txt\n   $   ls   projects/\n   target_acquired.txt\n  With the   mv  command, you can seamlessly relocate files, covering your tracks or strategically positioning data.     $   rm   -r   backups/\n  The   rm  command becomes your digital erasure tool, allowing you to remove files and directories, leaving no trace behind.  Part 3: The Power of Words  As you delve deeper into the command line, you realize the power of text manipulation.     $   cat   projects/target_acquired.txt\n   The   corporation's secret plans are as follows...\n  The   cat  command allows you to view the contents of files, uncovering hidden messages and crucial data.     $   grep   \"password\"   data/logs.txt\n   [  2023  -05-10   23  :45:22] User entered password: s3cr3t_c0d3\n  With the   grep  command, you can search through vast amounts of text, finding needles in digital haystacks.     $   echo   \"Infiltration successful.\"   >   mission_log.txt\n   $   cat   mission_log.txt\n   Infiltration   successful.\n  The   echo  command becomes your voice, allowing you to leave messages or write data to files.     $   sed   's/corporate/resistance/g'   manifesto.txt   >   new_manifesto.txt\n  The   sed  command enables you to perform complex text transformations, altering reality with a few keystrokes.  Part 4: System Infiltration  In the world of cyberpunk, knowledge of the system is paramount.     $   top\n   PID    USER        PR    NI      VIRT      RES      SHR   S    %CPU    %MEM       TIME+   COMMAND\n   1234   hacker      20     0   1423216   218064    42680   S     8.3     5.4     1  :32.84   chrome\n   5678   hacker      20     0    890112    90564    16840   S     3.2     2.2     0  :45.67   vscode\n  The   top  command becomes your system monitor, giving you real-time insights into the resource usage of your digital environment.     $   df   -h\n   Filesystem        Size    Used   Avail   Use%   Mounted   on\n   /dev/sda1          64  G     32  G     30  G    52  %   /\n   /dev/sda2         128  G    100  G     24  G    81  %   /home\n  With the   df  command, you can assess the disk space, ensuring you have enough room for your digital exploits.     $   ps   aux   |   grep   \"surveillance\"\n   hacker      7890    0.8    1.2   1134020   102840   pts/2    Sl+    10  :45     0  :30   python   surveillance.py\n  The   ps  command allows you to view running processes, identifying potential threats or targets.  Part 5: Access and Control  As you navigate the digital landscape, you must master the art of access control.     $   ls   -l   secret_file.txt\n   -rw-------   1   hacker   hacker   1024   May   12   15  :30   secret_file.txt\n   $   chmod   644   secret_file.txt\n   $   ls   -l   secret_file.txt\n   -rw-r--r--   1   hacker   hacker   1024   May   12   15  :30   secret_file.txt\n  The   chmod  command becomes your tool for granting or restricting permissions, determining who can access your files and directories.     $   sudo   chown   root:root   critical_system.txt\n   [sudo] password   for   hacker:\n   $   ls   -l   critical_system.txt\n   -rw-r--r--   1   root   root   2048   May   12   16  :45   critical_system.txt\n  With the   chown  command and   sudo , you can transfer ownership and elevate your privileges, manipulating the digital hierarchy to your advantage.  Part 6: Network Reconnaissance  In the interconnected world of cyberpunk, network skills are essential.     $   ssh   hacker@remote-server.com\n   hacker@remote-server.com  's password:\n   Welcome to the remote server.\n  The   ssh  command allows you to securely access remote systems, infiltrating enemy networks or collaborating with fellow hackers.     $   ping   target-server.corp\n   PING   target-server.corp   (192.168.1.100) 56(  84  ) bytes of data.\n   64   bytes   from   target-server.corp   (192.168.1.100): icmp_seq=1 ttl=64 time=12.5 ms\n  With the   ping  command, you can probe the connectivity of devices, mapping out the digital landscape.     $   traceroute   secure-system.net\n   traceroute   to   secure-system.net   (10.0.0.50), 30 hops max, 60 byte packets\n    1    gateway   (192.168.1.1)  3.141 ms  2.792 ms  2.694 ms\n    2    *   *   *\n    3    secure-system.net   (10.0.0.50)  48.127 ms  50.258 ms  50.223 ms\n  The   traceroute  command enables you to trace the path your data takes, identifying potential vulnerabilities or surveillance points.  Part 7: Data Extraction and Manipulation  As you uncover the secrets of the command line, you learn to extract and manipulate data with precision.     $   curl   -o   secret_data.json   https://corp-server.com/api/secrets\n     %   Total      %   Received   %   Xferd    Average   Speed     Time      Time       Time    Current\n                                    Dload    Upload     Total     Spent      Left    Speed\n   100   1024  k    100   1024  k      0       0    2048  k        0   --:--:--   --:--:--   --:--:--   2048  k\n  The   curl  command becomes your tool for retrieving data from the web.     $   awk   '{sum+=$1} END {print \"Total: \", sum}'   financial_data.txt\n   Total:   10582940\n  The   awk  command allows you to process and analyze text data, uncovering patterns and insights.     $   cat   user_data.json   |   jq   '.users[].email'\n   \"john@example.com\"\n   \"alice@example.com\"\n   \"bob@example.com\"\n  With the   jq  command, you can parse and manipulate JSON data, the lifeblood of the digital world.  Part 8: Scripting and Automation  In the fast-paced world of cyberpunk, automation is key.     #!/bin/bash\n   \n   for   file   in   *  .log;   do\n       grep   \"ERROR\"   \"  $file  \"   >>   error_logs.txt\n   done\n   \n   echo   \"Error logs collected successfully.\"\n  With the power of scripting, you can create your own digital tools and automate repetitive tasks. The   bash  scripting language becomes your ally, allowing you to combine commands and create complex workflows.     $   crontab   -e\n   0   2   *   *   *   /home/hacker/scripts/data_backup.sh\n  The   cron  utility enables you to schedule tasks, ensuring your scripts run automatically at predetermined intervals.  Part 9: Encryption and Steganography  In a world where information is currency, encryption and steganography are essential skills.     $   gpg   -c   important_file.txt\n   Enter   passphrase:\n   Repeat   passphrase:\n   $   ls\n   important_file.txt    important_file.txt.gpg\n  The   gpg  command allows you to encrypt and decrypt files, protecting your sensitive data from prying eyes.     $   steghide   embed   -cf   innocent_image.jpg   -ef   secret_message.txt\n   Enter   passphrase:\n   Re-Enter   passphrase:\n   embedding   \"secret_message.txt\"   in   \"innocent_image.jpg\"...   done\n  With steganography techniques and the   steghide  command, you can hide messages within seemingly innocent files, concealing your communication channels.  Part 10: The Final Hack  As you approach the final chapter, you realize that the command line is more than just a tool; it's a way of life. With your newfound skills, you embark on the ultimate hack, infiltrating the core of the oppressive corporation that controls society.     $   ssh   -i   secret_key.pem   hacker@corp-mainframe.com\n   Welcome   to   the   Corporate   Mainframe.\n   \n   $   cd   /root/secret_projects\n   $   tar   -czf   projects.tar.gz   *\n   $   scp   projects.tar.gz   hacker@hidden-server.net:~/\n  You combine all the commands you've learned, chaining them together in a symphony of keystrokes. As you execute the final command, you hold your breath, waiting for the outcome that will determine the fate of the world.     $   ./deploy_resistance_ai.sh\n   Initializing   Resistance   AI...\n   Uploading   virus   to   corporate   systems...\n   Infiltrating   security   protocols...\n   Overriding   control   systems...\n   \n   Corporate   control   terminated.\n   Freedom   restored   to   the   people.\n   \n   Congratulations,   hacker.   You   have   successfully   liberated   society   from   the   grasp   of   the   oppressive   corporations.   The   command   line   has   been   your   loyal   companion   throughout   this   journey,   empowering   you   to   shape   the   world   according   to   your   vision.\n   \n   As   you   step   out   into   the   now-free   streets   of   the   city,   you   feel   a   sense   of   pride   and   accomplishment.   Your   mastery   of   the   Linux   command   line   has   not   only   saved   you   but   has   also   sparked   a   revolution   that   will   echo   through   the   ages.\n   \n   The   command   line   may   have   been   your   weapon,   but   it   was   your   spirit,   your   determination,   and   your   unwavering   commitment   to   justice   that   truly   made   you   a   hero.   And   so,   with   the   world   at   your   fingertips,   you   embark   on   a   new   journey,   ready   to   face   whatever   challenges   the   future   may   bring.\n   \n   $   echo   \"The future is ours to shape.\"\n   The   future   is   ours   to   shape.\n  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":72,"path":73,"dir":7,"title":74,"description":75,"keywords":76,"body":77},"content:2.analytics:1.index.md","/analytics","Data Analytics","Lots of content on data analytics and numerical computing.",[17],"  Data Analytics  Lots of content on data analytics and numerical computing.  Roadmap  Up next in the list   Analyzing patient health data with   DuckDB .  Drawing insights from financial data using   Ibis .  Sports Analytics with   Polars .  Click Baity titles analysis.",{"id":79,"path":80,"dir":81,"title":82,"description":7,"keywords":83,"body":102},"content:2.analytics:2.numerical_computing.md","/analytics/numerical_computing","analytics","Numerical Computing",[84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101],"1. Intro to Numpy","2. Arrays and Matrices","Exercise 2","Exercise 3","Exercise 4","Exercise 5","Exercise 6","3. Generating Data With NumPy","Exercise 7","Exercise 8","4. Computations with NumPy","Exercise 9","Exercise 10","5. Masking","Exercise 11","Exercise 12","5. Summary","6. References","  Numerical Computing   \"Learning is synthesizing seemingly divergent ideas and data.\" ~ Terry Heick  You can run through the exercises with the following links.    :simple-jupyter:    Download Jupyter Notebook  :fontawesome-brands-js:    Open in Colab  for interactivity  :fontawesome-brands-css3:    Open in Binder  :simple-github:    Open in GitHub Codespaces   \"The important thing to remember about mathematics is not to be frightened\" ~ Richard Dawkins    1. Intro to Numpy    Source:    https://en.wikipedia.org/wiki/NumPy  Numpy stands for Numerical Python and it is one of the core libraries in the suite of Python\ntools for data science and other numerical applications. Because of its numerical capabilities\nand speed, a lot of the libraries and packages in the data world depend on this wonderful tool.  The library allows us to run fast computations with arrays and matrices and calculate simple-to-complex\nmathematical calculations at great speed. One of the reasons why it is so powerful and fast is\nthat it is partly written in the C programming language. This means that computations with\nNumPy are executed much closer to the hardware of our computers than the regular lists in Python. We\nwill compare the speed of both, lists and numpy arrays, later in this lesson to better understand\nthe speed differences between the two.  Main Characteristics of NumPy:   Many built-in mathematical functions  Efficient computations with n-dimensional arrays (n is the number of dimensions)  Very fast  It allows for broadcasting. This is the equivalent computation of a loop but without the loop. Meaning, for a lot of tasks with large datasets, you won't need to create a loop  NumPy arrays use less computer memory than built-in Python equivalent data structures  Excellent tool for generating random data. This will be useful if you would like to simulate different scenarios in your analysese  Now, let's start coding.  2. Arrays and Matrices  Let's begin by importing NumPy with its industry alias. An alias is the nickname you give\nto the modules/libraries/packages you import, for example,   import cool_package.killer_function as kf  or\n  from my_great_package.hello import greetings_mom as gm  will both boil down to   kf  and   gm  respectively.     import numpy as np\n  Once you import numpy, and any other package for that matter, you can often check the version\nyou are using with   package.__version__  as shown next.     np.__version__\n   '1.26.2'\n  Arrays in numpy are similar to lists but they differ in that they can only contain\none data type. If you have ever added a number to a column, row, or both in Excel\nthen you have already worked with arrays and matrices. If you have ever written a\nlist of different things you needed to buy at the supermarket, as well as the quantity\nof each item, then you have already created a two-dimensional array or matrix.  Thinking of numpy arrays as the components of a spreadsheet will help you get a better  \nmental picture of these concepts. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Index  Date  Item  Quantity    1  12-Apr-20  toilet paper  1000   2  20-Apr-20  chicken  10   3  27-Apr-20  toilet paper  500   4  27-Apr-20  cereal  2   5  27-Apr-20  bread  1  We can start working with arrays by passing in a list of data to the   np.array()  method.\nThis method takes in Python lists (single or nested) and transform them immediately into\nthe   ndarray  dataclass.     # Create your first array\n   first_array = np.array([25, 40, 50, 16, 10, 19, 32, 27, 11])\n   print(first_array, type(first_array))\n   [25 40 50 16 10 19 32 27 11] \u003Cclass 'numpy.ndarray'>\n  You've probably already seen the native Python function,   range() , which creates a range\nof values up to but not including the stoping point, and with an optional step in between.\nThe equivalent of this function in numpy is the   np.arange()  (pronounced   a-range  not\n  arrange ). The nice thing about the numpy's version is that we can add an additional parameter\nthat lets us pick the data type of the array. Even thought you might not have to worry about\nthis until you tackle more advanced problems with larger datasets, the difference between an\n8-bit integer (  int8 ) and 64 one (  int64 ) can be drastic, effectively increasing or\ndecreasing the amount of time a computation might take.  Let's use both,   range()  and   np.arange()  to see how they work.     # since range() is a lazy function, we have to wrap it around a list() to print its values\n   print(list(range(10))) # create array of 10 numbers from 0 to 9\n   print(list(range(0, 10))) # create array of 10 numbers from 0 to 9\n   print(list(range(0, 10, 2))) # create array of even numbers from 0 to 9. Stepwise operation!\n   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 2, 4, 6, 8]\n     # NumPy example\n   print(np.arange(10)) # create array of 10 numbers from 0 to 9\n   print(np.arange(10, dtype='int8')) # create array of 10 numbers from 0 to 9 and with integers of 8 bites\n   print(np.arange(0, 10, dtype='int32')) # create array of 10 numbers from 0 to 9 and with integers of 32 bites\n   print(np.arange(0, 10, 2, dtype='int64')) # create array of 5 even numbers from 0 to 9. Stepwise operation  and with integers of 64 bites\n   [0 1 2 3 4 5 6 7 8 9]\n[0 1 2 3 4 5 6 7 8 9]\n[0 1 2 3 4 5 6 7 8 9]\n[0 2 4 6 8]\n  NumPy comes with its own attributes for checking the shape, data type, and dimensions\nof an array. This can come in handy when working working with images or ton investigate\nany data we might be given with little information about it.  When we use the attribute   .shape  on an array, we get back a   tuple  with the rows as\nthe first element and the columns as the second element. The   .dtype  attribute tells us\nthe data type of the array or matrix, and the   .ndim  attribute gives us the dimensions.     print(first_array)\n   [25 40 50 16 10 19 32 27 11]\n     print(first_array.dtype) # data type\n   print(first_array.shape) # (rows, columns)\n   print(first_array.ndim) # dimension\n   int64\n(9,)\n1\n  In addition to lists, tuples and sets can also be passed into a numpy array.     # this is a matrix of multiple lists\n   matrix_list = [[1, 2, 3, 4],\n                  [20, 22, 27, 15], \n                  [11, 14, 19, 7]]\n   print(matrix_list)\n   [[1, 2, 3, 4], [20, 22, 27, 15], [11, 14, 19, 7]]\n     matrix_array = np.array([[1, 2, 3, 4], # this is a matrix of multiple arrays\n                            [5, 6, 7, 8], # even though it looks just like a bunch of lists\n                            [9, 8, 7, 6]]) # it is treated differently by numpy\n   matrix_array\n     # this is a matrix created from the list of lists above\n   ndarray_lists = np.array(matrix_list)\n   ndarray_lists\n     print(matrix_list, type(matrix_list))\n   print(\"-\" * 40)\n   print(matrix_array, matrix_array.dtype)\n   print(\"-\" * 40)\n   print(ndarray_lists, ndarray_lists.dtype)\n     # this is a numpy matrix of multiple arrays with strings\n   \n   matrix_text = np.array([[\"Hi\", \"this\", \"is\", \"a\"],\n                            [\"matrix\", \"that\", \"only\", \"contains\"],\n                            [\"text\", \"and\", \"no\", \"numbers\"]])\n   matrix_text\n  If you are ever unsure of the number of dimmensions a numpy array has, you can always check this with the   .ndim  attribute. Also, whenever you see a list of lists, it helps to think of it as an Excel spreadsheet (i.e. a 2-dimensional grid of information).     print(matrix_text.ndim)\n  We can also select (i.e. slice and dice) elements from an array in the same fashion as with lists, but with an additional functionality that allows us to pinpoint exactly what we want by selecting columns and rows within the same bracket of an array in multiple dimensions.  Here is an example of how slicing data out of a numpy array works.    array  start🔚step, start🔚step  Let's see what the construct of   array[rows,columns]  looks like by starting with the rows first.     print(f\"Our first array --> {first_array} has {first_array.ndim} dimension(s)\")\n   print(\"-\" * 40)\n   print(f\"We can select one element --> {first_array[0]}\") # select an element first row\n   print(\"-\" * 40)\n   print(f\"More than one element with regular indexing --> {first_array[:5]}\") # select elements from 0 to 4\n   print(\"-\" * 40)\n   print(f\"And more than one element with negative indexing --> {first_array[2:-3]}\") # select elements from the 3rd to the third to last\n   # negative indexing starts from the end without using 0\n  Exercise 2   Create an array of with 30 even numbers of data type   int32  and assign it to a variable named   thirty_nums .  Select the last 10 elements of the array using a negative index in a slice and assign it to the variable   ten_nums .  Sum all 10 numbers and assign the result to a variable called   one_num .     # create the array\n     # create the slice\n     # sum the slice\n  Exercise 3   Create a Python list with 5 numbers.  Create a Python list with 5 words.  Create a NumPy matrix with the two lists above and assign it to a variable called   mixed_data .  Select a word from the matrix and change it to   ice cream . Assign it to a variable called   icy .  Select a number from the matrix and change it to 777. Assign it to a variable called   lucky_number .     # list of nums\n     # list of words\n     # mixed matrix\n     # change a word\n     # change a number\n  To select elements from a matrix we need to have a comma   ,  separating our data. The rows are on the left and the columns will be on the right. The colon   :  still allows us to select a range, and adding an additional colon   :  allows us to select elements with steps in between.  Being able to select columns becomes specially useful when we have different data in our arrays. Think about our shoping list again (a mix of arrays), when we go grocery shopping, we will need to select specific elements from our list as we go through the isles of the supermarket.      matrix[start🔚step, start🔚step]     matrix_array\n     print(matrix_array[1, 3]) # element on second row and second column\n     print(matrix_array[:, 3]) # all rows of column 2\n     print(matrix_array[:, :]) # all rows and columns i.e. a full copy\n     print(matrix_array[:, :2]) # everything in the first two columns\n  Let's clear a bit the distinction between the concepts we have covered. Selecting one element of an array/matrix can be considered indexing, and selecting a group of elements from an array/matrix can be thought of as slicing your data.  Before we jump into some exercises, let's go over a few more examples. We will start by createing a 5 by 5 matrix and call it   matrix_x .     matrix_x = np.array([[15, 41, 30, 11, 29],\n                       [22, 8, 13, 10, 14],\n                       [31, 19, 45, 16, 23],\n                       [17, 16, 33, 1, 34],\n                       [6, 27, 38, 25, 18]])\n   matrix_x\n  Let's figure out how to select the following rows and columns from Matrix X.       matrix_x\n     # Matrix X number 1\n   \n   # We want all rows in the first column\n   matrix_x[:, 0]\n     # Matrix X number 2\n   \n   # We want the first row and all columns\n   matrix_x[0, :]\n     # Matrix X number 3\n   \n   # We want the intersection of the first column and the first row\n   matrix_x[0, 0]\n  Let's make things a bit more interesting by scaling the complexity of how we slice our matrix X.       # Let's print matrix x again\n   matrix_x\n     matrix_x.shape\n     # Matrix X number 4\n   \n   # We want all columns and every other row\n   matrix_x[::2, :]\n     # Matrix X number 5\n   \n   # We want all rows and every other column\n   matrix_x[:, ::2]\n     # Matrix X number 6\n   \n   # We want every other row and column starting from the second one of each\n   matrix_x[1::2, 1::2]\n  You can also add values to these numbers and reasign the new values to a new variable.     temp_matrix = matrix_x[1::2, 1::2] + 5\n   temp_matrix\n  For our last example, let's explore how to get values at a specific point in our matrix.       # Let's print matrix x again\n   \n   matrix_x\n     # Matrix X number 7\n   \n   # We want the first, third, and fifth element of the first, third, and fifth column-row combination\n   matrix_x[[0, 2, 4], [0, 2, 4]]\n     # Matrix X number 8\n   \n   # We want the elements at the:\n   # second row and first column\n   # first row and second column\n   # fourth row and third column\n   # fifth row and fourth column\n   # third row and fifth column\n   \n   matrix_x[[1, 0, 3, 4, 2], [0, 1, 2, 3, 4]]\n     # Matrix X number 8\n   \n   matrix_x[-1, []]\n       Exercise 4   Create a matrix with 4 rows and 5 columns.  Select the first and last columns and add the result to a new variable.  Add 5 to the first column and multiply the second by 10.            Exercise 5   Create a matrix with 6 rows and 3 columns.  Select the first, third, and fifth rows.            Exercise 6   Create a matrix with 5 rows and 9 columns.  Select every third column starting from the first one.            3. Generating Data With NumPy    We have already seen how to create numpy arrays of different dimmensions, however, numpy has even more functionalities that allow us to generate data on the fly either as a one-dimensional array or multiple.  Here are some of the most important functions to generate data with numpy.    np.arange(start, end, step, dtype='something')  will create a one-dimensional array   .reshape(4, 5)  this method can be applied to almost any array and it will reshape it into a matrix or other n-dimensional array   np.zeros(nums, dtype='something')  creates an array full of zeros   np.ones((rows, columns), dtype='something')  creates an array/matrix full of ones   np.random.random(num)  creates an array of random values from 0 to 1. If passed in a tuple such as (3, 5), it will crate a matrix of random values   np.linspace(start, end, nums_in_between)  creates an array of linearly-spaced numbers from the start point to the end with a user-defined amount of numbers in between   np.random.normal(mean, std, size)  draws random numbers from a normal distribution with the mean, standard deviation, and size of your choosing   np.random.randint(start, end, shape)  creates random integers   np.full(shape, num_you_like)  creates an array or matrix with the same number all accross   np.empty()  creates a completely empty array of a size and shape of your choosing  You might be wondering, why would I use all of these data generating functions if I will always have data for my use case? The answer is that no model is perfect, and sometimes, a bit of simulation can save you a lot of time in your work or within a project. In addition, there are analyses that are way too costly or harmful to run a test on and collect new data from. For example, flight routes, weather, car crashes, etc., and simulating scenarios in these use-cases would save a lot of time, money, effort, and even lives.  Let's go over some examples of each.     import matplotlib.pyplot as plt\n   \n   %matplotlib inline\n     # let's create a one dimmensional array\n   arr1 = np.arange(0, 50, 0.5, dtype='float')\n   arr1\n     # we will plot our array against the same array but with every element raised to the power of 2\n   plt.plot(arr1, (arr1**2))\n   plt.show()\n  Let's now turn our array into a matrix while keeping in mind that the product of the rows and columns of a matrix should be equal to the lenght of the array. Hence, if our array has 50 values, our columns times our rows has to be equal to 50 (10x5, 2x25).     len(arr1)\n     matrix1 = arr1.reshape(10, 10).copy()\n   matrix1\n     arr1.reshape(20, 5)\n     arr1.reshape(5, 20)\n     arr1.reshape(25, 4)\n  You might be wondering why we used the   .copy()  above and the reason is that the data numpy keeps in memory for any given array doesn't change when the pointer changes (i.e. a new variable assignment) unless we create a copy. So essentially arr1 and matrix one would be sharing the same data and whathever happens to matrix1 would happen to arr1 and vice-versa.  Let's visualise this with an example.     # Let's create an array with 10 zeros\n   zeros_arr = np.zeros(10, dtype='int8')\n   zeros_arr\n     print(f\"Here is the original array --> {zeros_arr}\") # here is the original array\n   print('-' * 55)\n   \n   new_zeros = zeros_arr # here is our new copy\n   \n   new_zeros[4] = 5 # let's change the value at index 4\n   \n   print(f\"Here is the new, altered array --> {new_zeros}\") # let's print the modified one\n   \n   print('-' * 55)\n   \n   print(f\"Here is the original again --> {zeros_arr}\") # and let's print the original as well\n     new_zeros = zeros_arr.copy()\n   new_zeros[4] = 10\n   new_zeros\n     zeros_arr\n  Now that we know that arrays can be fixed in memory and shared with other varables, or be copied to another variable. Let's continued exploring our data generation functions.     # the ones and only : )\n   matrix2 = np.ones((4, 5), dtype='float32')\n   matrix2\n     # have now become the lucky seven's\n   matrix2 + 6\n     # how about having lot's of tiny spaces in between two numbers\n   arr2 = np.linspace(-10, 10, 100)\n   arr2\n     # what would the cosine of our previous array and the original array look like when plotted\n   plt.plot(arr2, np.cos(arr2))\n   plt.show()\n     # everyone needs random integers every now and then\n   # from 10 to 50 give me 100 randoms\n   arr3 = np.random.randint(low=0, high=50, size=100, dtype='int8')\n   arr3\n     plt.plot(sorted(arr3))\n   plt.show()\n     # distributions are everywhere and the normal ones tend to follow us (hint: the weather 🌤🌧🌞)\n   arr4 = np.random.normal(loc=12, scale=6, size=200)\n   arr4\n     plt.hist(arr4, bins=30, color='green')\n   plt.show()\n     # these are completely empty even though they appear not to be\n   arr5 = np.empty((3, 5), dtype='float32')\n   arr5\n     # to look at the shape of this array we use\n   arr5.shape\n     # we can fill in the rows and/or columns with new values\n   arr5[0] = (5**2) # first row will have 25s\n   arr5[0]\n     arr5[1] = (25 / 5) # second row will have 5s\n   arr5[2] = 17 # third row only 17s\n   arr5\n     # Let's create a matrix with a number we like\n   filled_matrix = np.full(shape=(3, 5), fill_value=17, dtype='int8')\n   filled_matrix\n  Exercise 7   Create an empty matrix of 3 by 5 dimensions. Add it to the variable   empty_matrix .  Add positive numbers to the odd columns.  Add negative numbers to the even columns.     # create a 3 by 5 matrix\n     # add positive numbers to the odd columns\n     # add negative numbers to the even columns\n  Exercise 8   Create a one dimensional array of 80 random numbers of type   float32 . Call it   my_randoms .  Reshape the array into a matrix of at least 8 rows. Call it   rand_matrix .  Create a slice of the even columns and odd rows. Call it   silly_slice .                 4. Computations with NumPy    Let's begin this section with a quick comparison between operations on regular Python lists and operations on NumPy arrays.     # let's create a list with 10M numbers\n   comp = list(range(10_000_000))\n     %%time\n   \n   # let's add 1 to each and then time it\n   some_list = []\n   for i in comp:\n       some_list.append(i + 1)\n     # let's create a numpy array of 10M\n   np_array = np.arange(10_000_000)\n     # let's make sure our arrays have the same amount of elements\n   assert len(np_array) == len(comp), \"They don't match!\"\n     %%time\n   \n   # let's add 1 to each one of our numpy arrays\n   new_arr = np_array + 1\n  As you can see, the difference in speed is stark. Try to play with the 0s and see how things evolve with these 2 computations.  4.1 Operations    To be able to perform element-wise operations with 2 or more lists we need to do it either in a loop, a list comprehension, or add the numbers manually. With numpy, you can do element-wise operations without ever writing a loop, so long as your arrays are always of the same length or have a matching structure for between matrices and arrays. This last part is out of the scope of this course but it is important to learn about n-dimensional array operations to do more advanced work as you progress throughout your career.  Let's begin by adding two arrays.     array1 = np.array([3, 2, 1])\n   array2 = np.array([9, 5, 7])\n   \n   array1 + array2 # this addition will happen element-wise\n  What happens when we try to add python lists with other lists?     l1 = [3, 2, 1]\n   l2 = [9, 5, 7]\n   \n   l1 + l2\n  The correct way to do it with lists would be,     l3 = []\n   for idx in range(len(l1)):\n       l3.append(l1[idx] + l2[idx])\n   \n   print(l3)\n  Or,     l3 = [l1[idx] + l2[idx] for idx in range(len(l2))]\n   l3\n  Notice what happens when we try to add two arrays with different lenghts.     # If the arrays do not have the same lenght, numpy will give us an error\n   array1 = np.array([3, 2, 1, 6])\n   array2 = np.array([9, 5, 7])\n   \n   array1 + array2\n  If the arrays don't have the same length, numpy will give us an error.  We can also do element-wise operations on arrays using the rest of the mathematical functions we saw on lesson 1.     # let's create 2 more arrays\n   array3 = np.array([3.7, 2.3, 1.8])\n   array4 = np.array([9.1, 5, 7.4])\n     print(array3 * array4) # multiplication\n     print(array3 / array4) # division\n     print(array3 ** array4) # exponent\n     print(array3 - array4) # subtraction\n  You also get the same functionality with a single number, only this time the single number will be applied to all elements in your array. This is also called broadcasting, and it is what we saw in the speed comparison at the beginning of this section.     # notice the 10\n   print((array3 * array4) * 10) # multiplication\n     print((array3 / array4) / 20) # division\n     print((array3 ** array4) ** 2) # exponent\n     print((array3 - array4) - 4) # subtraction\n     print(array3 * 3) # multiplication\n  Can you guess what would happen to a python lists when we try to multiply it by a scaler?     [1, 2, 3] * 3\n     print(array3 / 10) # division again\n     print(array3 ** 3) # exponentiation again but with 1 number only\n     print(array3 - (-20)) # subtraction\n     auto_array_1 = np.random.randint(low=5, high=20, size=40)\n   auto_array_1\n  If we wanted to make the one dimensional array above a two-dimensional one, e.g. a matrix, we could do it with the method   .reshape()  we say earlier, which takes as arguments the dimensions our new matrix should have. Please remember, the first number is always the rows and the second is always the columns.  What we have to keep in mind though is that the reshaped version has to match the amount of element in the array. For example, a one dimensional array with 20 elements can only be reshaped into a matrix of:   2 rows and 10 columns (2, 10)  10 rows and 2 columns (10, 2)  4 rows and 5 columns (4, 5)  5 rows and 4 columns (5, 4)     len(auto_array_1)\n     auto_array_1.shape\n     print(f\"New matrix: \\n {auto_array_1.reshape(5, 8)}\")\n   print(f\"Same array: \\n {auto_array_1}\")\n  The   .reshape()  method does not work inplace so we would have to assign the new matrix to a new variable and chain the method   .copy()  to it if we want our reshaped matrix to be unique.     auto_matrix_1 = auto_array_1.reshape(5, 8).copy()\n   auto_matrix_1\n  The opposite to reshape would be   .ravel() . This method takes in a matrix of any shape and form and returns a 1-dimensional (flattened) array.     print(\"Original matrix --> {}\".format(auto_matrix_1))\n   print(\"-\" * 45)\n   auto_array_2 = auto_matrix_1.ravel().copy()\n   print(\"New, unraveled array --> {}\".format(auto_array_2))\n  4.2 Descriptive Statistics on Arrays     Illustration Source:    http://www.lindavanbruggen.nl/  Now that we have learned how to create arrays, there are many operations we can do on arrays to try to understand the information they contain. For example, we can add all of their numbers, get their range, max and min values, and even learn how many even/odd numbers are in the array.  The kind of operations described above can also be referred to as descriptive statistics, which one of the branches of the field of Statistics. It gives us tools to interrogate the data and pull out some facts from it.  You can also think of descriptive statistics a regular analysis of data where no predictions of fancy modeling takes place. The importance of descriptive statistics in the data analytics/science cycle is monumental. It helps us examine our data and, more often than not, find inconsistencies that we had not previously encountered and will have to deal with.  Some useful statistics you should always begin with are:   Minimum  25th percentile  Median  Mean  Mode  75th percentile  Maximum  Count  Standard deviation  Variance     auto_array_1\n     print(f\"Minimum Value: {np.min(auto_array_1)}\")\n     print(f\"25th percentile: {np.percentile(auto_array_1, 25)}\")\n     print(f\"Median: {np.median(auto_array_1)}\")\n     print(f\"Average: {np.mean(auto_array_1)}\")\n     print(f\"75th percentile: {np.percentile(auto_array_1, 75)}\")\n     print(f\"Maximum Value: {np.max(auto_array_1)}\")\n     print(f\"Count: {auto_array_1.size}\")\n     print(f\"Total Sum: {np.sum(auto_array_1)}\")\n     print(f\"Square root of the array: {np.sqrt(np.sum(auto_array_1))}\")\n     print(f\"Sandard Deviation: {np.std(auto_array_1)}\")\n     print(f\"Variance: {np.var(auto_array_1)}\")\n  Exercise 9   Create an array of 100 random numbers between 2 numbers of your choosing.  Build a function that takes in an array and prints at least 5 of the descriptive statistics shown above. Name it   clue_finder .  Test your function on the array from step 1.     # your random array\n     # your function\n     # test your function with your array\n  Exercise 10   Using the array you created in exercise 9, reshape it into a matrix of dimensions (6, 8).  Sum every second column of that matrix matrix with a numpy function.     # reshape your array here\n     # sum every second column here\n  4.3 Combining Arrays  Sometimes, we will need to combine different arrays containing valuable information for a particular analysis. NumPy has three particular ways in which it allows us to do this:    np.vstack()  allows you to stack multiple arrays on top of one another, i.e. vertically or row-wise   np.hstack()  allows you to stack arrays next to each other, i.e. horizontally or column-wise   np.concatenate()  gives you both stacking options, vertical and horizontal stacking  Let's have a look at all three of these.       x = np.arange(0, 20).reshape(5, 4)\n   y = np.linspace(0, 9, 20).reshape(5, 4)\n   z = np.ones(20).reshape(5, 4)\n   \n   print(\"Here is the row length of \\n x --> %i, \\n y --> %i, and \\n z --> %i\" % (len(x), len(y), len(z)))\n     # Horizontal Stack of x and y\n   h_level1 = np.hstack([x, y])\n   h_level1\n     # horizontal stack of all three but with an operation on the third\n   h_level2 = np.hstack([h_level1, z + 2])\n   h_level2\n     # vertical concatenation\n   v_level1 = np.vstack([x, y])\n   v_level1\n     # vertical concatenation with an operation on z\n   v_level2 = np.vstack([v_level1, z + 7])\n   v_level2\n     # horizontal concatenation\n   c_level1 = np.concatenate([x, y, z], axis=1)\n   c_level1\n     # vertical concatenation\n   c_level2 = np.concatenate([x, y, z], axis=0)\n   c_level2\n  5. Masking    In the previous lesson, we explored conditional statements with if, elif's, and else constructors and noticed how powerful these functionalities can be when we try to evaluate statements based on logical conditions. NumPy allows us to take advantage of these logical statements to create powerful filtering methods.  One of the most common tasks of a data analyst is to dig deep into the data to figure out interesting relationships and patterns that could help us answer important questions. One way to achieve this is through filtering out data based on a given condition or set of conditions. NumPy allows us to do this quite efficiently and we will learn how to do this in this section.  Say we have an array of 1 year worth of temperatures in farenheit and we want to know how many days of the year the temperature was higher than 70 (a nice and probably sunny day 🌤).     year_temp = np.random.randint(low=-30, high=115, size=360, dtype='int8')\n   year_temp[:20]\n  After we have our array of temperatures for the year, we can move on to creating the condition we want.     nice_days_mask = (year_temp > 70) \n   nice_days_mask\n  Notice that we now have an array of booleans where the True's corresponde to the days where the temperature was greater than 70 and the False's correspond to the days where it was equal to or lower than 70. Now, if we pass our new variable of booleans as an index to our   year_temp  array, we will get back only the values we want (i.e. the ones that evaluated to True in the condition above).     nice_days = year_temp[nice_days_mask]\n   print(nice_days.shape, '\\n\\n', nice_days)\n  Now we can move on to answer some useful questions about our new filtered data. For example, we can examine the max temperature experienced during the nice days, or we could check out the average to understand what was the most common temperature during these days.     print(\"The Average temperature on nice days was %i F ☀️\" % np.mean(nice_days))\n     print(\"The Max temperature on nice days was %i F 😎\" % np.max(nice_days))\n     print(\"The Min temperature on nice days was %i F 🌥\" % np.min(nice_days))\n  You do not need to create these computation and assign them to a new variable, instead, you can pass them directly to the array in question the same way you would do your regular slicing. Let's look at an example together.     year_temp[0:30] # regular slicing\n     year_temp[year_temp \u003C 80] # masking directly\n  We can also combine multiple logical evaluations and pass them as masks to our arrays. For example, imagine we are only interested in the days where the temperature was less than 80 and greater than 50. If so, we could wrap our comparisons around paretheses (i.e. round brackets) and separate them with the   &  or   |  objects in between two or more masks.     # get only the days where the temp was less than 80 and greater than 50\n   year_temp[(year_temp \u003C 80) & (year_temp > 50)]\n  Lastly, we could create a matrix resembling the days and months for the 360 (data points) days we have in our array. We could then use one of the many numpy aggregation functions to understand what happened in any given months. (We are simulation the weather in a year 😎.)  First, let's reshape this array into a matrix with 30 rows to represent the days in a month and 12 columns to represent the months in a year.     matrix_year = year_temp.reshape(30, 12).copy()\n   matrix_year\n  The next step is to create our mask. We are interested in what we consider nice days. Which are days that are not too hot but also not too cold (e.g. not hotter than 100 degrees but also not colder than 70).     # create the mask\n   good_days = (matrix_year > 70) & (matrix_year \u003C 100)\n   good_days\n  Lastly, we will use the   np.sum(matrix, axis=0)  function while making the axis parameter equal to 1 so that numpy knows that we want the operation to pass through the mask matrix column-wise and not row-wise. That way we can count how many days per month throughout the year were within our criteria.     np.sum(good_days, axis=0)\n  There are many other cool tricks you can do with with masking, and we will explore those as we progress throghout the course.  Exercise 11   Go to   yahoo finance , pick a company you like and figure out the minimum and maximum prices it traded for in the last year.  Create an array of 240 random floats with the minimum and maximum values determined in step 1.  Reshape your array into a matrix with 12 columns.                 Exercise 12   Get the average price of each month on the matrix from exercise 11.  Create a mask to determinte the numbers between the 25th and 75th percentiles of the original 1-dimensional array not the matrix.  Test your mask in your 1-dimensional array with stock prices.                 Congratulations you are more and more resembling a full-fledged data analyst.  5. Summary  You have learned a great deal today and should be proud of your accomplishments. Let's recap what we have seen thus far.   Lists in Python can be treated as the arrays and matrices that will hold our data for us. They are extremely powerful and versatile data structures and can be used in almost every aspect of the data analytics cycle.  NumPy is a library built on top of the programming language C. This particular characteristic allows it to communicate with the hardware of our machines and run our computations faster than with regular Python code.  When possible, use broadcasting instead of loops in order to optimise your code and run-time.  Generating random data allows us to test models and functions very fast and numpy has a lot of useful methods to generate random data. It has functions such as   np.ones ,   np.random.random ,   np.linspace , and many more.  Masking is a type of filtering method that allows us to have a closer look at our data. It is, in a way, a similar way of constructing if-else evaluations.  List comprehensions are a type of for loop that gives us the ability to generate a list from repeated commands.  6. References  Sweigart, Al.   Automate the Boring Stuff with Python: Practical Programming for Total Beginners . No Starch Press, 2020.  VanderPlas, Jake.   A Whirlwind Tour of Python . O'Reilly, 2016.  VanderPlas, Jake.   Python Data Science Handbook . O'Reilly, 2017.  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":104,"path":105,"dir":81,"title":106,"description":107,"keywords":108,"body":112},"content:2.analytics:3.structured_data.md","/analytics/structured_data","Intro to pandas","Source: https://analyticsindiamag.com/",[109,110,111,86,87],"Outline for this Noteboook","DataFrames and Series in pandas","Exercise 1","  Intro to pandas     Source :   https://analyticsindiamag.com/  Outline for this Noteboook   Arrays, Matrices, and NumPy Recap  Learning Outcomes for the Module  Introduction to pandas  Series  DataFrames  Indexing - Accessing and Selecting Data with pandas  1. Arrays, Matrices, and NumPy Recap  In the last lessons we covered,   Lists methods and how in Python they can be treated as arrays and matrices that will hold our data for us in the same way as rows and columns do in Excel. Lists are extremely powerful and versatile data structures and they can be used in almost every aspect of the data analytics cycle.    In Python     my_array = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n    In Spreadsheets \n|  | A |\n|:--------:|:--------:|\n| 1 | my_array |\n| 2 | 10 |\n| 3 | 20 |\n| 4 | 30 |\n| 5 | 40 |\n| 6 | 50 |\n| 7 | 60 |\n| 8 | 70 |\n| 9 | 80 |\n| 10 | 90 |   NumPy is a library for fast computations on arrays and matrieces, and it is built on top of the C and Fortran programming languages.  Where possible, we can take advatage of broadcasting instead of using loops to apply an operation to every element in an array.    With Python Lists     a_list = []\n   \n   for num in lots_of_nums_list:\n       a_list.append(num + 5)\n    With NumPy     new_array = nums_numpy_array + 5\n   Generating random data allows us to test models and functions and numpy has a lot of functions to help us generate random data. Some of the functions are   np.ones ,   np.random.random ,   np.linspace , and many more.  Masking is a type of filtering method that allows us to slice and dice the data given a condition or a set of them. It is, in a way, similar to constructing if-else statements with regular Python code.     import numpy as np\n   \n   # in the thousands\n   habitants_per_state = np.array([1000, 700, 1100, 500, 300, 450, 640])\n   \n   high_population_mask = (habitants_per_state > 600)\n   \n   habitants_per_state[high_population_mask] # returns array([1000, 1100, 700, 640])\n   List comprehensions are a type of for loop that gives us the ability to generate a list from repeated instructions. The main differences between loops and list comprehensions is that in the former, the action takes place after defining the for loop while in the latter, the action takes place at the beginning.     ## for loops\n   \n   a_new_list = []\n   \n   for a_element in some_list:\n       a_new_list.append(a_element + 2)\n       \n   ## lists comprehensions\n   a_new_list = [a_element + 2 for a_element in some_list]\n  2. Learning Outcomes  In this module you will,   Learn how to create and load datasets to Python using the pandas library  Learn how to manipulate different datasets  Learn how to clean and prepare data for analysis  Understand why data preparation is one of the most important steps in the data analytics cycle  3. Introduction to pandas     Source :   https://pandas.pydata.org/   pandas  is a Python library originally developed with the goal of making data manipulation and analysis in Python easier. The library was created by Wes McKinney and collaborators, and it was first released as an open source library in 2010. It has been designed to work (very well) with tabular data. In essence, pandas gives you, in a way, the same capabilities you would get when working with data in tools such as Microsoft Excel or Google Spreadsheets, but with the added benefit of allowing you to use and manipulate more data.  The pandas library is also built on top of NumPy, this means that a lot of the functionalities that you learned in the previous module will transfer seamlessly to this lesson and this new tool we are about to explore. What you will find in pandas is, the ability to control your NumPy arrays as if you were viewing them in a spreadsheet.  Some of pandas main characteristics are:   Straightforward and convinient way for loading and saving datasets from and into different formats  Swiss army knife for data cleaning  Provides the same broadcasting operations as NumPy, hence, were possible, avoid loops...  Allows for different data types and structures inside its two main data structures, Series and DataFrames  Provides functionalities for visualising your data  pandas, like NumPy, also has a industry standard alias that we will be using throughout the course. This library is usually imported as   pd .     import pandas as pd\n  Just like NumPy has the very efficient data structure called   ndarray 's, pandas, as NumPy's child, has its own data structures called   DataFrame s, which are the equivalent of a NumPy matrix with many more functionalities, and   Series  (the equivalent of a NumPy array). We will cover these two structures next.   Warning:  It is possible that the control boost you will feel as you begin to learn how to use pandas to clean, manipulate, and analyse data, will prevent you from going back to using the tools you have been using in the past (e.g. Excel, Google Sheets, regular calculators, etc.). 😎  DataFrames and Series in pandas    Before we are able to import data into Python from outside sources, we'll walk over how to transform existing data (i.e., data we will come up with), into the two main data structures of pandas,   DataFrame s and   Series . We will do so through several different avenues, so let's first talk about what   DataFrame s and   Series  are.  A   DataFrame  is a data structure particular to pandas that allows us to work with data in a tabular format. You can also think of a pandas DataFrames as a NumPy matrix but with (to some extent and depending on the user) more flexibility. Some characteristics of   DataFrame s are:   they have a two-dimesional matrix-like shape by default but can also handle more dimensions (e.g. with a multilevel index)  their rows and columns are clearly defined with a visible index for the rows and names (or numbers) for the columns  pivot tables, which are one of the main tools of spreadsheets, are also available in pandas  lots of functionalities for reshaping, cleaning, and munging the data  Indexes can be strings, dates, numbers, etc.  very powerful and flexible   .groupby()  operation  A pandas   Series  is the equivalent of a column in a pandas   DataFrame , a one-dimensional numpy array, or a column in Excel. In fact, since pandas derives most of its functionalities from NumPy, you can tranform a Series data structure (and also a   DataFrame ) back into a NumPy array (or matrx) by adding the attribute   .values  to it. A Series has most of the functionalities you will see in a   DataFrame  and they can be concatenated to form a complete   DataFrame  as well.  Let's first start by importing   pandas  with its industry alias,   pd , and then check the version we have installed.   Note:  At the time of writing, the latest version of pandas is 1.1.4.     import pandas as pd\n   import numpy as np\n     pd.__version__\n  4. Series  Let's create some fake data first and turn it into a pandas   Series . We will do so in the following ways:   with lists  with NumPy arrays  and with a dictionary containing lists and tuples  Say we have data for a large order of pizzas we purchased a while back for a friends gathering. We ordered the pizzas from different stores and now we would like to have a look at the quantity we order using a pandas   Series  and assign it to a variable for later use. Let's see what the data looks like first in a list.     # This will be your fake pizza data representing amount of pizzas purchased\n   [2, 1, 6, 5, 1, 4, 2, 6, 2, 1]\n  To create a pandas Series we use the   pd.Series(data= , name=)  method, pass our data through the   data=  parameter and give it a name using the   name=  parameter (the   name  parameter is optional though).     # This will be your first pandas Series\n   first_series = pd.Series(data=[2, 1, 6, 5, 1, 4, 2, 6, 2, 1], name='pizzas')\n   first_series\n  Notice that when we visualize a pandas Series we can immediately see the index next to the array.  We can also use NumPy arrays for the data we pass into our Series. As noted earlier, while using pandas we are essentially using NumPy data structures under the hood.     second_series = pd.Series(data=np.arange(20, 30))\n   second_series\n  Another neat functionality of Series is that they are not bound to only having a numerical index, unlike lists and NumPy arrays. Let's look at an example where we add our own index to a pandas Series. Note that indexes can be strings and dates as well.     third_series = pd.Series(data=np.arange(8), \n                             index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], \n                             name='random_data')\n   third_series\n     third_series['d']\n  In the previous module, we spent quite some time on NumPy because it is a great segway into pandas since a lot of the methods, and the slicing and dicing techniques you've already learned, will be applicable to pandas data structures as well. For example, broadcasting operations over an entire array, instead of using a loop, are perfectly doable operations with pandas Series.     # add 5 to every element in our second_series\n   second_series + 5\n     first_series\n     # raise every element to the power of 3\n   first_series ** 3\n  Keep in mind though that, just like with NumPy arrays, when we broadcast an operation on a pandas object, the change won't happen inplace so we would have to assign the changed object to a new variable, or back into the original one, to keep the changes in memory.     # the Series did not keep the changes\n   print(first_series)\n     # now the Series will keep the changes\n   first_series = first_series ** 3\n   print(first_series)\n  It is worth mentioning again that if you would like to access the NumPy   ndarray  data structure underneath a pandas Series, you can do so by calling the attribute   .values  on the Series. For example:     first_series, first_series.values\n     type(first_series), type(first_series.values)\n  We can also use a dictionary to create our pandas Series. The only caveat is that since key-value pairs can contain a lot of data, we have to explicitely call out the data we want in the rows by using the name of the key on the dictionary. If we do not select the key for the data we want, it would assign the key to the index of the Series and the values to the corresponding elements of such keys. The result won't be any better than using the regular dictionary itself unless, of course, there is a need for this kind of structure.  Let's look at an example.     first_series\n     pizzas = {'pizzas': [2, 1, 6, 5, 1, 4, 2, 6, 2, 1]}\n   # Not good, one index only\n   fourth_series = pd.Series(data=pizzas)\n   fourth_series\n  Notice that what we got back was a 1-element pandas Series where the key   pizza  is now the index and the amount of pizzas we purchased are, still as a list, represented as 1 element. To fix this, let's explicitly call out the values of our   pizza  key.     # good example\n   fourth_series = pd.Series(data=pizzas['pizzas'])\n   fourth_series\n  In some instances we might want to use the default behavior. For example, when we have one key mapping to one single element in a dictionary.     states_city = {\n       'NSW':'Sydney',\n       'VIC':'Melbourne',\n       'SA':'Adelaide',\n       'TAZ':'Hobart',\n       'WA':'Perth',\n       'QLD':'Brisbane',\n       'NT':'Darwin',\n       'ACT':'Canberra',\n   }\n     # this is a nice example\n   sc_series = pd.Series(states_city)\n   sc_series\n  Tuples work in the same way as lists when we pass them into a pandas Series, but be careful with sets though. Since sets are moody and don't like order, pandas cannot represent their index well and thus, is unable to build Series or DataFrames from them.     # this works well\n   some_tuple = (40, 3, 2, 10, 31, 29, 74)\n   pd.Series(some_tuple, index=list('abcdefg'))\n     # this does not work\n   some_set = {40, 3, 2, 10, 31, 29, 74}\n   pd.Series(some_set, index=list('abcdefg'))\n  Exercise 1   Create a pandas Series of 100 elements using a linearly spaced array from 50 to 75. Call it   my_first_series  and print the results.  Multiply the array by 20 and assign it to a new variable called,   my_first_broadcast . Print the results.            Exercise 2   Create a pandas Series from an array of integers from 100 to 500 in steps of 50, and with a index made up of letters. Call it   first_cool_index  and print it.  Do a floor division by 11 on the entire array, assign the result a variable called   low_div  and print the result.            Exercise 3   Create a pandas Series of 7 elements from a dictionary where the keys are a sport and the value is a famous player in that sport. Call the Series   sports_players  and print it.            5. DataFrame's    You can think of a pandas DataFrame as a collection of Series with the difference being that all of the values in those Series will share the same index once they are in the DataFrame.  Another distinction between the two is that you can have a DataFrame of only one column, but you cannot have a Series of more than one (or at least you shouldn't since that is what the DataFrame is for).  Let's now create some fake data and reshape it into a pandas   DataFrame  object. We will do so in the following ways:   a dictionary object with lists and tuples  lists and/or tuples  NumPy arrays  multiple pandas Series  One of the fastest and more common ways to construct a DataFrame is by passing in a Python dictionary to the   data=  parameter in the   pd.DataFrame()  method. Doing this with dictionaries can save us time with having to name each one of the columns in our DataFrame.     # Create a dictionary of fake pizza data\n   data_le_pizza = {\n       'pizzas': [2, 1, 6, 5, 1, 4, 2, 6, 2, 1], # some fake pizzas purchased\n       'price_pizza': (20, 16, 18, 21, 22, 27, 30, 21, 22, 17), # some fake prices per pizza \n       'pizzeria_location': ['Sydney', 'Sydney', 'Seville', 'Perth', 'Perth', 'Melbourne',\n                             'Sydney', 'Seville', 'Melbourne', 'Perth']\n   }\n   \n   data_le_pizza\n     # Check the data in the dictionary\n   data_le_pizza['pizzas']\n     # remember the get method\n   data_le_pizza.get('price_pizza', 'not here')\n     # we ordered international pizza, literally : )\n   data_le_pizza['pizzeria_location']\n     # we can pass in the dictionary as it is\n   df_la_pizza = pd.DataFrame(data=data_le_pizza)\n     df_la_pizza.head()\n  Notice how our new object, the pandas DataFrame, resembles the way we would see data in a spreadsheet. In addition, the keys of our dictionary map perfectly to the dataframe column names and the values to, well, their respective columns.  You can access the data inside your new DataFrame by calling the names of your columns as attributes (like a method without the round brackets or parentheses) or as a key in a dictionary. Note though that you can only access the columns of a dataframe as if it were an attribute if the name of the column has no spaces in it, otherwise, it can only be accessed as a key inside a dictionary.     # access the pizzas column as an attribute\n   df_la_pizza.pizzas\n     # access the pizzas variable as the key in a dictionary\n   df_la_pizza['pizzas']\n  You can broadcast operations to an entire column the same way you did with the Series in this lesson and the NumPy   ndarray s in the previous module.     df_la_pizza['pizzas'] + 2\n  You can also add the values to an entire DataFrame or subsection of it, although this might not be possible or desirable if all of the columns contain different data types, but it is still good to know that you can. For example, the following code will give you an error because there is a column with string data types in it, but the subsequent one, the group of numerical columns, won't.     df_la_pizza + 2\n     df_la_pizza[['pizzas', 'price_pizza']] + 2\n  DataFrames have several useful attributes such as   .index  and   .columns  that allows us to retrieve these pieces of information from the dataframe.     # shows us the start, stop, and step of our DataFrame's index, a.k.a. the range of the index\n   df_la_pizza.index\n     # shows the names of the columns we have in our DataFrame\n   df_la_pizza.columns\n  We can also add new columns by passing in the name of the new column as a key just like in a dictionary, and the corresponding values as an operation after the equal sign. The kind of assignment identical to that used when creating new variables.     df_la_pizza['new_pizzas'] = df_la_pizza['pizzas'] * 3.5\n   df_la_pizza\n  pandas also gives us the option of naming the set of columns we have as well as the index column of our DataFrame. We can do this by calling the sub-attribute   .name  on the   .columns  and   .index  attributes of our DataFrame. Let's name our columns array,   pizza_attr , for pizza attributes, and let's name our index array,   numbers , to see this functionality of pandas in action.     df_la_pizza.columns.name = 'pizza_attr'\n   df_la_pizza.index.name = 'numbers'\n   df_la_pizza\n  Notice how the new element assignment happened in place and now our DataFrame displays even more information than before. In addition to giving the set of columns a name, we can also rename a particular columns or columns ourselves if we wanted to with the method   .rename() . Which takes in a dictionary with the old column name as the key and new one a the value.     df_la_pizza.rename(mapper={'new_pizzas': 'New Pizzas'}, axis=1, inplace=True)\n   df_la_pizza\n  Let's unpack what just happened.   the   mapper=  argument takes in a dictionary with the old column name as the key and new column name as the value  the   axis=  argument set to one indicates that we want the change to happen in the columns. The other option, which is the default as well, applies to the rows  the   inplace=True  argument tells Python to keep the changes in the dataframe so that we don't have to reasign the dataframe back to its original variable  If we wanted to get rid of a column we don't need or want anymore, we can use   del  call of Python, just like we saw in the chapter of lists, arrays, and matrices in lesson 2.  For illustration purposes, let's delete the   New Pizzas  column we just renamed.     del df_la_pizza['New Pizzas']\n   df_la_pizza # notice that the column is now gone\n  Let us look at how to convert a list of lists and tuples into a pandas DataFrame. We will first create a list called   la_pizzas  with lists and tuples, and then pass this matrix into our DataFrame constructor.     la_pizzas = [[2, 20, 'Sydney'],\n               [1, 16, 'Sydney'],\n               (6, 18, 'Seville'),\n               [5, 21, 'Perth'],\n               [1, 22, 'Perth'],\n               (4, 27, 'Melbourne'),\n               [2, 30, 'Sydney'],\n               (6, 21, 'Seville'),\n               [2, 22, 'Melbourne'],\n               [1, 17, 'Perth']]\n   la_pizzas\n     df_one = pd.DataFrame(data=la_pizzas, \n                         columns=['pizzas', 'price_pizza', 'pizzeria_location'])\n   df_one\n  As you can see, because we didn't have any column names this time, we had to use the   columns=  argument with a list of the strings for the names we will like our columns to have. Otherwise, pandas would have numbered the column names and you could only imagine how difficult it might be to figure out the content of a column without a name on a lengthy dataset.  We can also add completely new lists to our existing DataFrame, and pandas will match the index of each element in our new list with the index of each element in our DataFrame. Let's see this in action.     new_pizza_code = list(range(20, 40, 2))\n   new_pizza_code\n     df_one['new_pizza_code'] = new_pizza_code\n   df_one\n  If the length of a list does not match that of our DataFrame, pandas will throw an error at us for the mismatched lenght.     another_list = list(range(40, 55, 2))\n   df_one['another_list'] = another_list\n   df_one\n  Now let's see how to use numpy arrays and matrices to create a DataFrame. Let's begin with a matrix.     # We first create our la_pizza numpy matrix\n   \n   la_pizza_np = np.array([[2, 20, 'Sydney'],\n                           [1, 16, 'Sydney'],\n                           [6, 18, 'Seville'],\n                           [5, 21, 'Perth'],\n                           [1, 22, 'Perth'],\n                           [4, 27, 'Melbourne'],\n                           [2, 30, 'Sydney'],\n                           [6, 21, 'Seville'],\n                           [2, 22, 'Melbourne'],\n                           [1, 17, 'Perth']])\n   \n   la_pizza_np\n     # then we pass the matrix into the pd.DataFrame method and provide a list of names for the columns\n   \n   df_np_pizza = pd.DataFrame(la_pizza_np, columns=['pizzas', 'price_pizza', 'pizzeria_location'])\n   df_np_pizza\n  Another cool inherited trait from NumPy is that we can use the descriptive attributes we learned about in the previous lesson, which are   .dtypes ,   .shape , and   .ndim .     # notice the shape of our new dataframe\n   \n   df_np_pizza.shape\n     # check the types\n   df_np_pizza.dtypes\n     df_np_pizza = df_np_pizza.astype({'pizzas':int, 'price_pizza':float})\n     df_np_pizza.dtypes\n     # check the dimension\n   df_np_pizza.ndim\n  It is important to note that creating a matrix where the row arrays represent three different columns is not the same as creating three arrays that represent three rows and and 10 columns. Our intuition might betray us in this instance.  Let's look at an example with fake weather data where we pass in three arrays to a NumPy array that should represent the same DataFrame as the one above, but with different data now, of course.     weather_np = np.random.randint(10, 45, 10)\n   weather_np\n     cities = ['Sydney', 'Sydney', 'Seville', 'Perth', 'Perth', \n             'Melbourne', 'Sydney', 'Seville', 'Melbourne', 'Perth']\n   cities\n     days = np.random.randint(10, 30, 10)\n   days\n     data_weather = np.array([weather_np,\n                            cities,\n                            days])\n   data_weather\n  Notice the shape of our new matrix. What do you think will happen when we pass it through our DataFrame constructor?     pd.DataFrame(data=data_weather, columns=['weather', 'cities', 'days'])\n  The tricky part of using NumPy arrays lies in that the arrays are interpreted as horizontal arrows, meaning, we would have 10 columns and 3 rows if we were to use our array with its current shape. You probably already noticed this by running the code above.  The solution is to transpose our matrix and shift the columns to the rows and the rows to the columns. NumPy provides a very nice way for doing this. By adding the method   .T  attribute at the end of any array or matrix you can transpose it into a different shape (e.g. reshape it).  Let's see what this looks like and then use it to create our new DataFrame.     # same list as before with the pizzas😎\n   \n   data_weather.T\n     pd.DataFrame(data=data_weather.T, columns=['weather', 'cities', 'days'])\n  Lastly, imagine we had several pandas Series representing different values but with similar indexes. If we wanted to combine all of these into a single DataFrame to use these Series in combination, we could do so with   pd.concat([Series1, Series2, Series3]) , or with   pd.DataFrame(data=dictionary)  where the keys of the dictionary would represent the variables (and the names the columns will take) in the DataFrame, and the values would be the pandas Series (e.g. the elements of the columns) you will be using in your DataFrame.  One important thing to keep in mind is that, just like with the   np.concatenate  function we saw on the last lesson, you will need to pick an axis when using this method.   Note:  pandas will try to match the indexes of your multiple Series when combining their elements, but, if the indexes do not match, it will add an   np.nan  (Not a Number) at that place to show that a particular element does not exist.     # let's start with two series\n   series_one = pd.Series(np.random.randint(0, 20, 20), name='random_nums')\n   series_two = pd.Series(list(range(20, 60, 2)), name=\"two_steps\")\n   print(series_one, '\\n', series_two)\n     df_of_series = pd.concat([series_one, series_two], axis=1)\n   df_of_series\n  As noted above, the concatenation happens at the index level and both columns have been merged into a single dataframe. Let's now see what happens with different indexes that are not numerical.     # let's start with two series\n   series_three = pd.Series(np.linspace(0, 3, 10), index=list('abcdefghij'), name='random_nums')\n   series_four = pd.Series(list(range(50, 70, 2)), index=list('abcdefghij'), name=\"two_steps\")\n   print(series_three, '\\n', series_four)\n     df_of_series = pd.concat([series_three, series_four], axis=1)\n   df_of_series\n  We are still able to concatenate at the index level on matching letters, which is what we'd like. So let's now examine what would happend if we don't have the same amount of elements in both Series' we are trying to concatenate.     # let's start with two series\n   series_five = pd.Series(np.linspace(0, 3, 8), index=list('abcdehij'), name='random_nums')\n   series_six = pd.Series(list(range(50, 70, 2)), index=list('abcdefghij'), name=\"two_steps\")\n   print(series_five, '\\n', series_six)\n     df_of_series = pd.concat([series_five, series_six], axis=1)\n   df_of_series\n  We get what is called an NaN value, which stands for   Not a Number . It is a special value assigned to missing values which we will learn how to deal with very soon in the cleaning notebook.  Let's now examine how to create a pandas dataframe from a dictionary of Series.     # same approach as above but with dictionaries of Series's\n   \n   dict_of_series = {\n       'random_nums': pd.Series(np.random.randint(0, 20, 20), name='random_nums'),\n       'two_steps': pd.Series(list(range(20, 60, 2)), name=\"two_steps\")\n   }\n   dict_of_series\n     dict_of_series['random_nums']\n     # we can use a regular dataframe call for this one\n   df_dict_series = pd.DataFrame(dict_of_series)\n   df_dict_series\n  Exercise 4   Create two pandas Series with ones and zeros, respectively  add 5 to the one with zeros and assign it to a new variable,  add 8 to the one with ones and it to a new variable  add the two previous Series together, and assign the result to a variable.  Create two pandas Series with 5 random numbers each, and 3 countries as the index of each. Only two countries should match in the Series's indeces.                 Exercise 5   Create a DataFrame using pandas lists of lists.  Create a DataFrame using a NumPy matrix with different data types in each column.  Perform a computation with two columns and add the result as a new column in the DataFrame. (e.g. add or subtract two columns to create a third one.)                 Exercise 6   Crate 3 Series with 15 linearly spaced numbers in each, and a index based on letters for each.  Concatenate the 3 Series you just create into a DataFrame.  Sum an entire column and assign the result to a variable call `one_num'.  Get the average of an entire column and assign the result to a variable call `one_mean'.                      2.3 Accessing and Selecting Data  To access and select data in a pandas DataFrame we can use the same tools we learned in lesson 2,   array[start:stop:step, start:stop:step]  for rows and columns, fancy indexing, and masking for n-dimensional arrays. pandas also provides us with two additional tools for accessing data inside a DataFrame   df.loc[]  and   df.iloc[] .    df.loc[]  helps us select data in the same manner as with NumPy arrays except that we need to select the columns by their names and not by their numbers.   df.iloc[]  allows to select rows and columns by numbers. For example, if I have the columns   [weather, cities, days] , I could select weather with index 0, cities with index 1, and days with index 2, that same way we would do it with NumPy. One caveat of this method is that regardless of were the index start (e.g. 10 to 20), or how it is represented (e.g. a, b, c, d), it will start counting from 0.  Let's look at the regular way first.     # our previous weather dataframe\n   df_weather = pd.DataFrame(data=data_weather.T, columns=['weather', 'cities', 'days'])\n   df_weather\n     # regular indexing\n   df_weather[3:5]\n     # masking\n   df_weather[df_weather['cities'] == 'Perth']\n     # more masking or fancy indexing\n   df_weather.loc[df_weather['cities'] == 'Sydney']\n     df_weather.loc[df_weather['cities'] == 'Sydney', 'days']\n  This is a great, quick and dirty approach, but if we wanted to get more granular with how we select our data, we would have to resort to the additional functionality of   .iloc[]  and   .loc[] . Namely, selecting what we want and how with want it with the rows and columns of our dataset.  It is important to note that   .loc[]  includes the end point of a slice but   .iloc[]  does not. Meaning,   df.loc[:10]  would actually select the element at index 10 as well. The same would apply with the columns.  Let's look at pandas methods for slicing and dicing.     # select the first 7 rows of the days column\n   \n   df_weather.loc[0:7, 'days']\n     # select the first 5 rows of the days and weather columns\n   \n   df_weather.loc[0:5, ['weather', 'days']]\n     # same as before but with iloc now and integers\n   \n   df_weather.iloc[0:7, 2]\n     df_weather.iloc[0:5, [0, 2]]\n     df_weather.iloc[0:-2, 1:]\n     # multiple steps iloc\n   df_weather.iloc[::2, 1:]\n     # multiple steps loc\n   df_weather.loc[[3, 5, 7], ::2]\n  Both of these methods,   .iloc[]  and   .loc[]  will become extremely useful as we move along the course and our data analytics journey. A good tip for remembering the differences between the two is to always think of integers when you see the i in   .iloc[] .  Exercise 7   Crate a numpy array with 100 random integers. Assign it to the variable   one_hundred_nums .  Reshape   one_hundred_nums  into a 10 by 10 matrix. Assing it the variable name   matrix .  With   matrix  create a pandas DataFrame and call it   df . Make the columns different upper-case letters.  Using   .iloc[]  select the first 5 rows and the last 5 columns. Assign it to the var my   islice .  Using   .loc[]  select the every other row starting from the third one, and every other column starting from the second one. Assign it to the var my   dos_slice .                      Awesome Work - Head to Notebook 06    html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":114,"path":115,"dir":81,"title":116,"description":117,"keywords":118,"body":128},"content:2.analytics:4.stats_intro.md","/analytics/stats_intro","Intro to Statistics","Source: Information is Beautiful 2018 Awards - Data Visualisation Created by Dimiter Toshkov",[119,120,121,122,123,124,125,126,127],"Notebook Outline","2. Learning Outcomes","So, what can we do with Statistics?","What should we not do with Statistics?","Statistical Data Collection","5. Descriptive Statistics","6. Inferential Statistics","Questions","References","  Intro to Statistics     Source:  Information is Beautiful 2018 Awards - Data Visualisation Created by   Dimiter Toshkov  Notebook Outline   Recap of Module 4  Learning Outcomes  Introduction to Statistics  Data Collection  Descriptive Statistics  Inferential Statistics  2. Learning Outcomes  By the end of this module you will have   a better understanding of what is Statistics and what questions it might help you answer  learned about data collection and its importance  learned what are descriptive statistics  a better understanding on how to describe data effectively to convey facts  learned how to explore your data more effectively by grouping different sets of information in categories within your data  3. Introduction to Statistics  Statistics is one of the most widely used areas of mathematics. It helps us mere mortals collect, describe, and make inferences about natural and unnatural phenomena using data. It allows us to approximate answers to questions that were thought to be impossible to answer. In addition, a large number of scientific advancements have used this area of mathematics to quantify, test, validate, and improve our answers to problems with a wide range of complexities. For example, Albert Einstein and his groundbreaking theories of Special and General Relativity used statistics to approximate the movement of the sun in relation to that of other bodies of masses. To provide you with a more formal definition of statistics, the UCI Department of statistics has defined it as:   \"...the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data.\"  Empirical here means based on observations and verifiable evidence as opposed to logic and deduction. In essence though, statistics as a field is broadly divided into three categories, data collection, description (analysis), and inference.  Now that we know what \"  Statistics \" is, let's define what \"  A Statistic \" means. A   Statistic  is a piece of evidence or a fact obtained via a calculation performed on a sample of data. When you hear comments on the radio or read news statements that begin with, \"the average rugby player...\", \"the average Australian nowadays...\", etc., you are essentially being given \"  A Statistic \". A fact compiled from data coming from (most likely) a smaple or a population.   What is a Sample?  A sample is a fraction of the population. For example, the students in this course are a sample of the overall number of students in Australia. In this case, the number of students in Australia would be considered the total population of students, and the students of this course the sample.   What is a Population?  The population is all of the data available for a particular questions, problem, or phenomenon. It is a set of data with some similarity, and it represents the whole from where a sample is drawn from. Think about a cake sliced into pieces, the full cake is the population and a slice of it is the sample. Our friend Mickey can show us a good representation of this example.    Going back to the example above, all students in Australia would be the population and one of the defining characteristics of this population is that they would all have to be active students. If we needed, for example, the address of every one of these students but were unable to get, say, 10 of them, we would call the set of students belonging to that characteristic, the address, a sample as it is a fraction of the full set. Another way to put it would be   Population - 10 = Sample .  So, what can we do with Statistics?   We can learn from information.  Draw informed conclusions based on evidence.  Simulate the reality in a completely made up world.  Mine massive amounts of data and extract relationships from them.  Spot liars and deal with outliers.  Separate correlations from causation.  As one of the most famous statisticians put it,   “The best thing about being a statistician is that you get to play in everyone else’s backyard.” ~ John Tukey  What should we not do with Statistics?   Lie/deceive others.     Source:  \"  How to Lie with Statistics \" by Darrell Huff, 1954   Assume that because two things seem to be related to each other, one causes the other. A famous example of this was given by Franz H. Messerli, MD in 2012. He found a surprisingly large positive correlation between the chocolate consumption of a country and the amount of Nobel Laureates that same country has produced. See below.     Overgeneralise results. Just because an analysis of a group of consumers found that 60% of them buy goods before a holiday does not mean that 60% of all humans do.  Avoid falling pray of confusing annecdotal evidence vs real evidence. For example, XYZ thing happened to both of my friends, hence, this happens to everyone or all the time.  Now let's talk about data collection.  2. Data Collection  It is very important to have a   question , project or idea you wish to explore before moving onto the data collection part. The idea behind this is that once you have some context on what you would like to study, you might be able to search for that information more efficiently. For example, (1) if you need to study the profitability of a movie theatre in a given year, you would need data about sales and airing times for the movies shown within your specific timeframe. (2) If you would like to evaluate peoples' sentiment towards a political candidate on social media, you would need social media data and might benefit from looking in Twitter first. (3) If you want to test the effects of a newly developed drug are real, you might want to do a randomised control trial and give one group the real drug and a placebo to the other, that way, any difference detected regarding the outcome, could be attributed to the drug.  Whether you have a question or set of questions to answer, how you collect the data matters and that is why it is one of the most important pieces of the statistics puzzle. Especially because after all, without data to analyse we would be left with theoretical work only. Theory is extremely important, of course, but you need people and data to prove/use them as well. There is also an important difference to keep in mind, data gets collected for many, many purposes and collecting data from transactions at a supermarket is not the same as collected information from a group of interest (e.g. survey, focus groups, experiments, etc.).  Collecting data can also be done by clicking a button and downloading a table full of content that can be opened in excel (structured data), or it can be as difficult as conducting an experiment with human subjects, or (although not the same) crawling over many websites to get (unstructured) data and then formatting it into a more usable structure. Here are two examples of how we might encounter data in the wild:   Structured \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Name  Income  Age  Favourite Brand    Abraham  70,000  25  Nike   Lisa  110,000  31  Apple   Bella  80,000  24  Garmin   Penelope  55,000  49  Microsoft   Mikaela  74,000  51  ZARA   Nicholas  61,000  36  Adiddas   Unstructured   \n   Source:    https://www.lubys.com/recipes  Notice that in the example above, the structured data resembles what we are used to seeing when we use or think of data. On the other hand, we might not be used to thinking that recipes from a restaurant, emails, poems, social media posts, all qualify as data as well but in a different format. The most notable differences being the immediate usability of the two and the unclear picture as to how to set it into a structured format.  As analysts, we can collect data through surveys, purchasing history (these are often stored automatically into a database like SQL or NoSQL), by tracking visits to a website, and many more. The information we get in a dataset, the data in its variables (or in excel lingo, the columns), is often represented in three ways: as categorical and quantitative information and as date-related data (e.g. years, months, time, etc.).  Categorical data represent groups or specific characteristics of the data. For example, a column called gender that contains whether a survey participant was a male, female, or other, will count as a categorical variable. This type of classification, gender, counts as a specific type of categorical data called nominal.  Nominal data is the type of data that can be classified into groups but with no particular order to it. In the example of gender, we cannot say that female is greater than male or vice-versa. This takes us to our next type of categorical data, ordinal.  Ordinal data are the type of data where order matters. Imagine you have a dataset with formula one drivers in the rows and all of the characteristics regarding their professional careers in the columns. If we were to have a variable for every race in which the drivers competed, plus their placement within each race, keeping the order in which they crossed the finish line would be crucial in order for our dataset to be meaningful. Especially since first has a higher value than second, and, of course, last. Another example of ordinal data is the temperature of a meal ordered at a restaurant. If we order a stake cooked medium-well, we would expect to get the stake at a temperature within that range. In this scenario, the order of the categories would go from hot to cold, and that matters in an analysis.  Quantitative data, on the other hand, represents numerical data that can be quantified, summarized, averaged, and visualized, usually in as many or more ways than categorical data. Especially because we can bin quantitative data into groups of categories but the reverse is not true. Quantitative data can be income, revenue, stock prices, miles, weight, height, and many more.  It is important to notice that categorical data, whether nominal or ordinal, can still be collected in a numerical format without it being considered quantitative. Say we collect data for the level of spiciness of different meals at a restaurant and assign these values the numbers 3 (hot), 2 (medium), and 1 (not spicy), they would still be considered categorical data even though it would now be represented by a numerical value.  Statistical Data Collection     Source:  Florence Nightingale  When we collect data specifically for statistical purposes we want to gain knowledge or learn about something that is important to us. There are two ways in which statistics concerns itself when it comes to data collection, and those are through observation and experimentation.  An important point to keep in mind is that, although we might have an idea of what the population we're interested in looks like, or what the magnitude of the data we need might be, we would hardly ever collect enough data to make a 100% prediction or inference based on it. With that said though, there are many techniques and sampling methods that do allow us to get pretty close to our desired outcome. Let's first define what observational and experimental studies are.   Sample Study:  a study where we try to estimate the true value of a parameter based on a sample of the real population. A parameter in this case would be a piece of information such as the mean, median, and mode in a sample regarding our question of interest. If we wanted to know how many people per country, on average, believe in global warming, asking everyone in the planet would be impossible. But by asking a random sample of people from each country, we could approximate the true average of people that believe in global warming.   Observational Study:  In these kinds of studies, we are interested in a particular subsection of the population and we would like to determine what causes what without intervening with the sample being studied. For example, say we want to study the relatioship between smoking and drinking and whether one causes the other and vice-versa. We would observe people in their natural environment and collect data about what happens when these actions take place. We would then different statistical techniques to try to arrive at the root of our question, what causes what, if such causal link exists at all.  We also need to be cautious when, upon first inspection, all of our assumptions seem to match. A high correlation between both, smoking and drinking does not mean that one causes the other. It could be that there is a   confounding variable  behind the scenes controling the outcomes. For example, drinking and smoking could be due to being in a circle of friends where everyone smokes. It could also be a habbit triggered by the cue, for example, beign out with friends socialising. Confounding variables, or for that matter, the real causes of a phenomena, are often hard to pick appart but not impossible.   Experimental Study:  Experiments are the toughest ones to conduct but are also the ones that can get you the closest to determine a causal effect between two or more variables. With experiments we are in search of what causes what and we might even go to great lenghts to pick a random sample of people, then assign them to different random groups, and induce the variable of interest to half of that sample and a placebo to the other. These two groups are also called, control and treatment group. If the size of the difference between the two groups is large or significant enough once the experiment ends, it can be possible to determine causation.  There are other concepts in data collection that, although they are beyond the scope of this course, you are higly encouraged to explore further. These are,    Different sampling techniques   Cluster sampling  Stratified sampling  Simple random sampling  Systematic sampling  Snowball sampling  ...  Selection bias  Qualitative data collection (e.g. focus groups, interviews, etc.)  Quantitative data collection (e.g. experiments)  5. Descriptive Statistics    Source:    https://github.com/jwilber/roughViz   What are descriptive statistics?  Descriptive Statistics is the process of describing and presenting your data. This is usually done through tables, visualisations and written descriptions of data. For example, if you conduct a survey to find out how much people like a particular brand, you will want to report the number of people that took the survey (the count), the average, minimum, and maximum age or even the median income of every respondant. With these data alone, we could move onto to making more informed and important decisions.  In the previous survey example, every survey taker would be a row, and every question in that survey would form the columns (see the table below). Income, age, and favourite brand, would all represent different arrays or vectors of information in a two-dimensional matrix or dataset. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Name  Income  Age  Favourite Brand    Abraham  70,000  25  Nike   Lisa  110,000  31  Apple   Mark  80,000  24  Garmin   Penelope  55,000  49  Microsoft   Michaela  74,000  51  ZARA   Nicholas  61,000  36  Adiddas  Every one of these arrays can represent a categrorical or quantitative variable. Since data are often messy and even unstructured, we might also find other data structures, or free text (long strings) as the elements of a table like the one above. For the most part though, we will focus on the two types of variables we will often see, quantitative and categorical variables to describe our dataset.  Descriptive statistics are often shown through tables and visualisations. For example, imagine you'd like to know how the average life expectancy of people at age 60 in different countries has changed over time. That visualisation would look like the one below.     So what should we look for when we want to describe data?  We want to look for information that gives us facts about the data, such as the most common value of a set of characteristics, how far is a given value in a variable from the average value of that same variable (e.g. how far is the age of a teacher selected at random in all the schools in Sydney from the average age of all teachers in New South Wales). These two kinds of values, the average of a set and the variability of each value in a set, are part of what is known as measures of central tendency and measures of variability, respectively.   Measures of Central tendency , also classified as summary statistics, are calculations that help us describe any data point in our dataset in relation to the most common values. The most common determinants of central tendency are the mean, median, and mode.   Measures of Variability  tell us how spread-out our data is. Think about this as how much variation there is, whether up or down, in the income of your closest friends to that of the income of the average Australian. That distance from the income of your friends to the most common income (the average of all) gives us insights as to how much income variability there is in the entire population of Australia. In this instance, your friends would be consider a sample of the population. The most common measures of variability are the range, variance and standard deviation.   How do we describe categories and quantities?  Categorical variables can be shown using frequency tables and visualisations where each category of the variable is given a number representing the amount of times that category was found in the dataset. For example, a question in a survey containing answer choices ranging from Strongly Agree to Strongly Disagree will have 5 categories.  Quantitative variables, on the other hand, can be summarised with averages, percentiles, scatterplots, etc., and tend to provide different perspectives and insights than qualitative variables.  The most useful set of facts we will need will often come from a combination of both, qualitative and quantitative variables. Nontheless, Descriptive Statistics are not only an essential step in the data analytics cycle but are also specially useful when we have a lot of data and want to convey relevant a message with relevant information fast.  We will go over the most important descriptive statistics (with code) in the following notebooks.  6. Inferential Statistics    Statistical inference is one of the divisions of statistics that deals with what cannot (or has not) been observed in the data. It is the side of statistics used to make predictions about what might happen in the future based on what has happen in the past. In other words, it helps us estimate, among many things, some degree of uncertainty in the future (something we don't know anything about).  Another way for thinking about inferential statistics goes as follows. We have evidence about the characteristics of all of the observations in our sample, the true parameters of the population are unkown but we can approximate an answer to the true parameters if the sample is large or representative enough. In this scenario, we would like to make an inference or educated guess about the truth, and representative would mean that we capture as much variation as possible in the sample as that which we could expect in to see in the population  Three broad goals of statistical inference are parameter estimation, prediction, and model comparison.    \n**Source:** https://xkcd.com/892/\n  Questions   What is Statistics?  What is \"a Statistic\"?  What are the three broad areas of Statistics and how does one define them?  What is one way in which we can collect data?  What is Statistics useful for?  How can Statistics be harmful?  What are measures of central tendencies?  What is an Observational Study?  What are the 2 types of categorical variables?  What is an experimental study and can you think of an example of it?  References  Herne, H., & Huff, D. (1973).   How to Lie with Statistics . Applied Statistics, 22(3), 401. doi: 10.2307/2346789  Downey, Allen B.   Think Stats: Exploratory Data Analysis in Python . Green Tea Press, 2014.  Lock, Robin H., et al.   Statistics: Unlocking the Power of Data . John Wiley & Sons, 2019.",{"id":130,"path":131,"dir":81,"title":132,"description":7,"keywords":133,"body":148},"content:2.analytics:5.describing_data.md","/analytics/describing_data","Describing Data",[134,135,136,137,138,139,140,141,142,143,144,145,146,147,147,147,147],"Count","Mean","Median","Mode","Percentiles","Range","Variance","Standard Deviation","Minimum & Maximum","Skewness","Kurtosis","Pivot Tables","GroupBy","Exercise","  Describing Data   \"Facts are stubborn things, but statistics are pliable.” ~ Mark Twain   “Most people use statistics like a drunk man uses a lamppost; more for support than illumination” ~ Andrew Lang   “Statistics don't determine whether you find love or get married. You do!” ~ Suzette Vearnon    Source:    Tudor Networks by Kim Albrecht, Ruth Ahnert, and Sebastian Ahnert   What are descriptive statistics?  Descriptive Statistics is the process of describing and presenting your data. This is usually done through tables, visualisations and written descriptions of data. For example, if you conduct a survey to find out how much people like a particular brand, you will want to report the number of people that took the survey (the count), the average, minimum, and maximum age or even the median income of every respondant. With these data alone, we could move onto to making more informed and important decisions.  In the previous survey example, every survey taker would be a row, and every question in that survey would form the columns (see the table below). Income, age, and favourite brand, would all represent different arrays or vectors of information in a two-dimensional matrix or dataset. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Name  Income  Age  Favourite Brand    Abraham  70,000  25  Nike   Lisa  110,000  31  Apple   Mark  80,000  24  Garmin   Penelope  55,000  49  Microsoft   Michaela  74,000  51  ZARA   Nicholas  61,000  36  Adiddas  Every one of these arrays can represent a categrorical or quantitative variable. Since data are often messy and even unstructured, we might also find other data structures, or free text (long strings) as the elements of a table like the one above. For the most part though, we will focus on the two types of variables we will often see, quantitative and categorical variables to describe our dataset.  Descriptive statistics are often shown through tables and visualisations. For example, imagine you'd like to know how the average life expectancy of people at age 60 in different countries has changed over time. That visualisation would look like the one below.     So what should we look for when we want to describe data?  We want to look for information that gives us facts about the data, such as the most common value of a set of characteristics, how far is a given value in a variable from the average value of that same variable (e.g. how far is the age of a teacher selected at random in all the schools in Sydney from the average age of all teachers in New South Wales). These two kinds of values, the average of a set and the variability of each value in a set, are part of what is known as measures of central tendency and measures of variability, respectively.   Measures of Central tendency , also classified as summary statistics, are calculations that help us describe any data point in our dataset in relation to the most common values. The most common determinants of central tendency are the mean, median, and mode.   Measures of Variability  tell us how spread-out our data is. Think about this as how much variation there is, whether up or down, in the income of your closest friends to that of the income of the average Australian. That distance from the income of your friends to the most common income (the average of all) gives us insights as to how much income variability there is in the entire population of Australia. In this instance, your friends would be consider a sample of the population. The most common measures of variability are the range, variance and standard deviation.   How do we describe categories and quantities?  Categorical variables can be shown using frequency tables and visualisations where each category of the variable is given a number representing the amount of times that category was found in the dataset. For example, a question in a survey containing answer choices ranging from Strongly Agree to Strongly Disagree will have 5 categories.  Quantitative variables, on the other hand, can be summarised with averages, percentiles, scatterplots, etc., and tend to provide different perspectives and insights than qualitative variables.  The most useful set of facts we will need will often come from a combination of both, qualitative and quantitative variables. Nontheless, Descriptive Statistics are not only an essential step in the data analytics cycle but are also specially useful when we have a lot of data and want to convey relevant a message with relevant information fast.  Let's go over each one of the most important descriptive statistics.     import pandas as pd\n   import numpy as np\n   import matplotlib.pyplot as plt\n   import seaborn as sns\n   from utilities import check_or_add\n   import os\n   \n   plt.style.use('fivethirtyeight')\n   pd.set_option('display.max_columns', None)\n   pd.set_option('display.max_rows', None)\n   \n   # Setting pandas fload option to 5 decimal places helps us get rid of scientific notation\n   pd.options.display.float_format = '{:.3f}'.format\n   \n   %matplotlib inline\n   %load_ext autoreload\n     path = '../data'\n   \n   # for windows users\n   # path = '..\\data'\n     parquet_file = os.path.join(check_or_add(path, 'clean_parquet'), 'clean_airbnb.parquet')\n     # csv_file = os.path.join(check_or_add(path, 'clean_csv'), 'clean_airbnb.csv')\n     df = pd.read_parquet(parquet_file)\n     df.info(memory_usage='deep')\n     df.head(3)\n  Count  We will need to know how many observations (also known as the n count) are in our dataset and that is exactly what   len()  gives us. We pass an array of values through the function and it returns the number of all values in that array.   len()  also takes in strings and gives us back the total number of elements inside a string. Please note, counting and adding several numbers is not the same. To put this into perspective, think about what the average represents, the nominator is a sum of all the elements in an array while the denominator is the amount of elements in such array.     len(df)\n     df.shape[1]\n  We can also count specific categories visually with seaborn.     sns.catplot(x='room_type', data=df, kind='count')\n   plt.xlabel(\"Room Type\")\n   plt.title(\"Count per room type offered in Melbourne\")\n   plt.show()\n  Mean  What we call the mean is actually the arithmetic mean. This is the sum of all the values in a set or array, divided by the amount of numbers in such array. While zeros are always counted in the arithmetic mean, in Python, empty values or   NaN s are never counted towards that or any other operation.  $\\overline{X}=\\dfrac{1}{n}\\sum x_$  In the folmula above,   $\\overline{X}$ stands for the mean of array X   $n$  is the lenght of the array, vector, set, or list  $\\dfrac{1}{n}$ means we will divide everything by the lenght  The greek letter $\\sum$ denotes the sumation of all proceding values  $x_$ means every value in the array starting from   $i$ , and   $i$  in this instance can be any number  We can observe the mean of some of the quantitative variables in our dataframe with the method   .mean() .     some_numerical_vars = ['bathrooms', 'price', 'extra_people', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm']\n     df[some_numerical_vars].mean()\n     # the same works with np methods\n   np.mean(df[some_numerical_vars])\n  We can also calculate the mean ourselves with plain Python code.     sum(df['price']) / len(df)\n  Median  The median will order an array of numbers from lowest to highest and select the number in the middle. This means that the median is effectively the 50th percentile of any array. If the array has an even amount of numbers, it will return the average of the middle two numbers. If the arrays has an odd amount of numbers, it will return the one in the middle.  Just like with the   .mean()  method, we can use the   .median()  method on the quantitative variables of our dataframe and pandas will return the median for all variables. We could also use the   .median()  method and it will return the a median value for all numerical variables in our dataset. This is not recommended if your dataset is too large and has a lot of variables.     more_numerical_vars = ['accommodates', 'guests_included', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', \n                         'maximum_maximum_nights', 'availability_30', 'availability_60', 'availability_90']\n     df[more_numerical_vars].median()\n  Say we know that our   price  array has an odd number of elements and we want to get the value at the 50th percentile of our variable. We could, for instance, pass in a slice that selects that same middle numberof the array by computing floor division on the lenght of the array. The reason we would use a floor division is because all divisions in Python return a float and we can't pass floats to a slice. We could, in turn, wrap   int()  around our division, or the function   round()  instead to achieve the same result. Let's have a look.     len(df)\n     39742 / 2\n     # if your number is even, get the lower bound\n   lower_bound = (len(df) // 2) - 1\n   lower_bound\n     # if your number is even, also get the upper bound\n   upper_bound = (len(df) // 2)\n   upper_bound\n     # check out the value at x bound\n   sorted(df['price'])[lower_bound]\n     # check out the value at x bound\n   sorted(df['price'])[upper_bound]\n     # confirm the true median\n   true_median = (sorted(df['price'])[lower_bound] + sorted(df['price'])[upper_bound]) / 2\n   true_median\n     np.median(df['price'])\n  We can also divide normally and surround the operation with   int()  to make sure we get an integer for our slice.     odd_array = np.arange(9)\n   # odd array again\n   sorted(odd_array)[int(len(odd_array) / 2)]\n     np.median(odd_array)\n  To manually get the median of an even array, we need to do a bit more work. We need to   sort the array  create a slice for the middle number and subtract one  add the value on the previous step to another slice without subtracting 1  divide the result by 2     # even array\n   arr = np.random.randint(0, 200, 14)\n   \n   # sum the lower and higher slicers surrounding the median and divide by 2\n   (sorted(arr)[len(arr) // 2 - 1] + sorted(arr)[len(arr) // 2]) / 2\n     arr\n     np.median(arr)\n  Now, the above cell is illegible and does not help much. Let's decompose it step by step.     # first step\n   sorted_array = sorted(arr)\n   sorted_array\n     # second step\n   array_count = len(arr)\n   array_count\n     # third step\n   middle_of_array = (array_count // 2)\n   middle_of_array\n     # fourth step\n   low_bound = sorted_array[middle_of_array - 1]\n   low_bound\n     # fifth step\n   upper_bound = sorted_array[middle_of_array]\n   upper_bound\n     # final step\n   answer = (low_bound + upper_bound) / 2\n   answer\n     # confirm results\n   np.median(arr)\n  Mode  Mode is the most frequent number in an array of numbers. To get the mode we can pass the   .mode()  method to a series or we take advantage of on of the many methods in the library SciPy. (SciPy stands for scientific computing.) This library is a cousing of NumPy, or maybe a children, and it comes with many similar functionalities as the ones you would find in NumPy.     df['city'].mode()\n     # we import the mode function from scipy\n   from scipy.stats import mode\n  When we import the   mode  function we can pass it to our dataframe and it will return a scipy object with two arrays, one for the values that appear the most, and another for the times it appears the most.     more_cols = ['price', 'cleaning_fee', 'security_deposit', 'host_is_superhost']\n   mode(df[more_cols])[1]\n  We can pass in this structure to an   np.array() , transpose it, and get an array of tuples with each element and its respective value next to each other.     np.array(mode(df[more_cols])).T\n  We can confirm the median of any variable using the   .value_counts()  method.     df['host_is_superhost'].value_counts(normalize=True)\n  Percentiles  Percentiles give us the value at a given location of the array. For example,   np.percentile(array, 25)  will return the number where 75% of the data is above of, and 25% of the data is below of. In contrast,   np.percentile(array, 50)  would return the median the median of the array.     # This answers the question, what prices am I looking within the lower quarter of the array?\n   np.percentile(df['price'], 25)\n     np.percentile(df['price'], 95)\n     np.median(df['price'])\n  We can also get a particular percentile manually by multiplying percentages by the length of the array inside a slice of a sorted array. Think of this as masking.     print(f\"Our fancy percentage finding scheme --> {sorted(df['cleaning_fee'])[int(0.30 * len(df))]}\")\n   print(\"NumPy's version --> %f\" % np.percentile(df['cleaning_fee'], 30))\n  Range  The Range of a set is the difference between the maximum and minimum numbers of an array. Going back to our income example, if the highest-paid person in Australia made 2,000,000/year and the lowest-paid person made 500/year, then the range of that set would be the difference between the two, or 1,999,500. This is regardless of the lenght of the array as the range focuses on content not quantity of values.  We can use   np.ptp()  to get the range of a numerical array or we can compute the range ourselves in several different ways with regular Python.     # numpy's way\n   np.ptp(df['cleaning_fee'])\n     # python's way\n   max(df['cleaning_fee']) - min(df['cleaning_fee'])\n     # no helper functions\n   sorted(df['cleaning_fee'])[-1] - sorted(df['cleaning_fee'])[0]\n     sorted(df['cleaning_fee'])[0]\n     sorted(df['cleaning_fee'])[-1]\n  Variance  Variance tells us how much variation to expect in an array of data ($x$) with at least 2 numbers. For example, say we have two groups with 5 professional athletes in each. One group has soccer players and the other has tennis players. Now imagine we first ask each group how much do they spend eating out each week, and then we calculate the average of those amounts. Once we do this, we are surprised to find out that both groups of athletes spend the same on average eating out, about 570/week. Here is the data. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Athlete  Group  Money Spent/Week    Serena Williams  Tennis  $570   Roger Federer  Tennis  $570   Venus Williams  Tennis  $570   Rafael Nadal  Tennis  $570   Maria Sharapova  Tennis  $570   Cristiano Ronaldo  Soccer  $700   Hope Solo  Soccer  $380   Loenel Messi  Soccer  $200   Mia Hamm  Soccer  $650   Diego Maradona  Soccer  $920  Although the average is the same for both groups, the variation per group and player (as seen above) is extremely large. The tennis group's variation is equal to   0  while the variation of the soccer group is much greater than that.  Here is the mathematical formula for the variance before we see it in code. To calculate it, we sum the squared difference of each data point of an array from the mean of that same array. We then divide the sum by the lenght of the array.  $\\sigma^{2}=\\dfrac {1}{n-1}\\sum   \\left( x -\\overline {x}\\right) ^{2}$  In the folmula above,   $\\overline{X}$ stands for the mean  $n$ is the lenght of the array, vector, set, or list  $\\dfrac{1}{n-1}$ means we will divide everything by the lenght  The greek letter $\\sum$ denotes sumation  $x_$ means every   x  value starting from   i . In other words, every element of the array  $\\sigma^{2}$ --> squared standard deviation, aka result  $\\left( x_-\\overline {x}\\right) ^{2}$ means the square difference  Let's calculate the variance using different methods in Python.     # remember this variable\n   some_numerical_vars\n     # we can use a method on the entire array\n   df[some_numerical_vars].var()\n  We can also use the numpy method on a one dimensional array.     np.var(df['minimum_nights_avg_ntm'])\n  The last option is to do it ourselved with by coding every step of the formula.     df['price'].mean()\n     # first take the difference between each data point and the mean of the array\n   diff_var = (df['price'] - df['price'].mean())\n   diff_var.head()\n     # then squared the difference\n   power_two = diff_var ** 2\n   power_two.head()\n     # sum the array of differences\n   sum_price_diff = power_two.sum()\n   sum_price_diff\n     # then divide the sum of differences by the lenght of the array minus 1\n   variance = sum_price_diff / (len(df) - 1)\n   variance\n     # imagine if we were to do that whole process in one line of code\n   sum([(x - df['price'].mean()) ** 2 for x in df['price']]) / (len(df['price']) - 1)\n  Let's unpack what just happened in the cell above.   We initialised a list comprehension  then subtracted the mean of the price column from every one its prices  we then squared the result  summed everything up  and finished by dividing the final product by the n count of the price variable minus 1  In statistics, the variance and the standard deviation of a   sample  al always divided by   n - 1 . This is to signify the understatement of the true parameter $\\overline {X}$ which we don't know.  Standard Deviation  The   Standard Deviation  measures the dispersion of some data from its mean. Think of the dispertion of (normally distributed) data as percentage blocks surrounding the average, mean and median values (see the picture below).     Source:    https://sixsigmadsi.com/standard-deviation-measure-of-dispersion/  Every data point in these blocks is said to be 1, 2, or 3 standard deviations away from the mean. Additionally, these blocks provide us with an expectation for future values in percentage terms. Meaning, if we were to ask how many of the data points in our set are 1 standard deviation below the mean, we would mention that X% of our data points would land on that block. A more succint example would be, if crazy rainy days in Australia happened with a frequency of 1 standard deviation below or above the mean, and the weather temperatures were normally distributed, we could say that we would expect crazy rainy weather about 34% (up or down) of the time in a year. Of course, we could be even more specific about this, but the important distinction to keep in mind here is that sequential regions within a distribution can only be found in this form when the distribution of the data is normal.  $\\sigma = \\sqrt{\\dfrac {1}{n-1}\\sum   \\left( x -\\overline {x}\\right) ^{2}}$  In the folmula above,   $\\overline{x}$ stands for the sample mean  $n$ is the lenght of the array, vector, set, or list  $\\dfrac{1}{n-1}$ means we will divide everything by the lenght minus 1  The greek letter $\\sum$ denotes sumation  $x_$ means every   x  value in our array starting from   i  $\\sigma$ --> sigma == Result  $\\left( x_-\\overline {x}\\right) ^{2}$ means the square difference   np.std()  and   df[quant_vars_list].std()  will return the standard deviation of an array or matrix.     # givent the average and the standard deviation of prices, what's the highest value we expect to see 34%\n   # of the time above the mean? Should this be a normally distributed array of course.\n   df['price'].std()\n     np.std(df['price'])\n  We can also calculate the standard deviation by taking the square root of the variance.     np.sqrt(np.var(df['price']))\n  Minimum & Maximum  The Minimum & Maximum are the lowest and highest values in an array, respectively. These are useful when we have quantitative variables such as income, or house prices, but not when we have categorical variables such as gender or weekdays. For example, imagine having a variable called food temperature that is classified as hot, warm, or cold (with numerical equivalents of 1, 2, 3). Our functions MIN and MAX will not be very useful for this categorical variable since the distance of such categories would not carry much meaning. In contrast, the difference between an income of   $97,000  and an income of   $45,000  would be   $52,000 , and that is also the range between the two.  We can pass in min and max as methods to a dataframe or as numpy functions to arrays.     np.min(df)\n     np.max(df)\n  We can also sort the array and select the first and last elements for the min and the max.     # manual min\n   sorted(df['minimum_nights'])[0]\n     # manual max\n   sorted(df['minimum_nights'])[-1]\n  Skewness  The Skewness of an array tells us how distorted the distribution of such array is from the most common value or the peak of the curve. A rightly-skewed distribution is said to be positively skewed, and the opposite means that there will be a mountain going on the opposite direction (see below).     Source:    https://www.resourceaholic.com/p/resource-library-statistics-level.html  $Skewness = \\frac{\\sum_^{N} (X_ - \\overline{X})^{3}}{(N - 1)\\sigma^3}$  In the folmula above,   $\\overline{X}$ stands for the mean  $N$ is the lenght of the array, vector, set, or list  The greek letter $\\sum$ denotes sumation  $X_$ means every   x  value in our array starting from   i=1  $\\sigma^3$ --> standard deviation to the cube  Another important point to remember is that the mean of a positively-skewed distribution will be larger than the median, and the opposite is true for a negatively-skewed distribution. When skeweness is zero, there's no distortion in the distribution.     # let's see if the cleaning fees in our dataset present a distorted distribution\n   sns.kdeplot(df['cleaning_fee'].sample(500))\n   plt.title(\"Density distribution for Price per stay\")\n   plt.show()\n  We can also pass the   .skew()  method to our dataframe and get a sense of how distorted our variables might be.     df[['cleaning_fee', 'security_deposit']].sample(500).skew()\n  SciPy has convenient skew formula in the stats library as well.     from scipy.stats import skew\n     df['extra_people'].skew()\n     print(\"Extra People Skewness is %.2f\" % skew(df['extra_people']))\n     sns.kdeplot(df['extra_people'])\n   plt.title(\"Density distribution for the price for extra people\")\n   plt.show()\n  Kurtosis  Kurtosis is a very useful statistic that tells us how much do the tails of the distribution of a random variable differ from those of a normal distribution (e.g. a true bell-shaped curve). It tells us if we have extreme values or not in our variable.  The kurtosis of a normal distribution is usually at a value of 3. A much higher kurtosis than this means that we are dealing with outliers. On the other hand, a lower kurtosis means that the distribution we are dealing with has less extreme values than those seen in a normaly distributed array of data.     Source:  image author,   Anuradha Saha  Formula:  $Kurtosis = \\frac{1}{n} \\sum_^{n} (\\frac{x_ - \\overline{x}}{n - 1})\\sigma^4$  In the folmula above,   $\\overline{x}$ stands for the mean  $n$ is the lenght of the array, vector, set, or list  The greek letter $\\sum$ denotes sumation  $x_$ means every   x  value in our array starting from   i=1  $\\sigma^3$ --> standard deviation to the cube  Our pandas dataframe has two methods to calculate the Kurtosis.   .kurt()  and   .kurtosis() . They both return the same output.   NB.  There is a great medium article on skewness and kurtosis that you can find   here .     some_numerical_vars\n     df[some_numerical_vars].kurt()\n     df[some_numerical_vars].kurtosis() == df[some_numerical_vars].kurt()\n     from scipy.stats import kurtosis\n     print(\"Cleaning fee Kurtosis %.2f\" % kurtosis(df['cleaning_fee']))\n  Relationships  As data analysts we want to be able to determine how does one variable changes or moves in relation to another. We can do this visually using quantitative variables and scatter plots or with some handy mathematical functions that we can either create ourselves, or use from libraries like NumPy and SciPy. Let's begin by talking about correlation.   Correlation  is a measure of how strongly related two variables are with one another. It gives us a way of quantifying the similarity, disimilarity, or lack-therof between variables. The value of the correlation between two variables goes from -1 to 1, where 1 means positively correlated, -1 means negatively correlated, and 0 means no correlation whatsoever. This value is derived by calculating the   Pearson Correlation Coefficient , among other measures.     Source:    Investopedia  The correlation between two or more variables can be best observed through visualisations such as scatter plots and they can also be directly computed by hand or by using different functions in Python. The mathematical formula is:  $r_=\\dfrac {\\sum \\left( x_-\\overline {x}\\right) \\left( y_-\\overline {y}\\right) }{\\sqrt {\\sum \\left( x_-\\overline {x}\\right) ^{2}\\sum \\left( y_-\\overline {y}\\right) ^{2}}}$  Where   $r_$ is the relationship between the variables X and Y  $x_$ is every element in array X  $y_$ is every element in array Y  $\\overline {x}$ is the mean of array X  $\\overline {y}$ is the mean of array Y  For the correlation, we are dividing the covariance between the product of the standard deviations of each array. More on covariance below.     # let's create some columns for our prices\n   prices = df['price']\n   clean_fee = df['cleaning_fee']\n   \n   mean_price = prices.mean()\n   mean_clean = clean_fee.mean()\n   mean_price, mean_clean\n     # let's sum up the product of the differences between the elements in each array and their mean\n   arrays_diff_sum = sum((prices - mean_price) * (clean_fee - mean_clean))\n   arrays_diff_sum\n     # let's now compute the square root of the sum of the squared difference between each of the \n   # elements in an array and its mean\n   diff_sqrt = np.sqrt(sum((prices - mean_price)**2) * sum((clean_fee - mean_clean)**2))\n   diff_sqrt\n     r_xy = arrays_diff_sum / diff_sqrt\n   print(\"The correlation between regular prices per stay and the cleaning fees is: %.3f\" % r_xy)\n     # you can conveniently call the pandas method .corr with two Series as well and achieve the same result\n   print(\"Correlation between prices per stay and the cleaning fees is: %.3f\" % prices.corr(clean_fee))\n  The   pearsonr()  function from scipy stats will also give you the pearson correlation plus the p-value of the arrays. The P-value is the probability that you would have found the current result in another similar sample if the correlation was in fact zero. Think about this as the least likely chance to see another value like the one we got.     from scipy.stats import pearsonr\n     pearsonr(prices, clean_fee)[0]\n     df.head()\n     sample_data = df.sample(500)\n   \n   g = sns.JointGrid(x='cleaning_fee', y='number_of_reviews', data=sample_data, space=0, ratio=17, height=8)\n   \n   g.plot_joint(sns.scatterplot, size=sample_data[\"price\"], sizes=(30, 120),\n                color=\"#B48EAD\", alpha=.8, legend=False)\n   \n   g.plot_marginals(sns.rugplot, height=1, color=\"#A3BE8C\", alpha=.6);\n   \n   g.set_axis_labels(xlabel='Cleaning Fee', ylabel='# of Reviews');\n   Covariance:  this statistic tells us how two variables vary together. That is, how does the variation of one variable relates to the variation of another.  We can calculate the covariance of two arrays X and Y by first computing the distance between the mean and each of the values within the arrays.   dx  below represents the distance of each value   x  from its mean. The same is true for the   dy .  $dx = x_ - \\overline {x}$  \n$dy = y_ - \\overline {y}$  Lastly, we multiply each distance from both arrays, add the resulting values, and then divide by the lenght of one of the arrays (which have to be both of the same length).  $cov\\left( X,Y\\right) =\\dfrac {1}{n}\\sum dx_dy_$  If both arrays variate in the same direction, the result will be positive. If their variations are completely unrelated, we are likely to get a 0. If both arrays variate negatively, the result will be negative. This is also visible when the a corresponding value in each array is below or above the mean. In this instance, the latter would be positive and former negative.  Both numbers tend to scale with each other. A large positive covariance means that if x is large then y is large as well, and when x is small then y is small as well. If the covariance between arrays is 0, there is no relationship whatsoever between them.     prices.shape\n     # let's make sure both have the same lenght\n   prices.shape[0] == clean_fee.shape[0]\n     # np.dot takes the products of two elements and sums them up\n   np.dot((prices - prices.mean()), (clean_fee - clean_fee.mean())) / len(prices)\n     sum((prices - prices.mean()) * (clean_fee - clean_fee.mean())) / len(prices)\n  Notice that Pearson Correlation only measures whether there is a linear relationship between the variables in question. This mean, that the relationship could be other than linear, hence, no relationship in the Pearson's test doesn't mean that there is no relationship at all.  One of the flaws of pearson correlation is that it does not deal well with outliers. There are other methods available for determining non-linear relationships that do deal with outliers, but we won't cover those here. These are the Spearman Rank Correlation test and the Kendall Correlation test.  Cross-tabulating and Visualising Data  Another way to use descriptive statistics to gain deeper insights from a particular dataset is by cross-tabulating its variables. This technique allows us to aggregate and observe data based on distinct variables sitting on top of one another (columns) or standing next to each other (rows).  You have already seen a bit of cross-tabulation in previous lessons and provably while using a spreadsheet software, but let's dig a bit deeper into how to use these tools.     df.head(3)\n     df.shape\n  Pivot Tables  Let's look at the comparison between prices charged by super hosts and prices charged by non superhosts per different countries. We will do so by using a pivot table for our aggregation method. The most important arguments of a   pivot_table  are    values  --> what we want to aggregate (single column or list of columns). Usually a quantitative variable   index  --> what we want to represent in the rows (single column or list of columns). Usually a categorical variable or a date one   columns  --> what we want to represent in our columns (single column or list of columns). Usually a categorical variable   aggfunc  --> the aggregation method we want to use (single method or list of methods)     super_country_hosts = df.pivot_table(\n           values='price',\n           index='country',\n           columns='host_is_superhost',\n           aggfunc='mean'\n   )\n   \n   super_country_hosts\n     super_country_hosts['t']\n  Because the output of a   pivot_table  is essentially a reduced dataframe, we can use the plot method from pandas to visualise our results.     super_country_hosts.plot(kind='bar', title='Average Price Charged by Super and Regular Hosts', rot=45, color=['#434C5E', '#88C0D0'])\n   plt.legend(['False', 'True'])\n   plt.show()\n     df.columns\n     super_hosts = df.pivot_table(\n           values='price',\n           index='room_type',\n           columns='host_is_superhost',\n           aggfunc=['mean', 'count', 'median']\n   )\n   \n   super_hosts\n     super_hosts.loc[:, 'mean'].loc[:, 't']\n  Upon first inspection it seems as if the most     cancel_super_host = df.pivot_table(\n       index='property_type',\n       values='price',\n       columns='host_is_superhost',\n       aggfunc='median'\n   )\n   \n   cancel_super_host\n     # we can also perform operations with the reduced pivot table\n   cancel_super_host['diff'] = cancel_super_host['t'] - cancel_super_host['f']\n   cancel_super_host\n  We can also explore very complex interactions within the data.     identity_price = df.pivot_table(index=['host_is_superhost', 'country'], \n                                   values=['price', 'cleaning_fee'], \n                                   columns=['host_identity_verified', 'instant_bookable'],\n                                   aggfunc=['count', 'mean'])\n   \n   identity_price\n  I found that Japan has about 1500 listings from non-identified regular hosts whose listings are not instantly bookable.     identity_price['count']['price']['f']['f'].loc['f', :].loc['f', :]\n     identity_price.loc['f', ('count', 'price', 'f', 'f')]\n  As a side note. It would also be interesting to see whether there is a big gap between the distribution of reviews received by super hosts and regular hosts, and across rooms.     sns.set('notebook')\n   sns.set(rc={'figure.figsize':(11.7, 8.27)})\n   sns.violinplot(x=\"room_type\", \n                  y=\"number_of_reviews\", \n                  hue=\"host_is_superhost\", \n                  data=df.sample(5000), \n                  palette=\"Pastel1\")\n   plt.title(\"Reviews Distribution by Host and Room Type\")\n   plt.xlabel(\"Types of Rooms\")\n   plt.ylabel(\"Prices\")\n   plt.show()\n  GroupBy  To understand groups better, we cam create a multi-index object using the pandas method   .grouby() . This method is another useful tool for aggregating data and one that you might end up using a lot for data analysis with Python. Its mechanics are a bit different than that of   pivot_tables  but the results are almost the same. How does   .groupby()  works then   We first call the method on our data, e.g.   df.groupby  We then select the variable or variables (as a list if multiple) that we would like to split our data by, e.g.   df.groupby('cars')  or   df.groupby(['cars', 'cylinders'])  We now have a lazy   groupby  object from which we can select columns and aggregation methods for, e.g.   df.groupby('cars')['km_per_galons'].mean()  would give us the average kilometers per gallons by car brand  We can also specify multiple aggregation functions for a column, or specific aggregation methods for each columns (e.g. as a dictionary   {'column': 'mean'} ) using the   .agg()  method after the a specific columns or the   groupby  object itself  You can learn more about the awesome   groupby  through the pandas documentation   here .     df.head(2)\n     # first groupby\n   group_rooms = df.groupby('room_type')\n   group_rooms\n     # select one column\n   a_column = group_rooms['accommodates']\n   a_column\n     # perform a computation on the column\n   a_column.max()\n     # you can call other pandas functions on that same column\n   # here is the percentage of people each room accommodates\n   a_column.value_counts(normalize=True)\n     # you can also describe a column by the categories of your groupby object\n   group_rooms['price'].describe()\n     # in addition, you can be as specific as you want when specifying your aggregation methods\n   group_rooms['cleaning_fee'].agg(['mean', 'count', 'max', 'min', 'var', 'std'])\n     # multiple columns will give you multiple levels between your columns and aggregations\n   group_rooms[['accommodates', 'beds']].agg(['mean', 'count', 'max', 'min', 'var', 'std'])\n     # you can also get multiple levels on the indeces with the groupby call\n   rooms_cancel_policy = df.groupby(['room_type', 'cancellation_policy'])\n   rooms_cancel_policy['number_of_reviews'].max()\n     # we can also aggregate specific columns\n   specific_group = df.groupby(['host_identity_verified', 'instant_bookable'])\n   specific_group.agg({\n       'price':'mean',\n       'cleaning_fee':'median',\n       'security_deposit': 'max',\n       'extra_people': 'min'\n   })\n  Exercise   Create a pivot table with at least one index variable, two columns, and 3 aggregation methods.       Exercise   Create a pivot table with a multi-index, one quantitative variable as the values, and 1 aggregation method.       Exercise   Create a groupby object with 4 aggregation methods for 4 different columns.       Exercise   Create a groupby object with a 2-level index, 3 columns, and 1 aggregation method.       5. Summary  We have covered a lot in this module, so let's go over a few of the things we covered   The three main branches of statistics, collecting, describing and inferring from data, at a high level  A variety of techniques to deal with missing data  The shape and form of how data, structured and unstructured, might be collected  Different approaches for collecting data through sampling, observation and experimentation  The characteristics of measures of central tendency and measures of variability and what each describes  In-depth descriptive statistics and how to calculate these with plain Python  We also covered a variety of aggregation, or cross-tabulation methods with   pivot_table()  and the   .groupby()  functions\n   The best way to think of   .groupby()  is by stating: _for each \"category or group\" I'm interested in, show me \"x\" column(s) aggregated by \"z\" method  The   pivot_table()  function provides us with a different kind of control over the way we cross-tabulate data  6. References  Herne, H., & Huff, D. (1973).   How to Lie with Statistics . Applied Statistics, 22(3), 401. doi: 10.2307/2346789  Downey, Allen B.   Think Stats: Exploratory Data Analysis in Python . Green Tea Press, 2014.  Lock, Robin H., et al.   Statistics: Unlocking the Power of Data . John Wiley & Sons, 2019.  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":150,"path":151,"dir":81,"title":152,"description":7,"keywords":153,"body":158},"content:2.analytics:6.eda.md","/analytics/eda","Exploratory Data Analysis",[119,154,155,156,157,147,147],"1. What is EDA?","2. Questions to Explore","3.1 A Bit of Preparation","3.2 Exploratory Stage","  Exploratory Data Analysis   “Most of the world will make decisions by either guessing or using their gut. They will be either lucky or wrong.” ~ Suhail Doshi   “There is nothing more deceptive than an obvious fact.” ~ (Sherlock Holmes) Arthur Conan Doyle     Source:    New Retail Big Data Situation Awareness Screen by Zoe Shen  Notebook Outline   What is EDA?  Questions To Explore  Exploratory Data Analysis Stage\n   A Bit of Data Prep  Exploratory Stage  Dashboard  Takeaways / Reporting  Blind Spots  Future work  1. What is EDA?     Source:    Paulina Urbańska  Exploratory Data Analysis (EDA) is summarised in the name itself, it is an approach to data analysis where the goal is the exploration of the data and not necessarily hypothesis/model testing, although this can happen at this stage.   Wikepedia, along with all of its cited sources,  has a great definition which in turn was derived from the work of late John Tukey, a well-known statistician who is considered the creator of EDA as a concept and approach to data analysis.   In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.  Characteristics of EDA   It allows us to spot further inconsistencies within the data after the preparation stage  It helps us ask questions that expose facts about the data we have  It allows us to see complex interactions within the data  The visualisations at this stage are not necessarily meant to be publication-ready but rather quick and dirty constructs to asnwer questions from a different perspectives  2. Questions to Explore     Source:    xkcd.com  Here are the questions we will be exploring in this notebook. Before you head to the explanations, pause for a bit and think about how you would go about answering the following questions. Ask yourself, what kind of information would be most useful, which method/function and the like would be most appropriate to answer it? Should I visualise the answer as well?   What does the distribution of our monetary columns look like?  What's the difference in the average price charged by super hosts versus the regular ones?  Do more bathrooms make a listing more expensive? 🛁 | 🚽  Do more rooms make a listing more expensive?  Does the availability of more beds make a listing more expensive? 🛏  Is there a noticeable price difference between room types offered by hosts? 🏘  Is there a noticeable price difference between room types that offer different quantities of beds in the listing? 🛏 + 🛏 != 🛏  How important is it that our host is a verified one? ✍🏽  Should we care whether the listings asks for a license or not?  Do we need Wifi, or can we be without it?  Reviews! How important are they in our decision to buy or not to buy? 🤔  Do we care about the cancellation policy?  What is the average price difference between getting a listing that can be booked instantly vs one that we cannot book it instantly?  What is the price difference between listings that charge a security deposit vs those that don't?  What does the average price between different property types look like?  3. Exploratory Data Analysis Stage  3.1 A Bit of Preparation  You will need to install the following packages for this session. Once you install them, make sure you restart the notebook before you begin working on the lesson.     # !conda install -c pyviz holoviews panel bokeh -y\n   # !jupyter labextension install @pyviz/jupyterlab_pyviz\n     import pandas as pd, os, numpy as np\n   from bokeh.plotting import figure, show, output_file, output_notebook\n   from bokeh.models import ColumnDataSource # similar to a pandas dataframe but specific for bokeh\n   from bokeh.transform import dodge # a helpful tool for placing bar charts close to each other\n   import seaborn as sns\n   import matplotlib.pyplot as plt\n   import holoviews as hv\n   from holoviews import opts, dim\n   \n   \n   import urllib # we will use this again to get more websites\n   from PIL import Image # we will be looking at some Airbnb images\n   import requests\n   from io import BytesIO # for the images\n   \n   pd.options.display.max_columns = None\n   pd.options.display.max_rows = None\n   pd.options.display.float_format = '{:.4f}'.format # reduce numbers to 4 decimals\n   hv.extension('bokeh') # holoviews will be using bokeh behind the scenes\n   \n   output_notebook()\n   \n   # this magic command helps us to not reload our session every time we install a new package\n   \n   %matplotlib inline\n   %load_ext autoreload\n  Let's first add a variable with the path to where all of our data lives at. If yours is different than the one below, make sure you change the variable below to the correct path.     path = '../data'\n  You can choose to read in the cleaned dataset in whichever format you prefer, CSV or parquet. Uncomment the one you prefer and continue on.   Note:  If you were not able to save your file in the parquet format at the end of the previous notebook, make sure you go back and install the packages at the end of the notebook before you run it again. For now, go on with your CSV file.     # csv file\n   # df = pd.read_csv(os.path.join(path, 'clean_csv', 'clean_airbnb.csv'), parse_dates=True)\n   \n   # parquet file\n   df = pd.read_parquet(os.path.join(path, 'clean_parquet', 'clean_airbnb.parquet'))\n   \n   df.shape\n     # let's quickly examine our data\n   df.head(2)\n  The first step we are going to take is to create a column that represents the real price per stay. You might wonder, what do we mean by the real price per stay, so here is a quick explanation for that. Airbnb is an online marketplace with buyers (us) and sellers (the hosts), and in the same way we have our own specifications regarding where we would like to stay, the hosts might also have their own specifications in regards to whom they rent their places to. Here are some characteristics we need to pay attention to when creating our real price column.   The price is given per night  There is a cleaning fee which, if available, is not included in the price  The minimum amount of nights one can rent a listing for differs from country to country, and from listing to listing  There might be a security deposit we have to pay in advance  The second step we are going to take is to start introducting our specifications to our analysis to narrow down the scope of our search. We will be traveling solo and even though our specifications will reflect this, each step of our process can also be extended to a much larger group of travelers.  Let's begin by creating our   min_price_per_stay  column by multiplying the   price  column by the   minimum_nights  column and then adding the   cleaning_fee  and   security_deposit  columns. We will check the data types of our variables first to make sure these are all numerical variables.     # let's make sure all of the columns we need are numerical\n   df.dtypes\n  Our   minimum_nights  column was not of a numerical type so we will convert to an integer column in the cell below.     # minimum_nights was not numerical so we will convert it to int32 with the .astype() method\n   df['minimum_nights'] = df['minimum_nights'].astype(np.int32)\n   df['minimum_nights'].describe()\n     # let's now create our true cost variable\n   df['min_price_per_stay'] = (df['price'] * df['minimum_nights']) + df['cleaning_fee'] + df['security_deposit']\n   df[['price', 'min_price_per_stay']].head()\n  Now, let's examine the differences between the columns we just used, first throughout the entire dataset, and then split by country using the pandas   describe()  and   .groupby()  methods to see what we have.     # select the columns we want\n   money_columns = ['price', 'cleaning_fee', 'security_deposit', 'minimum_nights', 'min_price_per_stay']\n     # describe them to see what we have\n   df[money_columns].describe().T\n     # now look at the distribution of these variables per country\n   money_measures = df.groupby('country')[money_columns].agg(['min', 'mean', 'median', 'max'])\n   money_measures.T\n  The first thing that should come to our attention is the fact that we have extremely low and high prices that, although we may have been able to deal with during the cleaning stage, they would have been difficult to spot without some analysis throughout that stage (and we would have peaked at the data too 👀). Nonetheless, since we know our budget quite well, max 1,500 USD for 2 weeks, we will go ahead and filter out the listings that don't match that criterion. In addition, since we know (or assume that) it is very unlikely to see free listings (a price of 0) in Airbnb, we will get rid of any listing that costs less than 40 USD per stay, as this is a reasonable amount for a minimum per night (you can pick another value if you'd like).  Let's create a max budget column by multiplying the   price  column by   14  and then adding the   cleaning_fee . We will count on getting our deposit back so we will not include it in this particular variable.     df['two_weeks_price'] = df['price'] * 14 + df['cleaning_fee']\n   df['two_weeks_price'].head()\n  Now that we have our   two_weeks_price  we will create a low price condition to filter out prices less then   x  (  x  can be anything you'd like). We will also create a minimum amount of nights condition as we are going on a 2 week trip, nothing more, nothing less. Lastly, we will need a budget condition for our   two_weeks_price  variable that filters out anything over 1,500 USD, and then we will filter our data using these conditions.     # our minimum amount\n   low_price_condition = df['price'] > 40\n   # our highest amount\n   budget_condition = df['two_weeks_price'] \u003C= 1500\n   # minimum amount of nights cannot be greater than 2 weeks\n   nights_min_condition = df['minimum_nights'] \u003C 15\n   # let's filter our data\n   df_budget = df[low_price_condition & budget_condition & nights_min_condition].copy()\n   df_budget.shape, df.shape\n  We should make sure there are no more   0  in the   min  index for the price column of our countries.     money_measures = df_budget.groupby('country')[money_columns].agg(['min', 'mean', 'median', 'max'])\n   money_measures.T\n  3.2 Exploratory Stage  Question 1  What does the distribution of our monetary columns look like? In other words, where are most of the prices at in relation to the most common vlues of our columns.  For this question we could use the pandas plotting functionality first, and then move on to making a bit more informative plots.     # plain pandas\n   price_hist = df_budget['price'].hist(bins=25)\n   price_hist;\n  We could also use the library we just imported, holoviews, with NumPy and create a nicer looking histograms. But, what is a histogram anyways?   Acording to Wikipedia  and their wonderful cited sources, a histogram is   an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent and are often (but not required to be) of equal size.  Awesome! How can we create one with holoviews and NumPy?   Use   np.histogram()  to create 2 new arrays, one with the edges of the bins and another with the frequencies of the values within each edge. The function returns a tuple with 2 arrays so we will unpack the tuple into 2 variables.  Pass your two variables as a tuple with your   edges  first and the   frequencies  second, to the   hv.Histogram  function.  Evaluate your plot.  Let's see how this works.     frequencies, edges = np.histogram(df_budget['price'], bins=40)\n     frequencies[:5] # first array\n     edges[:5] # second array\n     hv.Histogram((edges, frequencies)) # our dataviz\n  It would be great if we could see all columns in one visualisation and have a widget to pick which column we'd like to see. Let's create just that.  First we will create a function that takes in a column, creates a numpy histogram based on the data and the column we have selected, and then create a holoviews histogram. Remember that   **kwargs  means any combination of key-value pairs that could be overwritten or added to a function. We will call our new function   load_currency .     def load_currency(column, **kwargs):\n       frequencies, edges = np.histogram(df_budget[column], 50)\n       return hv.Histogram((frequencies, edges)).opts(framewise=True, tools=['hover'])\n     money_columns = ['price', 'cleaning_fee', 'security_deposit', 'minimum_nights', 'min_price_per_stay', 'two_weeks_price']\n  We will then use holoviews   DynamicMap  function as it allows us to map functions to the data to make interactive charts. The first argument is the plain function we just created and the second is the key dimension where the interactivity is coming from. In our case, this is our list of currency columns which will now be in a dropdown box for us to pick and choose. We will assign our new object to a variable called   dmap  and finish our visualisation by adding some options to the figure, mainly, height and width. The last step is to tell holoviews where these columns are coming from using the   .redim.values()  method. Note that   Money_Cols  is a name we came up with for the widget but it can be anything we'd like. All we need to do is to make sure that we use the same name for our kdims argument.   Note:  You could add more numerical variables to the list above and visualize even more histograms.     dmap = hv.DynamicMap(load_currency, kdims='Money_Cols').redim.values(Money_Cols=money_columns)\n   dmap.opts(height=600, width=800)\n  That's an awesome visualisation and it allows us to evaluate the dispersion within our monetary columns more clearly. See if you can tweak the function and other parameters to come up with another cool visualization.  Question 2  What's the difference in the average price charged by super hosts versus the regular ones? What would be the difference if we were to split it by country?     price_super_diff = df_budget.pivot_table(\n       columns='host_is_superhost',\n       values='two_weeks_price',\n       aggfunc='mean'\n   )\n   price_super_diff\n     price_super_diff['t'] - price_super_diff['f']\n  Not a big difference at all. Let's look at the same measure but by country now.     q1 = df_budget.pivot_table(\n       index='country',\n       columns='host_is_superhost',\n       values='two_weeks_price',\n       aggfunc='mean'\n   )\n   q1\n  As we can see, even though there are some instances where the regular hosts are more expensive than the super hosts, for the most part, super hosts seem to have slightly higher prices for a two week stay than the regular ones. Let's go ahead and visualize this.  We will first use   bokeh's ColumnDataSource object to create  a bokeh-specific dataframe. This makes it easier for bokeh to convert any specification to JavaScript code before creating and showing the plot for us. In addition, it not only allows us to share data between different plots but there are also bokeh objects/glyphs/models that will only interact with data coming from a   ColumnDataSource  frame. To use it we can pass in the full dataset or specify the pieces of a dataset we will need using a dictionary or a similar object. We will use the dictionary to reassign all of the pieces from our table above and add this to a variable called   source .     source = ColumnDataSource(dict(\n       countries=q1.index,\n       super_host=q1.t,\n       regular_host=q1.f\n   ))\n  We will now create a bar chart with our data.     # create your figure\n   p = figure(x_range=list(q1.index), # the names or numbers to be used in the x axis\n              plot_height=350,\n              title=\"Average Cost for a 2-week Stay per Country\",\n              toolbar_location=None, tools=\"\") # we won't be using any toolbars here but you can if you'd like\n   \n   p.vbar(x=dodge('countries', -0.15, range=p.x_range), # dodge helps us separate the bars\n          top='super_host', \n          width=0.25, # this is the width of the green bars\n          source=source, # the data\n          color=\"#A3BE8C\", legend_label=\"Super Host\")\n   \n   p.vbar(x=dodge('countries',  0.15,  range=p.x_range), # dodge helps us separate the bars\n          top='regular_host', width=0.25, source=source,\n          color=\"#5E81AC\", legend_label=\"Regular Host\")\n   \n   \n   p.x_range.range_padding = 0.3 # the space from the range of countries to the edges of the figure\n   p.xgrid.grid_line_color = None # bars from each country to the top\n   p.legend.location = (320, 255) # play with these numbers and see where you can place the legend at\n   p.legend.orientation = \"horizontal\" # items are next to each other in the legend\n   \n   show(p)\n  There doesn't seem to be a big difference between the prices charged by super hosts and regular hosts among the countries we picked. Let's keep exploring.  Question 3  Do more bathrooms make a listing more expensive?  To answer this question, let's first plot the distribution of prices among the bathrooms available per listing.     bathrooms_group = df_budget.groupby('bathrooms')\n   bathrooms_group['two_weeks_price'].mean().plot(kind='bar', rot=-70);\n  It seems as if listings with 7.5 bathrooms will give us the absolute best deal, right? Let's look at the frequencies of these listings first to see whether this is a true average of more than one value.     bathrooms_group['two_weeks_price'].agg(['count', 'min', 'mean', 'median', 'max'])\n  In effect, this average is not what we were expecting, but, we can explore further and see what's up with the ones with a lot of bathrooms and a relatively low price. Remember our image function?     def image_show(image_url):\n       return Image.open(BytesIO(requests.get(image_url).content))\n     bathroom_num = 7.5\n   some_images = df_budget.loc[df_budget['bathrooms'] == bathroom_num, ['id', 'picture_url', 'two_weeks_price']]\n   some_images.head()\n     image_show(some_images.iloc[0, 1])\n  Okay at least we know that we do not want to stay a place without a single bathroom, so we will remove observations without at least one bathroom and assign the resulting dataframe to a new variable.     bathroom_cond = df_budget['bathrooms'] \u003C 1 # we have to have bathrooms\n   bathroom_cond.head()\n     df_bath = df_budget[~bathroom_cond].copy()\n   print(f\"We reduced our search space by {df_budget.shape[0] - df_bath.shape[0]} listings!!\")\n  Question 4  Do more rooms make a listing more expensive on average? or, put in other words, what is the difference between prices in a listing with 0 or a few bedrooms versus a listing with (say) more than 5?     room_num_group = df_bath.groupby('bedrooms')['two_weeks_price'].agg(['count', 'min', 'mean', 'median', 'max'])\n   room_num_group\n     room_num_group.iloc[:, 1:].plot(kind='bar');\n     room_num_group['mean'].plot(kind='bar');\n  We can't really compare averages here as the amount beds available in our dataset differs drastically, but there's still something useful to be said though. Let's look at the sorted values in decreasing order to see if we can spot the lowest prices of all. We'll create a function that takes in a dataframe, a column name, and a number to display after it sorts the array. We will then use our function in a for loop to print all of the variables.     def sort_n_show(series, col_to_sort, n_toshow):\n       print(series.sort_values(col_to_sort)[col_to_sort][:n_toshow])\n     for col in room_num_group.columns:\n       print(f\"This is the {col} column\")\n       sort_n_show(room_num_group, col, 3)\n       print('-' * 30)\n  Although 12 bedrooms seem to be the one with the overall lowest price (and the one that appears the most), we need to do some further digging to really be sure of any selection. Before we move on, let's have a look at our infamous cheap listing with 12 bedrooms.     df_budget.loc[df_budget['bedrooms'] == 12, 'picture_url']\n     image_show(df_budget.loc[df_budget['bedrooms'] == 12, 'picture_url'].iloc[1])\n     df_budget.loc[df_budget['bedrooms'] == 12, 'description'].iloc[0]\n  Ahhh! That makes sense now, the guest house is not necessarily commenting on a room per se but rather putting down information that belongs to both, the entire house and the rooms. Hence, this might be a bargain for such a low price. Let's keep exploring though.  Question 5  What about the beds? Do we care about the amount of beds available within our listing, and/or is there a price difference between having more, none, or just 1? In other words, does the availability of more beds make a listing more expensive?     beds_group = df_bath.groupby('beds')['two_weeks_price'].agg(['count', 'min', 'mean', 'median', 'max'])\n   beds_group.T\n     prices_two = df_bath['two_weeks_price'] > 1498.2335\n   beds_0 = df_bath['beds'] == 0\n   image_show(df_bath.loc[prices_two & beds_0, 'picture_url'].iloc[0])\n     beds_group['mean'].plot(kind='bar');\n  While there is very little variation within the minimum price of a listing regardless of the amount of beds available, the median and mean tell a different story. As shown in the table and image above, while the minimum prices vary drastically, the mean and median do seem to be capturing the most commmon prices for x number of beds available in a listing. The oddball here was the number 22. We don't necessarily want to stay in a room with 22 beds if we don't have to so let's see what the image of that listing has to offer.     bed_url = df_bath.loc[df_bath['beds'] == 22, 'picture_url'].iloc[0]\n     image_show(bed_url)\n  While this is the second time we see this listing with the super low price, and while it doesn't seem like that bad a deal to take, we'll continue exploring to see if we can find better offerings.  Question 6  Is there a noticeable price difference between room types offered by hosts?     rooms = df_bath.groupby('room_type')['two_weeks_price'].agg(['count', 'min', 'mean', 'median', 'max'])\n   rooms\n     roomba = rooms[['min', 'mean', 'median', 'max']].plot(kind='bar', rot=45)\n   roomba;\n  Here we can see that there are not that many differences between most of the prices per se but the n count differs drastically between room type, and the shared rooms seem to have the lowest price of all on average. Let's keep exploring.  Question 7  Is there a noticeable average price difference between room types that offer different quantities of beds?  Let's create a dummy variable of three categories for no beds, 1 bed, or more beds. We will use a function alogside some if-else statements, and the apply it to every element in our beds column.     def get_a_dummy(x):\n       if x == 1:\n           return \"One\"\n       elif x \u003C 1:\n           return 'None'\n       else:\n           return \"More than One\"\n     df_bath['beds_dummy'] = df_bath.loc[:, 'beds'].apply(get_a_dummy).copy() # applies a function to every element in a column\n   df_bath['beds_dummy'].value_counts()\n  Let's create a slightly more complex groupby object and see how our   two_weeks_price  changes with our new categorical variables.     rooms_and_beds = df_bath.groupby(['room_type', 'beds_dummy'])['two_weeks_price'].agg(['count', 'min', 'mean', 'median', 'max'])\n   rooms_and_beds.T\n  So we want at least one bed, and because of this, we will filter out those with no beds whatsoever.     df_beds = df_bath[df_bath['beds_dummy'] != 'None'].copy()\n   print(f\"We reduced our search by {df_bath.shape[0] - df_beds.shape[0]} listings!!\")\n     df_beds.shape\n  Question 8  How important is it that our host is a verified one? Should we stay with a host that has not been verified? Probably not, but let's see what we have in terms of prices.     df_beds.host_identity_verified.value_counts()\n     df_beds.pivot_table(\n       index='host_identity_verified',\n       columns='host_is_superhost',\n       values='two_weeks_price',\n       aggfunc='count'\n   )\n     df_beds.pivot_table(\n       index='host_identity_verified',\n       columns='host_is_superhost',\n       values='two_weeks_price',\n       aggfunc='mean'\n   )\n  Is it even possible to be a super host without being verified first? Apparently one can, but the verified ones are still cheaper. Let's go ahead and remove the hosts that have not been verified.     df_verified = df_beds[df_beds['host_identity_verified'] == 't'].copy()\n   print(f\"We reduced our search by {df_beds.shape[0] - df_verified.shape[0]} listings!!\")\n  Question 9  Should we care whether the listings asks for a license or not?  We don't have a license for either of this countries and your intructor, personally, does not have one at all (although he is a great driver nontheless 😎 + 🚗 = 🙌🏼), but that does not mean we should get rid of the ones that require one since we technically don't know whether they mean an ID or an actual license. In addition, we might be getting rid of a lot of obsevations that we would not want to get rid of in the first place. Let's examine this variable.     df_verified.requires_license.value_counts()\n     df_verified.pivot_table(\n       index='room_type',\n       columns='requires_license',\n       values=['two_weeks_price', 'cleaning_fee'],\n       aggfunc=['count', 'mean']\n   )\n  It seems upon first inspection the price for our stay is cheaper with hosts that don't require a lisence. Let's see the rest of the story by country.     df_verified.pivot_table(\n       index='country',\n       columns='requires_license',\n       values='two_weeks_price',\n       aggfunc=['count', 'mean']\n   )\n  In effect, if we get rid of the listings that do require a lisence (which might be the hosts saying you need an ID), we would be getting rid of all listings in Japan and we don't want that to happen.  Question 10  Do we need Wifi, or can we be without it?     df_verified.shape\n     df_verified['amenities'].head().iloc[0]\n     wifi_yes = df_verified.amenities.str.contains('Wifi', case=False)\n   wifi_yes.sum()\n     hot_water = df_verified.amenities.str.contains('Washer', case=False)\n   hot_water.sum()\n     df_verified.shape\n  We won't have service and would hate to miss any important information regarding any activity we might have scheduled ahead of time. Because of this, we will have to take out the places that do not include Wifi.     df_wifi = df_verified[wifi_yes].copy()\n   df_wifi.shape\n  Question 11  Reviews! How important are they in our decision to rent or not to rent? 🤔  Let's look at the distribution first.     df_wifi.shape\n     df_wifi['number_of_reviews'].describe()\n  Review columns are informative but we need to make sure there are no reviews equal to 0 before we examine the next ones. That way we can actually explore them.  Let's extract the reviews columns.     review_cols = [col for col in list(df_wifi.columns) if 'review_scores' in col]\n   review_cols\n     reviews_condition = df_wifi['number_of_reviews'] != 0 # condition for no reviews\n   yes_reviews = df_wifi.loc[reviews_condition].copy() # dataset with reviews\n   no_reviews = df_wifi.loc[~reviews_condition].copy() # dataset without reviews\n   yes_reviews['number_of_reviews'].describe(), no_reviews['number_of_reviews'].describe()\n     yes_reviews[review_cols].describe().T\n     yes_reviews.groupby(['country', 'room_type'])[review_cols].mean().T\n     revs_price = sns.scatterplot(x='review_scores_rating', y='two_weeks_price', data=yes_reviews)\n   revs_price;\n  Notice how the story changes when we split by room. We know we don't want to spend a fortune with this trip, but we are also aware that a bad pick could be detrimental to our stay. So, since we rather err on the cautious side, let's pick a cut off point for the reviews that makes sense to us.     clean = yes_reviews['review_scores_cleanliness'] > 9\n   rating = yes_reviews['review_scores_rating'] > 85\n   value = yes_reviews['review_scores_value'] > 8\n   accuracy = yes_reviews['review_scores_accuracy'] > 9\n   checkin = yes_reviews['review_scores_checkin'] > 7.5\n   comms = yes_reviews['review_scores_communication'] > 8\n   location = yes_reviews['review_scores_location'] > 8.5\n   rating.head(2)\n     df_revs = yes_reviews.loc[clean & rating & value & accuracy & checkin & comms & location].copy()\n   df_revs.shape\n     df_revs.country.value_counts()\n  Question 12  Do we care about the cancellation policy?  Since it is an expensive trip, it is important to at least account for last minute emergencies and be sure that we can recuperate at least some of our money.     df_revs.cancellation_policy.value_counts()\n     df_revs.cancellation_policy.value_counts().plot(kind='bar', rot=45)\n     super_strict = df_revs['cancellation_policy'] != 'super_strict_30'\n   super_less_strict = df_revs['cancellation_policy'] != 'strict_14_with_grace_period'\n   \n   df_cancellation = df_revs.loc[super_strict & super_less_strict].copy()\n   df_cancellation.shape\n  Since emergencies don't happen 14 days in advance, maybe, we will got rid of the two very strict cancellation policies and are down to about 1000 listings.     df_cancellation.country.value_counts()\n  Question 13  What is the average price difference between getting a listing that can be booked instantly vs one that we cannot book it instantly?     df_cancellation.instant_bookable.value_counts()\n     money_columns\n     df_cancellation.pivot_table(\n       index='country',\n       columns='instant_bookable',\n       values=money_columns,\n       aggfunc=['count', 'mean']\n   )\n  Prices don't seem to be too one-sided when it comes to the speed at which one can book the listing, because of this, we will not worry about this measure.  Question 14  What is the price difference between listings that charge a security deposit vs those that don't?     # let's see the count difference first\n   deposit = df_cancellation['security_deposit'] == 0\n   print(f\"Require a deposit - {df_cancellation.loc[~deposit, 'security_deposit'].count()}\")\n   print(f\"Does not require a deposit - {df_cancellation.loc[deposit, 'security_deposit'].count()}\")\n     df_cancellation[~deposit].pivot_table(\n       index='country',\n       columns='room_type',\n       values=['two_weeks_price', 'security_deposit'],\n       aggfunc='mean'\n   )\n  Overall, the average security deposit fee varies drastically and Belgium seems to be the country with the highest deposit fee on average. Prices per country don't seem to vary much in Entire home/apt but they do vary a bit in a Private room.  Question 15  What does the average price between different property types look like?     df_cancellation.pivot_table(\n       index='property_type',\n       values='two_weeks_price',\n       aggfunc=['count', 'mean']\n   )\n     loft = df_cancellation.loc[df_cancellation['property_type'] == 'Hostel', ['picture_url', 'country']]\n   loft.tail(28)\n     image_show(loft.iloc[0, 0])\n  Exercise  Come up with 5 questions to help you narrow the search to at most 100 listings.            4. Dashboard     Source:    Material Design Blog  We will now go over several of the many ways in which we can create a dashboard in Python. We will use the library panel for this. Here is the description of Panel from its website,   Panel is an open-source Python library that lets you create custom interactive web apps and dashboards by connecting user-defined widgets to plots, images, tables, or text. ~   HoloViz Team  What is a dashboard anyways?   A dashboard is a tool for summarizing critical or general information about a multitude of things that involve data  In business dashboards are used as graphical user interfaces to show the performance of a company from different angles  At a research center or market research firm, dashboards are used to explore data interactively  Dashboards display important statistics found in a dataset  Dashboards are used to enhance the viewers experience and ability to see more than one piece of information at a time  In the words of data visualisation expert,   Steven Few   \"A dashboard is a visual display of the most important information needed to achieve one or more objectives; consolidated and arranged on a single screen so the information can be monitored at glance.”  Now that we now what a dashboard is, let's talk about how to build one with Python.   Load your libraries  Load your data  Sketch out what you need to build  Select a type, interactive or static  Select the appropriate chart for the information you would like to display  Choose an appropriate color  If it is interactive, build a function that returns your visual as an object  If it is static assign your visual to a variable  Build the blocks/components of your dashboard into a shape that makes sense for you, e.g. rows, columns, both, none  the list goes on ...  Let's import Panel and a few other functions we will need. A few things to note about Panel first   Panel has 3 main components:   Pane, Widget, and Panel  Almost anything can go into a   Panel  (e.g. markdown, visualisations, images, tables, etc.)   Widgets  provide us with interactivity   Pane 's can be any element, addition or subtraction for a Panel or dashboard  The convention for importing Panel is   pn   pn.extension()  allows us to build interactive objects in the notebook     import panel as pn\n   from bokeh.transform import factor_cmap, factor_mark\n   from bokeh.palettes import brewer\n   \n   pn.extension() # setting panel extension from the start allows us to create interactive object in the notebook\n  Let's create the title of one of our dashboards. Note that we can write markdown text in a piece of string that will go into panel.     text = \"# A Place to Stay\\nThis dashboard has information found in three places we wish to stay at during our next vacation: South Africa, Japan, and Belgium\"\n   text\n  We will now use our two visualisations from earlier to create a small dashboard.     pn.Row(pn.Column(text, p), pn.Spacer(width=25), dmap)\n  Let's talk about what just happened.    pn.Row()  allows us to pass in visualisations, plain or markdown text, and other Panel options row-wise   pn.Column()  does the same as   pn.Row()  but column-wise   pn.Spacer  allows us to put space in between our visualisations  Let's create an informative plot. We will plot the price per night against the amount of bathrooms in our dataset, and split each point by the   room_type  column and increase the size of the plots by the amount of beds in that listing. In addition, because our prices vary drastically, we will transform our price variable to a logarithmic scale, this means that the prices will be in the scale of 10 to the power of 1, 2, 3, and so forth. The benefit of doing this is that it allows us to better observe variables that have values very spread out.   Note  that we are using a sample of our entire dataset from earlier.     listings_types = df.room_type.unique()\n   MARKERS = ['hex', 'circle_x', 'triangle', 'square']  # you can pick any markers you want from bokeh\n   colors = ['#5E81AC', '#EBCB8B', '#A3BE8C', '#B48EAD'] # these colors belong to Nord\n   \n   bottom_left = figure(title = \"Prices, Bathrooms, and Rooms\", \n                        x_axis_label='Bathrooms', \n                        y_axis_label='Price 4 Our Stay', \n                        y_axis_type=\"log\"\n   )\n   \n   bottom_left.scatter(y=\"price\", \n                       x=\"bathrooms\", \n                       source=df.sample(2000), \n                       legend_field=\"room_type\", # our legend will have our 4 room types\n                       fill_alpha=0.5, # the points will be a bit transparent\n                       size='beds', # they points size will increase by the amount of beds available in that listing\n                       marker=factor_mark('room_type', MARKERS, listings_types), # we map the markers to the room types\n                       color=factor_cmap('room_type', colors, listings_types) # we map the colors to the room types\n   )\n   \n   show(bottom_left)\n  We will now proceed to create a sankey diagram with part of the process we have followed so far in the EDA stage. A Sankey diagram is an acyclical flow chart where the width of a line represents the proprotion of the values that flow from the source (e.g. a characteristic of feature in our data) to the target (e.g. the next characteristic or feature we mapped the source to). For the values we can use whichever measure we wish to have flowing through the diagram (e.g. prices, reviews, etc.). In essence, to create a Sankey diagram with holoviews we need the   source , a   target , and the   values . The source and the target can be any 2 characteristics you want to see a measure flow from and to.  To create these three measures (source, target, and the values), we will use pandas groupby to create a multilevel index where the source is the first level, the target is the second, and the values will be represented by whichever measure we aggregate by--our   two_weeks_price  in our case.     edges1 = df_bath.groupby(['room_type', 'host_is_superhost'])['two_weeks_price'].mean().reset_index() # first flow\n   edges2 = df_bath.groupby(['host_is_superhost', 'country'])['two_weeks_price'].mean().reset_index() # second flow\n   edges3 = df_bath.groupby(['country', 'cancellation_policy'])['two_weeks_price'].mean().reset_index() # third flow\n   \n   datasets = [edges1, edges2, edges3] # list of smaller datasets to concatenate\n   \n   for dt in datasets: \n       dt.columns = ['source', 'target', 'value'] # change the column names of all three datasets\n   \n   edges = pd.concat(datasets, axis=0) # combine all three\n   edges.head(10)\n  We are now ready to create our Sankey diagram. Luckily, Holoviews has abstracted most of the heavy lifting for us and all we need to do is to pass our new dataframe and a title to the   hv.Sankey()  function of Holoviews. As with most tools in Holoviews, we can add some options to the diagram such as, the position of the labels, the color of the target, the node color, and where to choose these colors from.     sankey = hv.Sankey(edges, label='Progression of Analysis')\n   sankey.opts(label_position='left', edge_color='target', node_color='index', cmap='tab20')\n  Exercise  Using the same schema from above, try to add one or more layers to another Sankey diagram. Use a different variable name for it.            We are always interested in the distributions of our variables, especially the distributions of combined variables. With that in mind, let's examine the distribution of beds within the different kinds of properties in our listing.     boxwhisker = hv.BoxWhisker(df_bath.sample(2000), # our data\n                              'property_type', # our x axis\n                              'beds', # our y axis\n                              label=\"Distribution of Beds per property Type\" # our title\n                             )\n   \n   boxwhisker.opts(show_legend=False, # don't show a legend, the x axis has the titles already\n                   height=500, \n                   width=800, \n                   box_fill_color='property_type', # color the boxes by the property type\n                   cmap='tab20', # color to choose from\n                   xrotation=70 # rotate the labels of the axes\n                  )\n  Panel can take any object we pass through it, this includes images, blocks of color, etc. Hence, our dashboard can have almost anything we want to add to it. Let's start with a picture.     png = pn.panel('https://www.clicdata.com/wp-content/uploads/2019/07/blog-difference-bi-dataviz-data-analytics.png', width=400, height=125)\n   png\n  For the second dashboard we will create will use the   GridSpec()  function from Panel. As the name implies, it is a tool that allows us to create a grid full of objects to be displayed as a dashboard. The grid can be extended and contracted as necessary and to place objects in a specific spot we can assign elements in the same fashion in which we assigned elements to our NumPy matrices, by slicing the matrix/grid and assigning the elements we want to each spot.  Lastly, we can save any Panel object with the   .save()  function. We can save our dashboard as an HTML file and send/share it with anyone in an email, post, etc., or we can save it as a PNG, SVG, or JPEG file and share a snapshot of our dashboard.  Let's create our dashboard.     # first create a variable containing your GridSpec object\n   # sizing mode specifies if we want it expand and contract with different window sizes\n   gspec = pn.GridSpec(sizing_mode='stretch_both', max_height=800, max_width=1000)\n   \n   # first row and first column, some color\n   gspec[0, 0] = pn.Spacer(background='#88C0D0')\n   \n   # second row and first column, our picture from earlier\n   gspec[1, 0] = png\n   \n   # third row and first column, our title for the dashboard\n   gspec[2, 0] = text\n   \n   # first 2 rows and 2nd and 3rd columns, our boxplots\n   gspec[0:3, 1:3] = boxwhisker\n   \n   # 4th and 5th rows and first column, our scatterplot\n   gspec[3:5, 0] = bottom_left\n   \n   # 4th and 5th rows and 2nd and 3rd columns, our sankey diagram\n   gspec[3:5, 1:3] = sankey\n   \n   # show the plot or save it by uncommenting the .save() method\n   gspec.save('testing_dashboard.html')\n  We will now create an interactive dashboard using the following columns.     money_columns = ['price', 'cleaning_fee', 'security_deposit', 'min_price_per_stay', 'two_weeks_price']\n   some_features = ['bathrooms', 'bedrooms', 'beds', 'accommodates', 'guests_included']\n  One of the ways in which we create an interactive is by first creating functions that generate our desire plots and where the only argument we care about is the variable we want to see change. Here are the steps.   Create a list, or lists, of variables you want to see change  Create a function that generates your plot and takes as an argument the variable(s) you want to see changing  Create one or two variables using   pn.widgets.Select() , pass as arguments\n   value - the starting variable from your list  options - the list of variables you want to choose from with your plot  name (optional) - a name for such variable  On top of your plotting function(s) add what is called a decorator using   @pn.depends()   Decorators in Python are functions that can be wrapped around another function and provide some additional functionality to it. Think about it as having ice cream and cone, we can eat the ice cream without the cone using a different base but cone will not serve the same purpose without any ice cream. In the same fashion, a decorator add to a function but doesn't necesarily work on it own  To   @pn.depends()  you will pass the values you want your function to change by using   var_name_you_picked.param.value     # the x axis will change by our list of features\n   x = pn.widgets.Select(value='bathrooms', options=some_features, name='x')\n   # the y axis will change by our monetary columns\n   y = pn.widgets.Select(value='price', options=money_columns, name='y')\n   \n   \n   @pn.depends(y.param.value, x.param.value) # the decorators always go on top of the function and start with a @ sign\n   def make_pbr_plot(y, x, **kwargs):\n       \"\"\"\n       This is the same plot we created a bit ago but it is now inside a function\n       and the only elements that will change are the x and y axes.\n       When a variable changes, so will the title of our plot as well as the axes titles\n       \"\"\"\n       \n       listings_types = ['Entire home/apt', 'Private room', 'Hotel room', 'Shared room']\n       MARKERS = ['hex', 'circle_x', 'triangle', 'square']\n       colors = ['#5E81AC', '#EBCB8B', '#A3BE8C', '#B48EAD']\n       \n       fig = figure(title = f\"{y.title()}, {x.title()}, and Rooms\", \n                    x_axis_label=f'{x.title()}', \n                    y_axis_label=f'{y.title()} 4 Our Stay', \n                    y_axis_type=\"log\")\n       \n       fig.scatter(y=y, x=x, source=df.sample(2000), legend_field=\"room_type\", fill_alpha=0.5, size='beds',\n             marker=factor_mark('room_type', MARKERS, listings_types),\n             color=factor_cmap('room_type', colors, listings_types))\n   \n       return fig\n   \n   \n   @pn.depends(x.param.value) # this function will only change the features of a listing\n   def cat_whiskers(x, **kwargs):\n       \"\"\"\n       Same function as before but by many features now\n       \"\"\"\n       title = f\"Rooms and {x.title()} distribution\"\n       boxwhisker = hv.BoxWhisker(df.sample(2000), 'room_type', x, label=title)\n       boxwhisker.opts(show_legend=False, width=600, box_fill_color='room_type', cmap='Set1')\n       return boxwhisker\n  The last piece of the puzzle is to use this functions to create a dashboard that shows the widget to interact with the variables and our two charts from before. We will arrange our dashboard in 3 rows where on will have 5 elements, another will be a bit of space between the charts, and the next will be our second chart.     # this is our dashboard\n   child_1 = pn.Row( # this creates our rows\n       pn.Column('## Vars Explorer', x, y, pn.Spacer(width=25), cat_whiskers), # first element of our row has a column of 5 elements\n       pn.Spacer(width=25), # the 2nd element of our row has a bit of space for our charts\n       make_pbr_plot) # the 3rd element has our second chart\n   \n   \n   child_1#.save('example.html') # save it and share it with anyone : )\n  Lastly, panel also allows us to create multiple tabs that might contain anything we want. This includes dashboards, graphs, text, etc. You can add a name to each tab by passing a tuple where the first element is the name of the tab and the second is whatever you would like to display. Let's create one with two of our dashboards and the sankey diagram.     # initialise your object and pass in the tuples with the tab name and the onject you want to display.\n   pn.Tabs(\n       ('Analysis', child_1), \n       ('Process', pn.Column(\"# This is our Sankey Diagram\", sankey)),\n       ('Grid', gspec)\n       \n   )#.save('example2.html') # save it and share it with friends : )\n  5. Takeaways / Reporting  Exploratory data analysis is fun and informative. It is a stage in which questions come and go, answers fly in and out, and we, in our way, shape up our knowledge of the data at hand. It is even more fun to do so with a dataset we are interested in, should we be doing it for a personal project, of course. So what have we learned about our quest for a good place to stay at thus far   Prices can vary by many different factors such as beds and bathrooms available as well as the room type, and having a pre-defined set of measures to go by (e.g. budget, minimum requirements such as at least 1 bathroom and 1 bed) can help us stay more focused during the analysis  Reviews are important and can give us a general idea on what to expect from a listing  Don't take out variables or characteristic from within a variable (e.g. lisence) without knowing the context  Room type alongside our own pre-defined price measure, is one of the most important factors guiding our search   Remember our Goal  - To find the best place to stay at for our next vacation in terms of costs, venue, and things to do around it, given our top 3 destinations for 2021.  Using the set of criteria chosen before our data analysis, we can continue narrowing down even further the space of options to choose from. It is now you turn to go from 1000 to 100 or less options to choose from. Don't forget to visualise them with our image function!  6. Blind Spots  Here are some additional questions or key points we could have completed or think about throughout our analysis.   We could have continued crosstabulating different variables together to spot diverese interactions within our data  We could have extracted more keywords from the listing description and even from the amenities variable  We could have explored the locations within our countries of choice further  Were there any price differences between the defferent amount of time hosts have been with Airbnb?  Even though we need Wifi for certain, we did not calculate the difference in prices between it and not having it  7. Future work  Here are some points we could explore and cover with further analysis.   We could explore the actual reviews to gain more insight as to where the best place to stay at is. This would require the other datasets in   Inside Airbnb  and diving deep into Text Analytics  Some statistical modeling. We could have evaluated whether the differences in average prices given   x  measure were statistically significant or not  Geospatial analysis. Are the prices within one location significantly different from another?  This is a shareable notebook with an analysis using previously generated data, other options could have been to create an app that refreshes its data every time Inside Airbnb scrapes data from Airbnb  Awesome Work!    html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":160,"path":161,"dir":162,"title":163,"description":7,"keywords":164,"body":174},"content:3.data-engineering:1.data_pipelines_getting_started.md","/data-engineering/data_pipelines_getting_started","data-engineering","Data Pipelines From Scratch",[42,165,120,166,167,168,169,170,171,172,173],"1. Overview","3. Data","4. Tools","5. Data Pipelines","6. Building a Framework","7. Reproducible Pipelines","8. Scheduling","9. Exercises","10. Resources","  Data Pipelines From Scratch   “Without a systematic way to start and keep data clean, bad data will happen.” ~ Donato Diorio    \nSource:   draw.io  Table of Contents    Overview   Learning Outcomes   Data   Tools   Data Pipelines   Building a Framework   Set Up Dev Environment  Extract  Transform  Load   Reproducible Pipelines   Scheduling   Exercises   Resources  1. Overview  In order to create successful and reproducible data pipelines we need -- at the bare minimum -- tools that allow us manage where and how we store our data, how we run our computations, and how we version control everything we do. This is what we will focus on in this part of the workshop.  The assumption of this section is that most of your data work can fit in a computer but, if the need were to arise, you could still use the code in this section with a beefier machine and it would get the job done without a problem.  Since the files we'll interact with will most likely live in a remote server, we'll   extract a copy of the data we'll use and save it to our local files;  transform the data into the shape and form we need it to be in;  load it into a local data warehouse based on the popular tool, duckdb;  version our code and data using dvc and git;  create a command line tool to run all of our jobs;  create a reproducible pipeline that can capture different workflows.  Before we get started, let's go over the learning outcomes for today. :)  Note: While one of the main components of data orchestration is scheduling, you can sill create pipelines and reproducible workflows that can be triggered manually rather than via a schedule.  2. Learning Outcomes  Before we get started, let's go over the learning outcomes for this section of the workshop.  By the end of this lesson you will be able to,   Discuss what ETL and ELT Pipelines are.  Understand how to read and combine data that comes from different sources.  Create data pipelines using open-source tools.  Develop command line tools.  3. Data    All three data files contain similar information about how many bicycles have been rented each hour, day, week and months for several years and for each city government we are working with.  You can get more information about the data of each city using the following links.    Seoul, South Korea   London, England, UK   Washington, DC, USA   Porto, Portugal  -- This one was shared in Kaggle, but you can also find the original source with more, up-to-date data,   here .  Note: Some datasets might come with another file containing daily information, but for our purposes, we will be using the hourly one.  Here are the variables that appear in all data sets. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     London  Seoul  Washington  Porto    date  Date  instant  instant   count  Rented Bike Count  date  dteday   temperature  Hour  seasons  season   t2  Temperature(C)  year  yr   humidity  Humidity(%)  month  mnth   wind_speed  Wind speed (m/s)  hour  hr   weather_code  Visibility  (10m)  is_holiday  holiday   is_holiday  Dew point temperature(ï¿½C)  weekday  weekday   is_weekend  Solar Radiation (MJ/m2)  workingday  workingday   seasons  Rainfall(mm)  weathersit  weathersit    Snowfall(cm)  temperature  temp    Seasons  count  atemp    Holiday  humidity  hum    Functioning Day  wind_speed  windspeed     casual  casual     registered  registered      cnt  Since all of these datasets were generated with different logic (e.g. celsius vs fahrenheit, or other divergent measures) and, most likely, by different systems, we can expect more inconsistencies than just unmatching column names, numerical formats, and data collected.  We will walk through an example pipeline and several cleaning steps after we discuss the tools we will be using today.  4. Tools  The tools that we will use in this section of the workshop are the following.    pandas  - \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"   dvc  - \"DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\"   ibis  - \"Ibis is a Python library that provides a lightweight, universal interface for data wrangling. It helps Python users explore and transform data of any size, stored anywhere.\"   typer  - \"Typer is a library for building CLI applications that users will love using and developers will love creating. Based on Python 3.6+ type hints.\"   pathlib  - allows us to manipulate paths as if they were python objects.  Let's get started building data pipelines! :)  5. Data Pipelines    \nSource:   Striim  There are different kinds of data pipelines, but two, in particular, dominate a big part of the data engineering world today, ETL and ELT pipelines.   What are ETL Pipelines?  The acronym ETL stands for Extract, Transform, and Load, and it is the process where data gets extracted from one or multiple sources, it gets processed in-transit, and then it gets loaded into place where data consumers can use (e.g. a data warehouse). These consumers can be data analysts, data scientists, and machine learning engineers, among many others.   What are ELT Pipelines? \nWith this approach, all the data, structured and unstructured, gets loaded into a data lake or warehouse before it gets transformed. With this approach, a lot of money spent on compute can be saved by only processing the data we need rather than all of it.   What are Reverse ETL Pipelines? \nReverse ETL tools take data from the data lake or warehouse back into business (critical) applications. For example, information about new customers that have not yet been populated into salesforce or other marketing tools for further use by the marketing, sales, and finance teams, and so on...   Why should you learn how to create them?  Data Pipeline tools enable data integration strategies by allowing companies to gather data from multiple data sources and consolidate it into a single, centralized location. ETL tools also make it possible for different types of data to work together, for example, data generated by the company can be combined with GPS and Temperature data coming from different sources.  As data professionals, our task is to create value for our organizations, our clients and our collaborators using some of or all the data at our disposal. However, there are factors that can delay this process a little bit or a lot, for example, we often need to understand beforehand,   Information about the process by which the data we're dealing with was generated, e.g.\n   Point of sale  Clicks on an online marketplace like Amazon, Etzy, Ebay, ect.  A/B Test Results  ...  Information about the transformations that occurred during the cleaning and merging process, prior to us jumping on board,\n   Celsius degrees were converted into fahrenheit  Prices in Chilean pesos were converted to Rands  Non-numerical and unavailable observations now contain \"Not Available\" or \"Unknown\"  ...  Information about how the data was stored and where. For instance,\n   Parquet format  NOSQL or SQL database  CSV  ...  Understanding how the three processes described above flow will help us have more knowledge about the data that we are going to use, and how to best access it, transform it, and model it before we put it to good use.  Let's walk through an example of a data pipeline using data from wildfires between 1983-2020 in the United States. You can find more information about the dataset   here .     import pandas as pd\n   from pathlib import Path\n     !pwd\n     data_path = Path().cwd().parent/\"data\"\n   data_path\n     example_data_in = data_path.joinpath(\"example\", \"federal_firefighting_costs.csv\")\n   pd.read_csv(example_data_in).head()\n     pd.read_csv(example_data_in).dtypes\n  As you can see, most columns contain a   $  dollar sign and some   ,  commas, and because this forces Python to treat numbers as objects (or strings) rather than   int 's or   float 's, we will have to remove these signs in our transformation step after extracting the data and before loading a clean version of it to a new location. Let's create 3 re-usable functions.     def extract(path):\n       return pd.read_csv(path)\n  As you saw above, only the last 5 variables have commas (  , ) and dollar symbols (  $ ) so we will replace both with an empty space (  \"\" ) using a   for  loop.     def transform(data):\n       for col in data.iloc[:, 1:].columns:\n           data[col] = data[col].str.replace(r'[^0-9]+', '', regex=True).astype(int)\n       return data\n  For the \"load\" process, we will save the data as a   parquet  file. This is one of the most popular formats to save data in due to its compression capabilities, orientation, and speed gains in analytical workloads.  Here's an example on the differences between the row-like format and the columnar format of parquet files. If this interests you you can read more about it   here    \nSource:   Scylla DB   def load(data, path):\n    data.to_parquet(path)\n    print(\"Successfully Loaded Your Modified Data!\")\n  Let's create an output path and with a file name to save our data as.     data_path\n     example_data_out = data_path.joinpath(\"example\", \"my_test.parquet\")\n  When we have all the steps ready, we create a new function containing our graph using the   flow  decorator. We can give this function a name, for example,   \"Example Pipeline! 😎\"  and then chain the tasks we created previously in the order in which they should be run.     def example_etl(example_data_in, example_data_out):\n       data = extract(example_data_in)\n       data_clean = transform(data)\n       load(data_clean, example_data_out)\n       print(\"Your Pipeline Ran Successfully!\")\n  We are ready to run our workflow.     example_etl(example_data_in, example_data_out)\n  To make sure we did everything correctly, let's create a quick visualization with pandas.     pd.read_parquet(example_data_out).plot(\n       x='Year',\n       y=\"ForestService\", \n       kind='line',\n       title=\"Forest Service costs by year\"\n   );\n  6. Building a Framework  In order to do our data engineering work with a personalized framework, there are a few strategies we would take.   We could keep the code in our computers and copy it to a new project whenever we need to create new pipelines.  We could create a package and either upload it to PyPI or   Gemfury  (a private repository of packages for different programming languages).  We could keep in neatly organized in GitHub and clone the repository to every new project.  For this use case, it would be great to work with a library that we could continuously update rather than notebooks and files getting copied around. This will help us stay organized and manage dependencies more effectively.  Let's get started. :)  6.1 Setting Up a Dev Environment  Building frameworks can be super straightforward or a slightly cumbersome project. Because of this and because we are already using an environment to run this workshop in, I will leave here what I think is an excellent resource for learning about how to create Python Packages.     from IPython.display import HTML\n   HTML(\"\"\"\n   \u003Cdiv align=\"center\">\n       \u003Ciframe width=\"700\" height=\"450\"\n       src=\"https://youtube.com/embed/l7zS8Ld4_iA\"\n       \u003C/iframe>\n   \u003C/div>\n   \"\"\")\n  6.2 Extract  Extracting data might seem straightforward, but it can come with plenty of caveats. For example, to access sensitive data you might need not only credentials but also access to VPNs (virtual private networks), you might have similar data in different kinds of formats (customer tables stored in legacy databases and new ones), or you might have different data all stored in one place (images, tables, and text all in a datalake) -- if you're lucky.  To tackle these challenges, (1) access and (2) distribution of data, companies such as   Airbyte ,   Fivetran , and others, have come up with solutions that do all the heavy lifting for us. They have created tools that either give you connectors to common data sources (such as S3, BigQuery, and others), or give you an API so that you can develop a custom connector.  That said, what we'll do in this section is to create functions that allow us to extract data in different formats or download a dataset from the web. We will create one for each of the examples that we have for the workshop today. Let's get started loading some data.     import urllib.request\n     def get_and_load_file(url, path_out, file_name):\n       path = Path(path_out)\n       if not path.exists(): path.mkdir(parents=True)\n       urllib.request.urlretrieve(url, path.joinpath(file_name))\n     get_and_load_file(\n       url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv\",\n       path_out=\"../data/example\",\n       file_name=\"seoul_exp.csv\"\n   )\n  The function above will download the file into the   \"data/example\"  directory. If we do not specify a path for the file to   urllib.request.urlretrieve , it would download the file into a temporary directory since it wouldn't know what to do with it or where to put it.  A popular alternative to   urllib  is   wget , so you can switch the tools in this function easily. The former is part of the Python Standard Library (so you'll never have to install it), and the latter can be installed with   pip  or   conda .  Let's create two more, one for csv files and the other for parquet files.     def extract_from_csv(path_in, encoding=None):\n       return pd.read_csv(path_in, encoding=encoding)\n     seoul_df = extract_from_csv(\n       path_in=data_path.joinpath(\"seoul\", \"raw\", \"SeoulBikeData.csv\")\n   )\n   seoul_df.head()\n  The reason, we've added   enconing  to our function is that files are not always shared in   utf-8  abd it is important to account for this discrepancy when creating an ETL framework. In fact, one of our datasets has a tricky format itself.     {1: \"value\", 1: \"value\", 1: \"value\", 1: \"value\", }\n     def extract_from_parquet(path_in, **kwargs):\n       return pd.read_parquet(path_in, **kwargs)\n     porto_df = extract_from_parquet(\n       path_in=data_path.joinpath(\"porto\", \"bike_sharing_hourly.parquet\")\n   )\n   porto_df.head()\n  One of our files is stored in a SQLite database so we'll use the   sqlite3  module, which is part of of the   Python Standard Library , to read it.     import sqlite3\n     def extract_from_db(path_in, query):\n       conn = sqlite3.connect(path_in)\n       return pd.read_sql_query(query, conn)\n     london_df = extract_from_db(\n       path_in=data_path.joinpath(\"london\", \"london_bikes.db\"),\n       query=\"SELECT * FROM uk_bikes\"\n   )\n   london_df.head()\n  Lastly, JSON files can be tricky to handle so we'll try two quick cases here using a try-except approach, but take note that as your projects evolve, it is highly likely that this function might change. A very good tool to handle large amounts of unstructured JSON that can later be formatted as a dataframe (or many), is   Dask Bags .     def extract_from_json(path_in, **kwargs):\n       try:\n           data =  pd.read_json(path_in, kwargs)\n       except:\n           with open(path_in, 'r') as f:\n               data = json.loads(f.read())\n               data = pd.json_normalize(data, kwargs)\n       return data\n     dc_df = extract_from_json(\n       path_in=data_path.joinpath(\"wash_dc\", \"washington.json\")\n   )\n   dc_df.head()\n  Now that we have our functions, we want to create a file and attach to it the minimum functionality possible to use it as a command line tool. We will do this with the popular package called   typer . It offers a delightful user experience, it respects (and annoys you at times) by type-checking your code, and it provides you with beautifully-formatted output based on the   rich  python library.  With   typer  we can create CLIs in several ways, and, for our purposes, we will pick the most straightforward one which works by adding   typer.run()  around a main function that encapsulates your application logic, in our case, our extract function and the upcoming ones.     %%writefile ../src/data_eng/extract.py\n   \n   import urllib.request\n   from pathlib import Path\n   import pandas as pd\n   import json\n   import typer\n   from typing import Optional\n   from load import save_data\n   \n   \n   def get_and_load_file(url, path_out, file_name):\n       path = Path(path_out)\n       if not path.exists():\n           path.mkdir(parents=True)\n       urllib.request.urlretrieve(url, path.joinpath(file_name))\n   \n   def extract_from_csv(path_in, encoding=None):\n       return pd.read_csv(path_in, encoding=encoding)\n   \n   def extract_from_db(path_in, query):\n       conn = sqlite3.connect(path_in)\n       return pd.read_sql_query(query, conn)\n   \n   def extract_from_parquet(path_in, **kwargs):\n       return pd.read_parquet(path_in, **kwargs)\n   \n   def extract_from_json(path_in, **kwargs):\n       try:\n           data =  pd.read_json(path_in, **kwargs)\n       except:\n           with open(path_in, 'r') as f:\n               data = json.loads(f.read())\n               data = pd.json_normalize(data, kwargs)\n       return data\n   \n   def main(\n       arg: str = typer.Option(...),\n       url: Optional[str] = typer.Option(None),\n       path_in: Optional[str] = typer.Option(None),\n       path_out: Optional[str] = typer.Option(None),\n       encoding: Optional[str] = typer.Option(None),\n       file_name: Optional[str] = typer.Option(None),\n       query: Optional[str] = typer.Option(None),\n   ):\n       if arg == \"get\":\n           get_and_load_file(url, path_out, file_name)\n       elif arg == \"csv\":\n           data = extract_from_csv(path_in, encoding=encoding)\n           save_data(data, path_out, file_name)\n       elif arg == \"pq\":\n           data = extract_from_parquet(path_in)\n           save_data(data, path_out, file_name)\n       elif arg == \"json\":\n           data = extract_from_json(path_in)\n           save_data(data, path_out, file_name)\n       elif arg == \"db\":\n           data = extract_from_db(path_in, query)\n           save_data(data, path_out, file_name)\n       else:\n           print(f\"Could not understand argument {arg}. Please use 'pq' for parquet files, 'db' for database, json, or csv in lowercase.\")\n   \n       print(\"Data Extracted Successfully!\")\n   \n   \n   if __name__ == \"__main__\":\n       typer.run(main)\n  Note that we added different parameters to our   main  function to control the behavior of our CLI app.    arg  - the kind of data we're extracting.   url  - the url for when we need to download files from somewhere   path_in  - where is the data at   path_out  - where is that data going to   encoding  - what kind of enconding are we reading the file with   file_name  - what is going to be the new name of the output file   query  - query for the data we want from the SQL database  A few important things to note:   parameters containing underscores   _  will be switched into a dash   -  by   typer , so   file_name  will be   file-name  commands are run with two dashes and with a space in between it and the argument being passes, e.g.   --name PyCon   typer.Option(None)  indicates to   typer  that this command can be optional, hence, when we change the kind of file we're extracting we can by pass having to put an option on others.  by adding   Optional  around the type of a function parameter, you are also letting typer that that parameter is optional and that its default argument is   None .  a parameter with only a type is a required parameter.     pd.read_parquet(\"../data/seoul/raw/testing_cli_extract2.csv\").head()\n  6.3 Transform   Depending on tasks that await the final output files or tables of an ETL pipeline (e.g. reporting metrics, building machine learning models, forecasting, etc.), this step can be very convoluted or slightly straightforward. In general, though, this step can include lots of cleaning and normalization functions, and a schema setter, among many others.  The cleaning steps might include dealing with missing values, cleaning emails, names, addresses, and others, or putting values of similar nature into the same format (e.g. different weather measurements into one).  Normalization can be anything from changing column names with the same values but different names to have the same one, or height in cm vs feet and inches to be in the same measure, or ..., you get the point. :)  A schema is the way in which we represent not only the content and type of our new data files/tables, but also the way in which we represent their relationship. A common schema is the   star schema . An uncommon (but rising star) schema is the   activity schema .     import re\n     #create some toy data for our Bicycle problem\n   \n   toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n                            \"Cities'\":       [\"Miami,  FL\", \"Dallas, TX\", \"Washington, DC\"],\n                            \"Dates as mm-dd-yyy\":         pd.date_range(start='9/27/2021', periods=3)})\n   toy_data\n     def clean_col_names(index_of_cols: pd.Index):\n       return [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in index_of_cols]\n     clean_col_names(toy_data)\n     def extract_dates(data, date_col, hour_col=None):\n   \n       data[\"date\"] = pd.to_datetime(data[date_col], infer_datetime_format=True)\n       if not data.columns.isin([\"hour\", \"Hour\", \"hr\", \"HR\"]).any():\n           data[\"hour\"] = data['date'].dt.hour\n           #Time series datasets need to be ordered by time.\n           data.sort_values([\"date\", \"hour\"], inplace=True)\n       elif hour_col:\n           data.sort_values([\"date\", hour_col], inplace=True)\n       else:\n           print(\"You must figure out how the hour works in your file.\")\n   \n       data[\"year\"]           = data['date'].dt.year\n       data[\"month\"]          = data['date'].dt.month\n       data[\"week\"]           = data['date'].dt.isocalendar().week\n       data[\"day\"]            = data['date'].dt.day\n       data[\"day_of_week\"]    = data['date'].dt.dayofweek\n       data[\"is_month_start\"] = data['date'].dt.is_month_start    \n   \n       data.drop('date', axis=1, inplace=True)\n       return data\n     extract_dates(london_df, \"timestamp\").head()\n  A common tasks in data science is to create dummy variables, which is ofter referred to as one-hot encoding. These are binary representations of a category, for example, if you have a columns with different kinds of cars, after you one-hot encode it, you will have one column for sedan, coupe, convertible, and so on, and each will be represented with a 0 or a 1 for when it is available and when it isn't, respectively.     def one_hot(data, cols_list, **kwargs):\n       return pd.get_dummies(data=data, columns=cols_list, **kwargs)\n     one_hot(seoul_df, [\"Seasons\", \"Holiday\"], drop_first=True).head()\n     def add_location_cols(data, place):\n       data[\"loc_id\"] = place\n       return data\n     add_location_cols(dc_df, \"DC\").head()\n     def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n       data[col_to_fix] = data[col_to_fix].map(mapping)\n       return data.drop(cols_to_drop, axis=1)\n     seasons_london = {'Spring': 0, 'Summer': 1, 'Fall': 2, 'Winter': 3}\n   cols_drop_london = ['t2', 'weather_code']\n     london_df.head()\n     london_df[\"seasonreal\"].map(seasons_london).value_counts()\n     fix_and_drop(london_df, \"seasonreal\", seasons_london, cols_drop_london).head()\n     def order_and_merge(data_lists, date=None):\n       pick_order = data_lists[0].columns #takes order of columns of the first dataset\n       #reindexing columns by the order of the first dataset, then sorting by date and hour.\n       if date:\n           new_list = [d.reindex(columns=pick_order).sort_values([date, 'hour']) for d in data_lists]\n       else:\n           new_list = data_lists\n       return pd.concat(new_list) #merge all\n  Exercise  Pick any two datasets and   select 5 columns from each  change column names as appropriate  use the order_and_nerge function to combine both     london_df.head()\n     london_df_small = london_df[[\"cnt\", \"t1\", \"wind_speed\", \"seasonreal\", \"hour\"]]\n   london_df_small.tail()\n     seoul_df.head()\n     seoul_df_small = seoul_df[[\"Rented Bike Count\", \"Temperature(�C)\", \"Wind speed (m/s)\", \"Seasons\", \"Hour\"]]\n   seoul_df_small.head()\n     seoul_df_small.columns = london_df_small.columns\n   seoul_df_small.columns == london_df_small.columns\n     order_and_merge([seoul_df_small, london_df_small]).shape\n  Finally, we take the same approach as with the extract set of functions and wrap our transform workflow into a   main  function, and then that   main  function into a typer CLI.     %%writefile ../src/data_eng/transform.py\n   \n   import pandas as pd, re\n   from pathlib import Path\n   from extract import save_data\n   import typer\n   from typing import Optional, List\n   \n   def clean_col_names(index_of_cols: pd.Index):\n       return [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in index_of_cols]\n   \n   def extract_dates(data, date_col, hour_col=None):\n   \n       data[\"date\"] = pd.to_datetime(data[date_col], infer_datetime_format=True)\n       if not data.columns.isin([\"hour\", \"Hour\", \"hr\", \"HR\"]).any():\n           data[\"hour\"] = data['date'].dt.hour\n           #Time series datasets need to be ordered by time.\n           data.sort_values([\"date\", \"hour\"], inplace=True)\n       elif hour_col:\n           data.sort_values([\"date\", hour_col], inplace=True)\n       else:\n           print(\"You must figure out how the hour works in your file.\")\n   \n       data[\"year\"]           = data['date'].dt.year\n       data[\"month\"]          = data['date'].dt.month\n       data[\"week\"]           = data['date'].dt.isocalendar().week\n       data[\"day\"]            = data['date'].dt.day\n       data[\"day_of_week\"]    = data['date'].dt.dayofweek\n       data[\"is_month_start\"] = data['date'].dt.is_month_start    \n   \n       data.drop('date', axis=1, inplace=True)\n       return data\n   \n   def one_hot(data, cols_list):\n       return pd.get_dummies(data=data, columns=cols_list)\n   \n   def add_location_cols(data, place):\n       data[\"loc_id\"] = place\n       return data\n   \n   def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n       data[col_to_fix] = data[col_to_fix].map(mapping)\n       return data.drop(cols_to_drop, axis=1)\n   \n   def order_and_merge(data_lists):\n       pick_order = data_lists[0].columns #takes order of columns of the first dataset\n       new_list = [d.reindex(columns=pick_order).sort_values(['date', 'hour']) for d in data_lists] #reindexing columns by the order of the first dataset, then sorting by date and hour.\n       return pd.concat(new_list) #merge all\n   \n   def main(\n       path_in:   str           = typer.Option(...),\n       date_col:  Optional[str] = typer.Option(None),\n       hour_col:  Optional[str] = typer.Option(None),\n       cols_list: List[str]     = typer.Option(None),\n       place:     Optional[str] = typer.Option(None),\n       path_out:  Optional[str] = typer.Option(None),\n       file_name: Optional[str] = typer.Option(None)\n   ):\n       data = pd.read_parquet(path_in)\n       # data.columns = clean_col_names(data.columns)\n       data = extract_dates(data, date_col, hour_col)\n       if cols_list:\n           data = one_hot(data, cols_list)\n       data.columns = clean_col_names(data.columns)\n       data = add_location_cols(data, place)\n       save_data(data, path_out, file_name)\n   \n       print(\"Data Extracted and Transformed Successfully!\")\n   \n   if __name__ == \"__main__\":\n       typer.run(main)\n     pd.read_parquet(\"../data/seoul/interim/cli_test_clean.parquet\").head()\n  6.2 Load  The loading stage, at least for analytical purposes, tends to be a data warehouse like BigQuery, Redshift, etc., but it can also be a directory in a data lake where files are saved in the highly optimized parquet format.  In order for us to simulate a data warehouse locally, we will create a duckdb database using the python library   ibis . The reason we will do it this ways is that ibis can translate the schema of our files into the duckdb SQL in one line of code. Effectively saving us boilerplate code that may or may not end up accounting for every feature in our dataframes.  Ibis is a very cool project, especially if your SQL skills as basic as mine, so I highly encourage you to check it out. :)   import ibis\nfrom pathlib import Path\n   data_path = Path().cwd().parent/\"data/\"\n   def create_db(path_in, path_out, file_name, table_name):\n    path = Path(path_out)\n    conn = ibis.duckdb.connect(path.joinpath(file_name))\n    conn.register(path_in, table_name=table_name)\n    print(f\"Successfully loaded the {table_name} table!\")\n   create_db(\n    path_in=data_path.joinpath(\"porto\", \"bike_sharing_hourly.parquet\"),\n    path_out=\"../data/dwarehouse\",\n    file_name=\"mytest_dw.ddb\",\n    table_name=\"one_day\"\n)\n  Let's read in our data to check that it was loaded successfully.   import duckdb\n   duck = duckdb.connect(\"../data/dwarehouse/mytest_dw.ddb\")\nduck.query(\"SELECT * FROM one_day\").df().head()\n   def create_parquet(path_in, path_out, file_name):\n    path = Path(path_out)\n    pd.read_parquet(path_in).to_parquet(path.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n   def save_data(data, path_out, file_name):\n    path_out = Path(path_out)\n    if not path_out.exists(): path_out.mkdir(parents=True)\n    data.to_parquet(path_out.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n  We take the same steps as before and wrap our main function with   typer.run()  to make it a CLI, we test that it works well, and then go on to the next stage.   %%writefile ../src/data_eng/load.py\n\nfrom pathlib import Path\nimport ibis\nimport typer\nfrom typing import Optional\nimport pandas as pd\n\n\ndef create_db(path_in, path_out, file_name, table_name):\n    path = Path(path_out)\n    conn = ibis.duckdb.connect(path.joinpath(file_name))\n    conn.register(path_in, table_name=table_name)\n    print(f\"Successfully loaded the {table_name} table!\")\n\ndef create_parquet(path_in, path_out, file_name):\n    path = Path(path_out)\n    pd.read_parquet(path_in).to_parquet(path.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n\ndef save_data(data, path_out, file_name):\n    path_out = Path(path_out)\n    if not path_out.exists(): path_out.mkdir(parents=True)\n    data.to_parquet(path_out.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n\n\ndef main(\n    kind: str = typer.Option(...),\n    path_in: Optional[str] = typer.Option(None), \n    path_out: Optional[str] = typer.Option(None),\n    file_name: Optional[str] = typer.Option(None),\n    table_name: Optional[str] = typer.Option(None)\n):\n    if kind == \"db\":\n        create_db(path_in, path_out, file_name, table_name)\n    elif kind == \"pq\":\n        create_parquet(path_in, path_out, file_name)\n    else:\n        print(f\"Could not understand argument {kind}. Please use 'pq' for parquet files or 'db' for database.\")\n\n    print(\"Data Extracted Successfully!\")\n\nif __name__ == \"__main__\":\n    typer.run(main)\n  7. Reproducible Pipelines  Data Version Control or   dvc  is a tool for version almost any kind of file you can think of. From images, text, videos, and excel spreadsheets, to machine learning models and other artifacts. In addition, it is also an excellent tool for tracking machine learning experiments and for creating language agnostic pipelines where you can automatically version control the inputs and outputs of your workflows.  If you have ever used git then the following commands will feel like home    dvc init  --> this the first step to get started using dvc (after installing it of course)   dvc remote add -d storage gdrive://your_hash  --> since we will be tracking files somewhere, this command will help set up a remote repository for these.   dvc add  --> add a file to track.   dvc push  --> push the file to your remote repository.   dvc pull  --> pull the file from your remote repository and into your local machine.   dvc stage  --> allows you to start adding steps to a pipeline. It will create a   dvc.yml  file containing the steps of the pipeline. This can be modified manually as well.   dvc repro  --> Once we finish our stage, or as we add steps to it, we can run our pipeline with this command. The best part is that the steps will get cashed, so if nothing changes, nothing will get rerun.   dvc dag  --> allows you to visualize your pipelines as a graph in the terminal.  Finally, some of the settings for our repo will be available at   .dvc/config  and these can be changed manually at any time.  Open a terminal in the main directory for this workshop and follow along.  To create our pipeline, we'll add the stages step by step.     dvc   stage   add   --name   seoul_extract   \\\n       --deps   data/seoul/SeoulBikeData.csv   \\\n       --outs   data/seoul/raw/seoul_raw.parquet   \\\n       python   src/data_eng/extract.py   --arg   csv   \\\n           --path-in   data/seoul/SeoulBikeData.csv   \\\n           --path-out   data/seoul/raw   \\\n           --file-name   seoul_raw.parquet   --encoding   iso-8859-1\n     dvc   stage   add   --name   seoul_transform   \\\n       --deps   data/seoul/raw/seoul_raw.parquet   \\\n       --outs   data/seoul/interim/seoul_clean.parquet   \\\n       python   src/data_eng/transform.py   \\\n           --path-in   data/seoul/seoul_raw.parquet   \\\n           --date-col   Date   --hour-col   Hour   --place   Seoul   \\\n           --path-out   data/seoul/interim   --file-name   seoul_clean.parquet\n  Exercise  Add the load stage into our pipeline following the examples above.     dvc   stage   add   --name   seoul_load   \\\n       --deps   data/seoul/interim/seoul_clean.parquet   \\\n       --outs   data/dwarehouse/analytics.db   \\\n       python   src/data_eng/load.py   --kind   db   \\\n           --path-in   data/seoul/interim/seoul_clean.parquet   \\\n           --path-out   data/dwarehouse   --name   analytics.db   \\\n           --table-name   seoul_main\n  Finally we can run our pipeline, evaluate it, and push our files into our remote storage.     # 1\n   dvc   repro\n   \n   # 2\n   dvc   dag\n   \n   # 3\n   dvc   push\n  Afterwards, dvc will provide us with git files to track for our project.  8. Scheduling  Scheduling our workflows can be super convenient and time-savior when we know we need to run tasks repeatedly. A very common tool for the task is cron, and here is an excellent tutorial to get started with it.  Note: Windows users would need to use Windows Subsystem for Linux to use cron or some other tool, potentially in PowerShell.   from IPython.display import HTML\nHTML(\"\"\"\n\u003Cdiv align=\"center\">\n    \u003Ciframe width=\"700\" height=\"450\"\n    src=\"https://www.youtube.com/embed/QZJ1drMQz1A\"\n    title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; \n    clipboard-write; encrypted-media; gyroscope; picture-in-picture; \n    web-share\" allowfullscreen>\n    \u003C/iframe>\n\u003C/div>\n\"\"\")\n  9. Exercises  Exercise 1   Open the terminal, create a new directory named   dc_bikes , and cd into it.  Create two subdirectories, data and src.  Create an ETL pipeline with two functions in the transform step.  Exercise 2   Initialize a git and a dvc repository for the project in Exercise 1.  Create a new repo in GitHub and commit your initial changes.  Create a dvc pipeline with your three stages.  Commit your changes.  10. Resources  If you'd like to expand your knowledge around the tools and concepts that we covered in this lesson, you might find the following list of resources helpful.   To learn more about pandas -->   Python for Data Analysis, 3E by Wes McKinney  To learn more about ibis -->   Ibis + Substrait + DuckDB by Gil Forsyth  To learn more about duckdb -->   DuckDB Tutorial by Data with Marc  To learn more about dvc -->   ML Pipeline Decoupled: I managed to Write a Framework Agnostic ML Pipeline with DVC, Rust & Python  To learn more about typer -->   Data and Machine Learning Model Versioning with DVC by Ruben Winastwan  To learn more about cron -->   Cron Job: A Comprehensive Guide for Beginners 2023 by Linas L.  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":176,"path":177,"dir":162,"title":178,"description":7,"keywords":179,"body":181},"content:3.data-engineering:1.getting_data.md","/data-engineering/getting_data","Getting Data as DataFrames",[119,111,86,87,180],"Awesome Work!","  Getting Data as DataFrames   \"You don’t understand anything until you learn it more than one way\". ~ Marvin Minsky   \"Wear your learning, like a watch, in a private pocket: and do not pull it out and strike it, merely to show that you have one.\" ~ Lord Chesterfield     Source:    Vallerio Pellegrini  Notebook Outline   Getting Data Into Python  Text  Excel  HTML  1. Getting Data into Python    When working with data in Python you will encounter datasets coming in all shapes and formats, so it is crucial to understand how to deal with them in order to work with data. We will be covering the following 4 formats in this section (yes, there are only 4 here 😁):   CSV --> Comma Separated Values -->   pd.read_csv(file, sep=',')  TSV --> Tab Separated Values -->   pd.read_csv(file, sep=' ')  Excel --> Microsoft Excel format (.xlsx) -->   pd.read_excel()  JSON --> JavaScript Object Notation -->   pd.read_json()  HTML --> Hypertext Markup Language -->   pd.read_html()  For this part of the lesson, we will be using some real world datasets that you can find more info about (e.g. how to download them) in datasets directory one level above this one. Please download them and add them to the datasets directory for this course, or whichever directory you'd like to use.    2. Text Files  Text files are extremely common among organisations, and hence, they will form a big part of the files you will encounter in your daily work. More specifically, files with a format such as   Comma  and   Tab  separated values, along with other text files that use different delimiters, might amount to half (if not more) of the files that you will see at work.  These two formats,   Comma  and   Tab , are still only a text file but they are very useful for saving and distributing small and large datasets in a tabular way. You can identify both kinds of files by looking at the suffix part in the name of a file, comma separated values will end in   .csv  while tab separated values will end with   .tsv .  What makes these two files so similar is that they are both separated by something called delimiter. If you have a CSV or TSV file, try opening them in a plain text editor application (notepad for windows users and ) and notice what comes up.    Notice that in the example above, every value is separated by a comma and although the column headers can be found at the very top of the file (this is common practice) sometimes you might not even have them available. When we save files as TSV, CSV or with any other kind of delimiter, words with spaces in them will be wrapped around quotation marks to differentiate the spaces from the delimiter (which might be a space itself) in the data.  Lastly, let's talk about how pandas handles these types of files. To read text files into a DataFrame with pandas we can use the function   pd.read_csv()  or   pd.read_table() . These functions, at the time of writing, have over 50 parameters that allow us to customise different specification on how we would like to read in the data. One of the most important parameters is the   sep= , which allows us to define the delimiter we would like to read in the data with.   \",\"  is the default for   pd.read_csv()  and   \"\\t\"  is the default for   pd.read_table() .  The following parameters are some of the most useful ones not only for reading in text files, but also to tackle, and save time with, many of the operations you might need to perform after reading the data. Please visit the pandas documentation for more info.    header=  --> tells pandas whether the first column contains the headers of the dataframe or not.   names=[list, of, column, names]  --> allows us to explicitly name the columns of a dataframe in the order in which they are read.   parse_dates=  --> gives pandas permision to look for what might look like date data and it will assign it the appropriate date data type format.   index_col=  --> allows us to assign a specific column as the index of our dataframe.   skiprows=\\[1, 2, 3, 4\\]  --> tells pandas which rows we want to skip.   na_values=  --> takes in a list of values that might be missing and assigns them the NaN value, which stands for not a number.   encoding=  --> data might coming in from a variety of sources could have different encodings, e.g. 'UTF-8' or 'ASCII', and this parameter helps us specify which one we need for our data.   nrows=4  --> how many rows do you want to read from a file. Very useful tool for examining the first few lines of large files.  Let's use the Air Quality Monitoring Dataset and first read in the CSV file and then the TSV one.     import pandas as pd\n  The first argument is the name of the file as a string or the path to the folder where the data lives followed by the name of the file and its extension. Once you load the dataset and assign it to a variable, you can see its first 5 rows plus the column names using the method   .head() , or the method   .tail()  for the last 5.     # The first argument is the folder where the data lives and the name of the data\n   \n   df_csv = pd.read_csv('../datasets/files/seek_australia.csv')\n     df_csv.head()\n  Notice how the look and feel of our file resembles that of a spreadsheet in Excel or Google Sheets.  To read in Tab Separated Value files, all we need to do is to pass in the   sep=  argument to our   pd.read_csv()  function and provide pandas with the specific delimiter the data is split by. For tab separated values we use   \\t , but there are many other delimiters one can choose from.     # air quality data in Australia\n   df_tsv = pd.read_csv('../datasets/files/Air_Quality_Monitoring_Data.tsv', sep='\\t')\n   df_tsv.head()\n  There is another method in pandas that uses the Tab Separated Values delimiter   \"\\t\"  as its default delimiter, and that is the   pd.read_table()  method. You should use whichever you prefer, especially since most of the options in one can be found in the other. This means that by indicating the   sep=','  with a comma, you can obtain the same result as with the   pd.read_csv()  and read in Comma Separated Values.     # occupational licenses data\n   df_table = pd.read_table('../datasets/files/occupational_licences.tsv')\n   df_table.head()\n  If we know a dataset has dates in it we can also convert it to the datetime format provided by pandas as we read in the data. We can do this by putting the names of the columns that have dates in the   parse_dates=  parameter of our   pd.read_csv()  function.     # The first argument is the folder where the data lives and the name of the data\n   \n   df_csv = pd.read_csv('../datasets/files/Crashes_Last_Five_Years.csv', parse_dates=['ACCIDENT_DATE', 'ACCIDENT_TIME'])\n   df_csv.head()\n  Now we can extract additional information from our date variables with some attributes that you can find in the   dates section of pandas .     # here we are accessing the year\n   df_csv['ACCIDENT_DATE'].dt.year.head()\n     # here we are accessing the month\n   df_csv['ACCIDENT_DATE'].dt.month.tail()\n  Exercise 1  Go to any of the websites below, download a dataset of your choosing and read it into memory with   pd.read_csv() . Use at least one additional argument to read in your file.    Kaggle Datasets   UC Irvine Machine Learning Repository   Data.GOV   FiveThirtyEight                 3. Excel Files    Excel files are very common, especially if some or all of the members in your team use it for their analyses and tend to share these with us periodically. If this is the case for you, this would mean that you would have to constantly read Excel files at work either with Excel or Google Sheets. But that is up until this point, of course. Fortunately, pandas provides a nice method to read in excel files, that is flexible enough to allow you te read in specific sheets at a time if that is what your use case requires.  The pandas function,   pd.read_excel() , just like   pd.read_csv() , provides a plethora of options that you can choose from to tackle the complexity with which many Excel created.     # read in a regular file\n   df_excel = pd.read_excel(\"../datasets/files/supermarket_demo.xlsx\")\n   df_excel.tail()\n  If you open up the supermarket dataset you will notice that it contains 2 sheets. pandas function   pd.read_excel()  by default reads in the first sheet it finds in a spreadsheet so in our case, that is the myanmar dataset as shown above. Let's now read in the egypt one with the help of the   sheet_name=  argument.  Note that in the call below we also use the   parse_dates=True  argument instead of specifying the columns we want to parse, this is to tell pandas to infer which variables represent dates while it reads in the data. This method works well often but in most cases, it is better to be explicit about which variables you would like to parse as date type as opposed to leaving it to pandas.     df_excel = pd.read_excel(\"../datasets/files/supermarket_demo.xlsx\", sheet_name='egypt', parse_dates=True)\n   df_excel.tail()\n     # check the new type\n   type(df_excel['date'][0])\n     type(df_excel.loc[0,'date'])\n     type(df_excel.iloc[0,8])\n  Exercise 2   Find an Excel (or any spreadsheet) file in your computer that has data in a tabular format (i.e. a big square with rows and columns) and read it into your session with   pd.read_excel() .  If you can't find one to read in, create one with fake data and use that one insted. It does not need to have a lot of data in it. 10 rows and 5 columns would work fine.  If you don't have Excel, you can create a spreadsheet using Google Sheets and download it as an Excel file.  If neiether option above is feasible for you, please move on to the next section.                 4. HTML Files     \"Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript.\" ~   Wikipedia  pandas has, among many things, a function to allow us to read   HTML  tables from a website. This function is   pd.read_html() , and although it is not a full-fledge web scraping tool such as   Scrapy  or   BeautifulSoup . These last two libraries are very powerful web scraping tools that you are more than encouraged to explore on your own. Intermediate to complex web scraping requires a fair amount of knowledge on how the structure of a website works but I have no dobts that with a few hours of focused studying, a couple of projects later, or in the next couple of minutes, you might be well on your way to scraping your own data with 🐼. 😎  Before we explore pandas method for web scraping, let's quickly define it:    Web Scraping  refers to extracting data, structured or unstructured, from websites and making it useful for a variety of purposes, such as marketing analysis. Companies in the marketing arena use web scraping to colect comments about their products. Others, like Google, scrape the entire internet to rank websites given a criterion or search query. While web scraping might be limited in scope to a single website, like what a marketer might do,   web crawling  is the art of crawling over many different and/or nested websites on one try, or repeadately over time, like what Google does.  We will be scraping the the International Foundation for Art Research website using the link below. An important thing to keep in mind is that, the pandas function   pd.read_html()  captures whichever tables it can find in the website provided and it then adds them to a list of dataframes where each  dataframe comes from a table. This means that you would have to first assign the list to a variable and then dump the table or tables you want into a combined dataframe.   http://www.ifar.org/catalogues_raisonnes.php?alpha=&searchtype=artist&published=1&inPrep=1&artist=&author=  How to check whether there is a table in a website or not. There are probably plenty of ways to check whether there is a table in a website or not, so here are two immediate ones.   See if there is a table-like shape in the website that you are interested in. This table would ideally have information in a shape that would fit into a pandas dataframe. For example,\n   The second option is to navigate to the website you are interested in and  If you have any issues with the   pd.read_html()  function, please check and see if you have the following packages installed and then try again.    conda install lxml   pip install beautifulsoup4 html5lib     data = pd.read_html('http://www.ifar.org/catalogues_raisonnes.php?alpha=&searchtype=artist&published=1&inPrep=1&artist=&author=')\n     print(type(data), len(data))\n     type(data[0])\n     df_html = data[0]\n   df_html.head()\n  Notice that the column names are not where they should be. Let's fix that.  We will take the column names from the first row, convert the selection to a regular Python list and then reasign these names to the column names of our dataframe.     # take the first row out and make it a list\n   col_names = df_html.iloc[0].tolist()\n   col_names\n     # reasign the names to the column names index\n   df_html.columns = col_names\n   \n   # drop the first row of the dataframe\n   df_html.drop(index=0, axis=0, inplace=True)\n   df_html.head()\n  Exercise 3  Find a table to scrape in World Wide Web and read it in with pandas.            Awesome Work!  You are now ready to start cleaning and preparing datasets for analysis!    html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":183,"path":184,"dir":162,"title":185,"description":7,"keywords":186,"body":196},"content:3.data-engineering:2.data_cleaning.md","/data-engineering/data_cleaning","Data Cleaning",[187,188,189,190,191,192,193,111,86,87,88,89,194,195,127],"Notebook Structure","1. Structured vs Unstructured Data","2. What is a Data Cleaning?","3. The Data","4. Data Loading","5. Data Inspection","6. Cleaning & Preparation","7. Save your work","8. Summary","  Data Cleaning   \"There are only two forces in the world, the sword dirty data and the spirit clean data. In the long run the sword dirty data will (not) always be conquered by the spirit clean data.\" ~ Napoleon CleanYourData   “Errors using inadequate data are much less than those using no data at all.” ~ Charles Babbage     Source:    Matthieu Bogaert  Notebook Structure   Structure vs Unstructured Data  What is Data Cleaning?  The Data  Data Loading  Data Inspection  Cleaning & Preparation  Save your work  Summary  1. Structured vs Unstructured Data  Data can be found in mainly two ways in this day and age, as   structured  and   unstructured .   Structured data  is the one we find \"neatly\" organized in databases as rows and columns. Data in databases are organized in a two-dimensional, tabular format (think of it as the data you see on a grid or matrix-like spreadsheet) where every data point, unit of measure or observation can be found in the rows with a unique identifier attached to it, and where the characteristics (also called variables or features) of each one of these observations can be found in the columns.     Unstructured data , on the other hand, is more difficult to acquire, format, and manipulate as it is not often found neatly organized in a database. Unstructured data is often heavily composed of an entangled combination of text, numbers, dates, and other formats of data that are found in the wild (e.g. documents, emails, pictures, etc.).    2. What is a Data Cleaning?     Source:    https://brewminate.com/  Wikipedia has a beatiful definition of data cleaning, which was in turned modified from a paper from Shaomin Wu titled,   \"A Review on Coarse Warranty Data and Analysis\"  (see citation below).    \"Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.\"  ~ Wikipedia & Shaomin Wu  When we first encounter messy data, we usually start by going through a non-exhaustive checklist and/or use some rules of thumbs to identify, tackle, and repeat, each mess from the messy pile of data we have. Some of the items in our checklist might be:   Do we have column names? If so,  Are the column names normalised? (e.g. lower case, spaces or no spaces, numbers only as names)  Do we have dates? If so,\n   how are these represented?  Do we have different formats in different rows? (e.g. 31-Oct-2020, October 31st 2020, ...)  Do they have the time in them or is this in a separate column?  Are there different data structures within an element of an observation? (e.g. do we have lists with lists in them inside the value of a row and column combinantion)  If we have numerical data points representing a monetary value, which denomination are these in?  How was the data generated?  Do we have any missing values? if so,\n   Are they missing at random?  Are they missing by accident? (e.g. was it due to an error during the data collection process)  Are they intentionally empty? (e.g. think of a conditional question in a survey, if the participant answered yes to the previous question, use this one next, if not, skip the next 3 questions)  Are there any outliers in our dataset? if so,\n   Are these true outliers? (e.g. finding the salary of Jeff Bezos in a list with the income of all of the people from the state of Washington)  Are these mistakes we need to take care of? (e.g. finding negative prices for the price of bread, that doesn't sound right)  Are there any duplicate observations/samples in our dataset?  Is the format in which the data is stored the best one available or should we use a different one?  All of this questions get tackled in a data format described by Hadley Wickham in a paper by the same name as the data format called,   \"Tidy Data\" . In his paper, Hadley describes   Tidy Data  as:    \"Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.\"  ~ Hadley Wickham  While our datasets might not contain all of the issues described in Tidy Data that might come un in messy datasets, the strategies and concepts outlined in it will prove useful in many cases you might encounter throughout your career, so I highly recommend that you read it at some point.  One last thing about data cleaning, it is not a one time thing inside the data analytics cycle but quite the opposite, you might find yourself going back to the data cleaning process 2 or more times as your understanding of the data increases during the same project.   Sources   Wu, Shaomin (2013) A Review on Coarse Warranty Data and Analysis. Reliability Engineering and System Safety, 114 . pp. 1-11. ISSN 0951-8320.  Wickham, Hadley (2014) Tidy data. The Journal of Statistical Software, vol. 59, 2014. 10.   http://www.jstatsoft.org/v59/i10/  3. The Data  For this lesson, we will be working with a dataset containing weather data for Australia from 2007 to 2017. The nice thing about this dataset is that, although it has been pre-processed and it is quite clean, there is still a fair amount work to do regarding missing values, outliers and the like. Once you are out in the real world, you will encounter a plethora of datasets with different data types per column, incomprehensible data structures and, not to scare you, many other issues such as different formats within different elements inside different structures. In other words, data that might look like this:     Source:    https://foodbinge.tumblr.com/post/26122779310   About the data:  \nThis dataset contains weather information from many of the weather stations around Australia. For most weather stations, we have about 365 observations for the years 2007 to 2017. More information about the dataset can be found in the   Australian Bureau of Meteorology website , and below you can find a short description of the variables in the dataset.   Variables info:   Date --> day, month, and year of the observation, each weather station has its own  Location --> location of the weather station  MinTemp --> minimum temperature for that day  MaxTemp --> maximum temperature for that day  Rainfall --> the amount of rainfall recorded for the day in mm  Evaporation --> the so-called Class A pan evaporation (mm) in the 24 hours to 9am  Sunshine --> the number of hours of bright sunshine in the day  WindGustDir --> the direction of the strongest wind gust in the 24 hours to midnight  WindGustSpeed --> the speed (km/h) of the strongest wind gust in the 24 hours to midnight  WindDir9am --> direction of the wind at 9am  WindDir3pm --> direction of the wind at 3pm  WindSpeed9am --> wind speed (km/hr) averaged over 10 minutes prior to 9am  WindSpeed3pm --> wind speed (km/hr) averaged over 10 minutes prior to 3pm  Humidity9am --> humidity (percent) at 9am  Humidity3pm --> humidity (percent) at 3pm  Pressure9am --> atmospheric pressure (hpa) reduced to mean sea level at 9am  Pressure3pm --> atmospheric pressure (hpa) reduced to mean sea level at 3pm  Cloud9am --> fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many  Cloud3pm --> fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm. See Cload9am for a description of the values  Temp9am --> temperature (degrees C) at 9am  Temp3pm --> temperature (degrees C) at 3pm  RainToday --> boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0  RISK_MM --> the amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the \"risk\".  RainTomorrow --> did it rain the following day?  The dataset and the information for the variables was taken from Kaggle, and you can find out more about the dataset either using the link above or the one below, and about Kaggle using the link below as well.  Link -->   https://www.kaggle.com/jsphyg/weather-dataset-rattle-package  Now, let's get to loading, inspecting, and preparing our dataset.  4. Data Loading  We will be loading the dataset using the   pd.read_csv()  method we learned about during the last lesson, but before we load the data, we will see if we can figure inspect the first few rows of it with a helpful command line script called   head  (*nix users) or   type  (Windows users). You might be wondering if these method resemble the   df.head()  method we learned in the last lesson, and the answer is yes. By passing a second parameter   -n , then a number   -n 5 , and then the path to the file, we can print the amount of rows we specified to the console. This same command will run smoothly in Git Bash or with the   type  command below for Windows users.  Let's try it out.     # for windows users\n   !type datasets\\files\\weatherAUS.csv -Head 5\n     # for mac users or windows users with Git Bash\n   !head -n 5 ../datasets/files/weatherAUS.csv\n  Although we described the variables above, we can see that we have a date variable that we can parse as date type while reading the data into memory to save us some time. Let's go ahead and read in the data after we import our packages. We will assign our data to the variable   df .     import pandas as pd\n   import numpy as np\n   \n   pd.set_option('display.max_columns', None)\n  Note that we changed a global option of pandas so that if we print our dataframe, examine its head or tail, we can see all columns printed and not just the first and last 5, which is pandas default.     # Should you need a quick refresher on pd.read_csv(), run this cell : )\n   pd.read_csv??\n  If you are a windows user, please don't forget to to use back slashes   \\  as opposed to forward ones   /  when raeding or saving the data.     df = pd.read_csv(\"../datasets/files/weatherAUS.csv\", parse_dates=['Date'])\n     # we have a dataframe\n   type(df)\n     df.head()\n  5. Data Inspection  The first thing we want to do as soon as we get the data is to examine its content not only to see the kind of data we have but also to see if we can spot any inconsistencies that need to be dealt with from the start. Here are a few very useful methods available in pandas.    df.head()  --> shows the first 5 rows of a DataFrame or Series   df.tail()  --> shows the last 5 rows of a DataFrame or Series   df.info()  --> provides information about the DataFrame or Series   df.describe()  --> provides descriptive statistics of the numerical variables in a DataFrame   df.isna()  --> returns True for every element that is NaN and False for every element that isn't   df.notna()  --> does the opposite of   .isna()     # Let's look at the number of rows and columns we have in our dataset\n   df.shape\n     # let's now see how our columns are represented\n   df.columns\n     # let's look at some of the rows at the begining of our dataset\n   df.head()\n     # let's look at some of the rows at the end of our dataset\n   df.tail()\n  The   .info()  method is a very useful method of pandas that gives use all of the information available in the dataset, plus the memory our dataset is occupying in our computers. To get the size of the dataset we can use the argument   memory_usage='deep' . Keep in mind though that this parameter can take quite a while to run if it the dataset is too large.     df.info()\n     df.info(memory_usage='deep')\n  Nice, we have a lot of numerical values and also know that our dataset takes up about 70MB of memory in our computer. Let's examine the unique weather locations for which we have data.     len(df['Location'].unique()), df['Location'].unique()\n  Australia collects, or the dataset contains, information from 49 weather stations around the country. Let's see how many missing values do we have in this dataset. To do this, we can chain the   .isna()  method with the   .sum()  method, to get the total count of the instances where a value is missing, for each of the columns.     df.isna().sum()\n  If we would like to see the percentage of missing values per column, we could divide each column by the total amount of rows in the dataset, and then multiply by 100.     # if we would like to see the percentage of missing values per row, we could use\n   missing_values = (df.isna().sum() / df.shape[0]) * 100\n   missing_values\n     df.describe() # describe excludes all missing data by default and shows us the descriptive stats of our numerical variables\n     df.describe().T # remember the .T method to transpose arrays?\n  Let's double check the years we have data for, and how many values do we have per year.     df['Date'].dt.year.value_counts()\n  We could also look at how much data do we have per city for all of the years. For this we can select the specific location we want, say Sydney, pass this selection as a boolean condition to our dataframe while selecting the Date column, and the use a very covenient pandas method called   .value_counts() . This method counts the instances of every category selected and returns the total number in descending order.     melbourne = df['Location'] == 'Melbourne'\n   melbourne.head()\n     sorted(df.loc[df['Location'] == 'Sydney', 'Date'].dt.year.value_counts())\n  Some of the things we found were:   Most variables have missing data below 10% of the sample and only a handful have more than 35% of missing values  Most variables are numerical  The columns could be made to lower case  We need to figure out why the data is missing where it is missing  6. Cleaning & Preparation  Cleaning and prepraring our data for analysis is one of the most crucial step of the data analytics cycle, and a non-perfect one as well. You will often find yourself coming up with different ways of reshaping and structuring the data, and thus, coming back to the   Clean & Prepare  stage of the process. This is completely normal and somewhat rewarding, especially since a lot of the times, insights come out when you least expect them, and even while you are working with different data.  Let's begin by normalising our columns so that they have no spaces and are all lowercase. This is never a necessity but rather a preference.     df.columns\n     [col.lower() for col in df.columns]\n     # Let's normalise the columns\n   df.columns = [col.lower() for col in df.columns]\n   df.columns\n  6.1 Dealing with Missing Values    In the last section we realised that we have quite a few missing values in some of the columns, and we should deal with them carefully. pandas provides a couple of great tools for dealing with missing values, and here are some of the most important ones dropping and detecting missing values.    .dropna()  --> drops all or some missing values by column or row. Default is row   .isna()  --> returns a boolean Series or DataFrame with a True for NaN values   .notna()  --> does the opposite of   .isna()   .isnull()  --> same as   .isna()   .notnull()  --> same as   .notna()   .fillna()  --> allows you to fill missing values given a criterion  When we encounter NaN values, our default action should never be to drop them immediate. We should first figure out why these values might be missing by thoroughly inspecting the data, and by looking at the documentation of how the data was gathered/acquired, should one exist and have enough details of the data collection process, of course. If you come up with a project where you scraped the data you needed, documentation might be a bit trickier.  One of the reasons we don't want to get rid of missing data immediately is that we might not be able to tell, upon first inspection, whether the missing values are due to an error with data collection or simply an instance that doesn't exist. For example, imagine you own a retail store that sells clothes for all kinds of weather and that you have a general survey that you send out to all of your customers. If you were to ask a customer in Latin America about whether they like to wear fluffy coats or regular coats whenever is winter season, they will probably leave that section blank because they don't experience a change of weather significant enough to buy that type of clothing. Hence, the missing value is not due to an error but rather an accurate representation of the answers provided by the respondents.  We do, however, might want to get rid of columns with too many missing values and/or rows with too few. And this is, in fact, what we will do first by dropping the rows with less than 10% of missing values.  We can accomplish this by first creating a condition with our   missing_values  values var where we filter out the columns with 10% or more missing values, and leave the ones with less so that we can remove the missing rows from them using the method   .dropna()  of pandas. Before getting rid of the missing values though, we will first check if there are any duplicate rows in our dataset that might be inflating the number of missing values.     df.duplicated().head()\n     # We use the .sum() method to add up the instances where the values are indeed duplicated\n   df.duplicated().sum()\n  Since we detected no duplicates, we will do one last step before removing rows with missing values, and that is to fill in any of the categorical variables with the word   Unkown  as a placeholder. We will do so by first creating a dictionary and then passing that dictionary into the   .fillna()  pandas method.     df.head()\n     categorical_vars = {\n       'windgustdir': 'Unknown',\n       'windgustspeed': 'Unknown',\n       'winddir9am': 'Unknown',\n       'winddir3pm': 'Unknown'\n   }\n     df[categorical_vars.keys()].head()\n     df.fillna(categorical_vars, inplace=True)\n     missing_values = (df.isna().sum() / df.shape[0]) * 100\n   missing_values\n     type(missing_values)\n     missing_values[(missing_values \u003C= 10) & (missing_values > 0)].index\n     mask_of_rows_to_drop = (missing_values \u003C= 10) & (missing_values > 0)\n   \n   rows_to_drop = list(missing_values[mask_of_rows_to_drop].index)\n   rows_to_drop\n     # we will assign the new dataframe to a new variable\n   \n   df_clean1 = df.dropna(subset=rows_to_drop, axis=0).copy()\n   \n   # and then check if there was a significant change\n   (df_clean1.isna().sum() / df_clean1.shape[0]) * 100\n  The following subtraction will tell us how many rows were deleted by the previous action. Remember that shape gives us back a tuple with   (rows_length, col_lenght) .     df.shape[0] - df_clean1.shape[0]\n  It is important to note that we not always want to pick such a high number like 10 to drop rows with missing values since we might be sacrificing way too much information. We instead, should work with stakeholders to figure out reasons and/or solutions for missing values. Ideally, dropping rows with 5% or less would be okay for any given dataset but again, it is best to deal with them alongside the subject-matter experts to tackle the issue as best as possible.     df_clean1.info()\n  Our next step would be to either drop the columns that have more than a third of their values missing or, to pick a value that makes sense to fill in the missing values. For example, we might want to use the mean or the median of the values of a column to fill in the missing values. If our data was orderd, i.e. time series, we might use methods such as forward and backward fill which take the previous and following available value, respectively, and fill in the missing ones with these.  We also need to keep in mind that there may be a few outliers in our columns with missing values, and if so, the mean would give us an unrealistic representation of the missing values. We could deal with these missing values in two ways, for the numerical values we will use the median, which is robust against outliers, and for the categorical variables we will use forward or backward fill or we could fill in the missing instance with the word   Unknown  as a placeholder, as done previously, and carry on with cleaning and analysing the data.  Let's examine the columns we have left with missing values.     df_clean1[['evaporation', 'sunshine', 'cloud9am', 'cloud3pm']].describe()\n  Notice that the minimum value for   cloud9am  and   cloud3pm  is   0 . This means that it could be that there are days with no clouds in the sky. Hence, it might be more realistic to fill in the missing value of our cloudy days with a 0 rather than the mean or the median. Let's do this with the   .fillna()  method.     df_clean1[['cloud9am', 'cloud3pm']] = df_clean1[['cloud9am', 'cloud3pm']].fillna(0)\n   df_clean1.isna().sum()\n  Lastly, evaporation and sunshine both have their mean and medians quite close to each other so we could, potentially, favor either option but there is one more caveat, the standard deviation. The standard deviation is a measure of dispertion that tells us how far, up or down, the fluctuations from the mean might be. To err on the safer side, let's use the median to fill in our missing values.  We will use a loop to do this.   we will iterate over the columns  use the column name to iterate over the dataframe  check for whether a column has missing values  if so, we will use the median of that same column to fill in its missing values     for col in df_clean1.columns:\n       if df_clean1[col].isna().any():\n           df_clean1[col].fillna(value=df_clean1[col].median(), axis=0, inplace=True)\n  Let's check if there are any remaining missing values.     df_clean1.isna().sum()\n  Nice work! Let's now get a few additional variables before we move on to saving our cleaned dataset.  Since the weather is time series data (e.g. data gathered over time), we will create additional date variables for visualisation purposes. When we have a date column of data type   datetime , we can access all of the attributes available in our date column using the   dt  attribute followed by the subattribute we would like to access. You can find out more about the additional subattributes in the   documentation of pandas here .     df_clean1['date'].dt.weekday.head(15)\n     df_clean1['month'] = df_clean1['date'].dt.month\n   df_clean1['year'] = df_clean1['date'].dt.year\n   df_clean1.head()\n  Exercise 1  Add a new column to the dataset that contains the week of the year. Call the new column,   week .       Exercise 2  Add a new column to the dataset that contains the weekday in numbers (e.g. 0 == Monday, 1==Tuesday, or something similar). Call the new column,   weekday .       Exercise 3  Add a new column to the dataset that contains the quarter of the year. Call the new column,   quarter .       Exercise 4  Add a new column to the dataset that contains the name of the day of a week (e.g. Monday, Tuesday, etc.). Call the new column,   day_of_week .       Exercise 5  Add a new column to the dataset that says whether it is a weekday or the weekend. Call the new column,   week_or_end .            We might want to represent the quarter variable as a category later on, so we will create a dictionary with the values we would like to change, and pass it to our Python's   .map()  function. A very useful fuction to map a function to an array of values, to a column or other data structure. We will assign the result to a new column called   qrt_cate .     # for more info on how map works, please run this cell\n   map?\n     mapping = {1:'first_Q',\n              2:'second_Q',\n              3:'third_Q',\n              4:'fourth_Q'}\n   \n   \n   df_clean1['qtr_cate'] = df_clean1['quarter'].map(mapping)\n  7. Save your work  The last thing we want to do is to reset the index of our dataframe and save its clean version for later use.  We can use pandas method   .reset_index()  to reset the index. Notice the   drop=True , if we do not make this parameter equal to True, pandas will assign the old index to a new column.  The next method we will use is   .to_csv() . By applying this method to a dataframe, all we need to do is to give the data a name (in quotation marks), and pass in the   index=False  parameter if we don't want the index to be added as a new column.     df_ready = df_clean1.reset_index(drop=True).copy()\n     df_ready.to_csv('weather_ready.csv', index=False)\n     !head -n 5 weather_ready.csv\n    Awesome Work! We will continued to clean more dataset but for now, on to DataViz  8. Summary  In this lesson we have covered pandas in great lenght, and still, we have yet to scratch the surface of what this powerful tool can do. Some keypoints to take away:   pandas provides two fantastic data structures for data analysis, the DataFrame and the Series  We can slice and dice these data structures to our hearts content all while keeping in mind the inconsistencies that we might find in different datasets  We should always begin by inspecting our data immediately after loading it into our session. pandas provides methods such as info, describe, and isna that work very well and allow us to see what we have the data  When cleaning data, missing values need to be treated carefully as the reasons behind them might differ from one variable to the next.  Always keep in mind to\n   Check for duplicates  Normalise columns  Deal with missing values, preferably with stakeholders or subject matter experts if the amount of missing values is vast  Use dates to your advantage  Don't try to learn all the tools inside pandas but rather explore the ones you need as the need arises, or, explore them slowly and build an intuition for them  References  Sweigart, Al.   Automate the Boring Stuff with Python: Practical Programming for Total Beginners . No Starch Press, 2020.  VanderPlas, Jake.   A Whirlwind Tour of Python . O'Reilly, 2016.  VanderPlas, Jake.   Python Data Science Handbook . O'Reilly, 2017.  McKinney, Wes.   Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython . OReilly, 2018.  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":198,"path":199,"dir":162,"title":200,"description":7,"keywords":201,"body":208},"content:3.data-engineering:3.etl_pipes.md","/data-engineering/etl_pipes","01 ETL Pipelines",[42,165,120,166,167,168,202,147,203,204,205,206,147,207],"6. 🐼 pipelines","7. Extract","8. Transform","9. Load","10. Launch a Pipeline","11. Summary","  01 ETL Pipelines   “Without a systematic way to start and keep data clean, bad data will happen.” — Donato Diorio    Source:   Mindaugas Sadūnas  Table of Contents    Overview   Learning Outcomes   Data   Tools   Data Pipelines   🐼   pipe lines   Extract   Transform   Load   Launch a Pipeline   Summary  1. Overview  You are a data analyst working at Beautiful Analytics, and you have been given a project in which you will work using data generated from shared bikes systems in the cities of London (England, UK), Seoul (South Korea), and Washington (DC, USA). Your customer's are the governments of each city and what they want to solve is the same,   Challenge #1   To predict/forecast how many bikes they will to need to have available in the city at every hour of the dat for the next few years?  Each government captures similar data but, as you can imagine, they all use different words and measure similar variables in different ways. This means that our first job before we can answer the question above is to fix the data and put it in amore user-friendly way. While we are at it, we should also try and automate our pipeline so that the next time we need to read, transform, and load new versions of all the data sources for this project, we could do so with the click of a button rather than having to write everything again from scratch. So our first real problem is,   Challenge #0   Create a data pipeline that extracts, transforms and loads the necessary data for the task at hand.  2. Learning Outcomes  Before we get started, let's go over the learning outcomes for this section of the workshop.  By the end of this lesson you will be able to,   Discuss what ETL and ELT Pipelines are.  Understand how to read and combine data that comes from different sources.  Create data pipelines using pandas and prefect.  Understand how to visualize the pipelines you create to help you with their development.  3. Data    All three data files contain similar information about how many bicycles have been rented each hour, day, week and months for several years and for each city government we are working with.  You can get more information about the data of each city using the following links.    Seoul, Korea del Sur   London, England, UK   Washington, DC, USA  Here are the variables that appear in the three data files. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     London  Seoul  Washington    date  date  instant   count  count  date   temperature  hour  seasons   t2  temperature  year   humidity  humidity  month   wind_speed  wind_speed  hour   weather_code  visibility  is_holiday   is_holiday  dew_point_temp  weekday   is_weekend  solar_radiation  workingday   seasons  rainfall  weathersit    snowfall  temperature    seasons  count    is_holiday  humidity    functioning_day  wind_speed     casual     registered  Since all of these datasets where generated with different business logic and, most likely, by completely different systems, we can certainly expect more inconsistencies than just unmatching column names, numerical formats, and data collected. We will walk through a few cleaning steps after we discuss the tools we will be using today.  4. Tools  The tools that we will use in the workshop are the following.    pandas  - \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"   Prefect  - \"is a new workflow management system, designed for a modern infrastructure and powered by the open source workflow engine, Prefect Core. Users organize tasks into   task 's and   flow 's, and Prefect takes care of the rest.\"   sqlite3  - \"SQLite is a library written in C that provides a lightweight disk-based database that does not require a separate server process and allows the database to be accessed using a non-standard variant of the SQL query language.\"   pathlib  - allows us to manipulate paths as if they were python objects.  Before we continue, let's load the modules we'll need and examine our datasets.     import pandas as pd\n   from prefect import flow, task\n   import sqlite3\n   from contextlib import closing\n   from pathlib import Path\n   \n   \n   pd.options.display.max_rows = None\n   pd.options.display.max_columns = None\n     path = Path().cwd().parent.joinpath(\"data\", \"01_part\")\n   path\n  We will need paths to all of our datasets.     london_path = path.joinpath('raw', 'london', 'london_bikes.db')\n   seoul_path = path.joinpath('raw', 'seoul', 'SeoulBikeData.csv')\n   wash_dc_path = path.joinpath('raw', 'wash_dc', 'washington.json')\n  The data we have about the bikes in London is in a SQLite database and to read it, we first need to create a connection to its database using the   sqlite3  package.  The next step is to use the pandas   read_sql_query  function to read the data. This function takes two arguments, the connection to our databaset and the query to get the data we want.  Note that we can make quite elaborate queries to get a subset of the data we'll be working with, but for our purposes this is not necessary.     conn = sqlite3.connect(london_path)\n   query = \"SELECT * FROM uk_bikes\"\n     london = pd.read_sql_query(query, conn)\n   london.head()\n  Seoul data is in text form and separated by commas, and the data for DC is in JSON format. For these two we can use   pd.read_csv  and   pd.read_json , respectively.     seoul = pd.read_csv(seoul_path)\n   seoul.head()\n     washington = pd.read_json(wash_dc_path)\n   washington.head()\n  5. Data Pipelines     What are ETL Pipelines?  The acronym ETL stands for Extract, Transform, and Load, and it is the process by which organizations' data passes through before it reaches the hands of data consumers. Which can be analysts, marketers, sales people, customers, and so on...   Why should you learn how to create them?  As data professionals, our task is to create value for our organizations, our clients and our collaborators using some of, if not all, the data that we have at our disposal. However, to get the most out of our organization's data, we often need   Information about the process by which it was generated, For example,\n   Point of sale  Clicks on an online marketplace like Amazon, Etzy, Ebay, ect.  Epidemiological studies  A/B Test Results  ...  Information about the transformations that occurred during the cleaning and merging process. For instance,\n   Celcius degrees were converted into fahrenheit  Prices in Chilean pesos were converted to {insert preferred 💸}  Non-numerical and unavailable observations now contain \"Not Available\" or \"Unknown\"  Numerical variables now contain the average or median value in observations where that data is missing. For example, a variable with the salary of all employees of a company now contains $40,000$ / year USD in the values that were not available becuase this was the median value.  ...  Information about how the data was stored and where. For instance,\n   Parquet format  NOSQL or SQL database  CSV  ...  Understanding how the three processes described above flow will help us have more knowledge about the data that we are going to use, and how to best access it, transform it, and model it before we put it to good use.  Let's walk through an example of a data pipeline using data from wildfires between 1983-2020 in the United States. You can find more information about the dataset   here .     example_data_in = path.joinpath(\"example\", \"federal_firefighting_costs.csv\")\n     pd.read_csv(example_data_in).head()\n  As you can see, most columns contain a   $  dollar sign and some   ,  commas, and because this forces Python to treat numbers as objects rather than   int 's or   float 's, we will have to remove these signs in our transformation step after extracting the data and before loading a clean version of it.   About Prefect\nAt its core, prefect create Directed Acyclic Graphs where nodes are functions of computations linking one step to another.  When you use prefect you have two important APIs, one is the   task  API and the other is   flow . The former is used as a decorator on top of functions and this allows us to signal to prefect that our decorated functions can become a node in a graph. The latter represents a set the graph we want to run and it is composed of one or multiple tasks, or even multiple flows.  For example, let's create 3 functions, one that extracts the data we need, another that takes anything that is non-numerical out of our variables, and another that loads the cleaned data somewhere. Each function will have the   task  decorator on top of it.     @task(name=\"Get Data!\")\n   def extract(path):\n       return pd.read_csv(path)\n  As you saw above, only the last 5 variables have commas (  , ) and dollar symbols (  $ ) so we will replace both with an empty space (  \"\" ) using a   for  loop.  For the download process, we will save the data in the   parquet  format. This is one of the most popular formats to save data in due to its compression capabilities, orientation, and speed gains in analytical workloads.  Here's an example on the differences between the row-like format and the columnar format of parquet files.    Source:   SAP HANA Central     @task(name=\"Transform Data!\")\n   def transform(data):\n       for col in data.iloc[:, 1:].columns:\n           data[col] = data[col].str.replace(r'[^0-9]+', '', regex=True).astype(int)\n       return data\n     @task(name=\"Load Data!\")\n   def load(data, path):\n       data.to_parquet(path, compression='snappy')\n  When we have all the steps ready, we create a new function containing our graph using the   flow  decorator. We can give this function a name, for example,   \"Example Pipeline! 😎\"  and then chain the tasks we created previously in the order in which they should be run.  After we create our function, we can simply run it with the arguments necessary and Prefect will do its magic.     example_data_out = path.joinpath(\"example\", \"my_test.parquet\")\n   type(example_data_out)\n     @flow(name=\"Example Pipeline! 😎\")\n   def example_etl(example_data_in, example_data_out):\n       data = extract(example_data_in)\n       data_clean = transform(data)\n       load(data_clean, example_data_out)\n       print(\"Your Pipeline Ran Successfully!\")\n     example_etl(example_data_in, example_data_out)#.execute_in_process()\n  To see the graph we just ran, we can open the terminal and run   prefect orion start . This will take us to the Prefect UI where we can evaluate not only the run and graph, but also the parameters and logs collected from all DAGs we run.  To make sure we have the correct data, let's create a visualization with pandas and hvplot, a package that allows us to add interactivity to our pandas' charts.     import hvplot.pandas\n     pd.read_parquet(path.joinpath(\"example\", \"my_test.parquet\")).hvplot(x='Year', y=\"ForestService\")\n  6. 🐼   pipe lines    The   pipe  operator is a pandas function that allows you to chain operations that take a data set, modify it, and return the modified version of the original data. In essence, it allows us to move the data through a series of steps until we reach the structure in which we want to have it.  For example, imagine that we have a group of data and 4 functions to fix it, the chain of operations would look like this.     (data.pipe(change_cols, list_of_cols)\n        .pipe(clean_numeric_vars, list_of_numeric_vars)\n        .pipe(add_dates_and_location, 'Auckland', 'NZ')\n        .pipe(fix_and_drop, 'column_to_fix', seasons_NZ, cols_drop_NZ))\n  Another way to visualize what happens with pandas'   pipe  is through the following food process where we have ingredients and we actually require food.    Let's start with a small example without   pipe  first.     toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n                            \"Cities\":       [\"Miami\", \"Dallas\", \"Washington\"],\n                            \"Date\":         pd.date_range(start='9/27/2021', periods=3)})\n   toy_data\n     def change_cols(data, cols_list):\n       data.columns = cols_list\n       return data\n     def clean_col_names(index_of_cols: pd.Index):\n       import re\n       new_cols =  [re.sub(r'[^a-zA-Z0-9\\s]', '', col).lower().replace(r\" \", \"_\") for col in index_of_cols]\n       print(type(index_of_cols))\n       print(index_of_cols)\n       print(type(new_cols))\n       print(new_cols)\n     pd.Index\n     clean_col_names(toy_data.columns)\n     change_cols(toy_data, [\"postal_code\", \"city\", \"date\"])\n  As you can see, with a single function it doesn't make much sense to pipe the process but with a chain of functions, the story changes.  The next thing we want to do is add additional information about the dates we have for each file. We can achieve this after converting the   date  variable to   datetime  format, which will allow us to access the year, month, week, ect. inside each date.     london_cols = ['date', 'count', 'temperature', 'temp_feels_like', 'humidity', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'seasons']\n   seoul_cols = ['date', 'count', 'hour', 'temperature', 'humidity', 'wind_speed', 'visibility', 'dew_point_temp', 'solar_radiation', 'rainfall', 'snowfall', 'seasons', 'is_holiday', 'functioning_day']\n   wa_dc_cols = ['instant', 'date', 'seasons', 'year', 'month', 'hour', 'is_holiday', 'weekday', 'workingday', 'weathersit', 'temperature', 'temp_feels_like', 'humidity', 'wind_speed', 'casual', 'registered', 'count']\n     def add_dates_and_location(data, city, country):\n       \n       data['date']       = pd.to_datetime(data['date'], infer_datetime_format=True, dayfirst=True)\n       data[\"year\"]       = data['date'].dt.year\n       data[\"month\"]      = data['date'].dt.month\n       data[\"week\"]       = data['date'].dt.isocalendar().week.astype(int)\n       data[\"day\"]        = data['date'].dt.day\n       data[\"weekday\"]    = data['date'].dt.dayofweek\n       data[\"is_weekend\"] = (data[\"weekday\"] > 4).astype(int)\n       \n       # note that we don't want to overwrite data that already has the hour as a column\n       if 'hour' not in data.columns: \n           data[\"hour\"]   = data['date'].dt.hour\n       data['date']       = data['date'].dt.date\n       \n       # non-date variables\n       data['city']       = city\n       data['country']    = country\n       \n       return data\n     add_dates_and_location(toy_data, \"Sydney\", \"AU\")\n  As you can see, we added a lot of information to our data with a simple function, but what happens when we want to chain two or three or four operations together? The following would not be very easy to read right?     add_dates_and_location(change_cols (toy_data, [\" postal_code \",\" city \",\" date \"]),\" Sydney \",\" AU \")\n  Let's move on to using our   pipe  operator now.     toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n                            \"Cities\":       [\"Miami\", \"Dallas\", \"Washington\"],\n                            \"Date\":         pd.date_range(start='9/27/2021', periods=3)})\n     (\n       toy_data.pipe(change_cols, [\"zip_code\", \"city\", \"date\"])\n               .pipe(add_dates_and_location, \"Sydney\", \"AU\")\n   )\n  As you can see, now our chained functions are more readable than before and we can continue and chain even more functions in the same fashion.  In our data we have the stages of the year with different names and we also have variables that we do not need or that are not in the three files. Let's fix both!     seasons_london = {0: 'Spring', 1: 'Summer', 2: 'Fall', 3: 'Winter'}\n   seasons_wa_dc = {1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'}\n   holidays_seoul = {'No Holiday': 0, 'Holiday': 1}\n     cols_drop_london = ['temp_feels_like', 'weather_code']\n   cols_drop_seoul = ['visibility', 'dew_point_temp', 'solar_radiation', 'rainfall', 'snowfall', 'functioning_day']\n   cols_drop_wa_dc = ['instant', 'temp_feels_like', 'casual', 'registered', 'workingday', 'weathersit']\n     def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n       data[col_to_fix] = data[col_to_fix].map(mapping)\n       return data.drop(cols_to_drop, axis=1)\n  Let's test the   pipe  operator but with the data from DC now and evaluate the results.     washington.head()\n     (washington.pipe(change_cols, wa_dc_cols)\n              .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)).head()\n  Lastly, we esnt to create a function to normalize the data for DC as the columns are not in the same metric/imperial, etc. system as the rest.     def normalize_vars(data):\n       data['temperature'] = data['temperature'].apply(lambda x: (x * 47) - 8)\n       data['humidity']    = data['humidity'].apply(lambda x: (x / 100))\n       data['wind_speed']  = data['wind_speed'].apply(lambda x: (x / 67))\n       return data\n  Finally, we can use our   pipe  operator again to complete the process and see the end result.  Since we haven't created a function to read JSON files, let's create one anc call it   extract_data .     def extract_data(path):\n       return pd.read_json(path)\n     washington = (extract_data(wash_dc_path).pipe(change_cols, wa_dc_cols)\n                                           .pipe(add_dates_and_location, 'DC', 'USA')\n                                           .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)\n                                           .pipe(normalize_vars))\n   washington.head()\n  Exercise   Create a function to extract the London data.  Create a data pipe similar to the one in Washington using pandas'   pipe .                      7. Extract  Depending on where the data is, in what format it is stored, and how we can access it, this can either be one of the shortest steps or one of the longest ones in our data pipeline. Here are some of the formats that you might encounter in your day to day.   Text: usually the text format is similar to what we see in Microsoft Excel but without formulas or graphics. For example, CVS or TSV.  JSON: JavaScript Object Notation is a very popular sub-language for its simple syntactics  Databases: These can be SQL, NOSQL, MPP (massively parallel processing), among others.  GeoJSON: It is a type of format for data that contains geographic information. There are many more types of data for GIS.  HTML: Refers to Hyper Text Markup Language and represents the skeleton of almost all web pages in existence.  ...  Since we already learned how to create useful pipelines with pandas, we now need to create functions for our main ETL pipeline to automate the process. We can achieve this using the prefect decorator,   @task  from earlier. This decorator takes note of the functions that we want to link together and helps us create a network in which each node is a function and each link connects one or more functions only once.  Remember, the decorator   @task  is also a function within prefect and as such, we can pass several arguments to it that help us modify the behavior of each function in our pipeline.  You can learn more about the   task  API in the official docs   here .     task??\n     @task\n   def extract_1(path):\n       return pd.read_csv(path)\n     @task\n   def extract_2(path):\n       conn = sqlite3.connect(path)\n       query = \"SELECT * FROM uk_bikes\"\n       return pd.read_sql_query(query, conn)\n     @task\n   def extract_3(path):\n       return pd.read_json(path)\n  8. Transform  The most common transformations that happen at this stage are usually the ones we created earlier. In short,   Clean text data  Normalize columns  Convert numeric variables to the same unit  Deal with missing values  Join data  Our transform step will be a parent function to our pipe operators from earlier. Hence, a combination of functions handle by a single one.     def order_and_merge(data_lists):\n       \n       pick_order = data_lists[0].columns\n       new_list = [d.reindex(columns=pick_order).sort_values(['date', 'hour']) for d in data_lists]\n       df = pd.concat(new_list)\n       return df\n     @task\n   def transform(london, seoul, washington):\n       \n       london = (london.pipe(change_cols, london_cols)\n                       .pipe(add_dates_and_location, 'London', 'UK')\n                       .pipe(fix_and_drop, 'seasons', seasons_london, cols_drop_london))\n       \n       seoul = (seoul.pipe(change_cols, seoul_cols)\n                     .pipe(add_dates_and_location, 'Seoul', 'SK')\n                     .pipe(fix_and_drop, 'is_holiday', holidays_seoul, cols_drop_seoul))\n       \n       wash_dc = (washington.pipe(change_cols, wa_dc_cols)\n                            .pipe(add_dates_and_location, 'DC', 'USA')\n                            .pipe(fix_and_drop, 'seasons', seasons_wa_dc, cols_drop_wa_dc)\n                            .pipe(normalize_vars))\n       \n       return order_and_merge([london, seoul, wash_dc])\n  9. Load  We will need to save our new file in a database or in a format in which we are given both what the file occupies and the speed with which we can open and use it. So we will create two paths and two names for the files that we will use later.     clean_path = path/'processed/clean.parquet'\n   clean_db_path = path/'processed/bikes'\n     clean_db_path\n  We will have a function for saving the data in a database. For this, Prefect provides us with a wrapper function around SQLite called,   SQLiteScript  which allows us to create a database and run a SQL scipt on top of it. This comes in hady for large and small operations/prototypes alike.     def SQLiteScript(db, script):\n       with closing(sqlite3.connect(db)) as conn:\n           with closing(conn.cursor()) as cursor:\n               cursor.executescript(script)\n     script= \"\"\"\n               CREATE TABLE IF NOT EXISTS bike_sharing (\n                       date text, count integer, temperature real,\n                       humidity real, wind_speed real, is_holiday real,\n                       is_weekend integer, seasons text, year integer,\n                       month integer, week integer, day integer, \n                       hour integer, weekday integer, city text,\n                     country text\n                 )\n   \"\"\"\n     next(washington.itertuples(name='Bikes', index=False))\n     @task\n   def load(data, path_and_name):\n       \n       data = list(data.itertuples(name='Bikes', index=False))\n   \n       insert_cmd = \"INSERT INTO bike_sharing VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n       with closing(sqlite3.connect(path_and_name)) as conn:\n           with closing(conn.cursor()) as cursor:\n               cursor.executemany(insert_cmd, data)\n               conn.commit()\n  10. Launch a Pipeline  In the same fashion as before, we will combine our functions in a function decorated with   flow  to signal to prefec's scheduler that we have a DAG we would like to run.     @flow(name='bikes-ETL')\n   def my_etl():\n       \n       new_table = SQLiteScript(db=str(clean_db_path), script=script)\n       \n       london = extract_2(london_path)\n       seoul = extract_1(seoul_path)\n       wash_dc = extract_3(wash_dc_path)\n       \n       transformed = transform(london, seoul, wash_dc)\n           \n       data_loaded = load(transformed, clean_db_path)\n  Running our pipeline is as simple as calling the function. In addition, even though we did not pass in any parameters to our flow function, it can contain any number of them if we choose to.     my_etl()\n     pd.read_sql_query(\"SELECT * FROM bike_sharing\", sqlite3.connect(clean_db_path)).head()\n  Exercise  Change the function to unload (  load () ) and make it save the results in   parquet  format. Run the pipeline again and make sure the results are the same as above by reading the data with pandas' respective function for parquet files.                 11. Summary   Creating ETL pipes helps us automate the cleaning steps to prepare our datasets.  pandas   pipe  helps you chain functions that operate on a dataframe and return a dataframe, effectively helping us save time and lines of code.  Prefect helps us create, monitor, and schedule directed acyclic graphs represented via the steps of our cleaning process.  html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":210,"path":161,"dir":162,"title":163,"description":7,"keywords":211,"body":174},"content:3.data-engineering:4.data_pipelines_getting_started.md",[42,165,120,166,167,168,169,170,171,172,173],{"id":213,"path":214,"dir":7,"title":215,"description":7,"keywords":216,"body":217},"content:4.data-science:0.index.md","/data-science","Intro to DS",[],"  Intro to DS",{"id":219,"path":220,"dir":7,"title":221,"description":7,"keywords":222,"body":223},"content:5.ml-engineering:0.index.md","/ml-engineering","Intro to MLE",[],"  Intro to MLE",1733708749192]